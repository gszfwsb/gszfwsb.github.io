<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://gszfwsb.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://gszfwsb.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-10-30T05:27:47+00:00</updated><id>https://gszfwsb.github.io/feed.xml</id><title type="html">Shaobo Wang</title><subtitle>Shaobo Wang&apos;s Homepage. </subtitle><entry><title type="html">第一章 Overview (课程概括与RL基础)</title><link href="https://gszfwsb.github.io/blog/2023/RL-lecture1/" rel="alternate" type="text/html" title="第一章 Overview (课程概括与RL基础)"/><published>2023-10-26T00:00:00+00:00</published><updated>2023-10-26T00:00:00+00:00</updated><id>https://gszfwsb.github.io/blog/2023/RL-lecture1</id><content type="html" xml:base="https://gszfwsb.github.io/blog/2023/RL-lecture1/"><![CDATA[<h1 id="一强化学习概述">一、强化学习概述</h1> <p>什么是强化学习：强化学习 (reinforcement learning, RL) 讨论的问题是<strong>智能体 (agent)</strong> 怎么在复杂、不确定的<strong>环境</strong> (environment) 里面去最大化它能获得的<strong>奖励</strong>。如图所示, 强化学习由两部分组成: 智能体和环境。在强化学习过程中, 智能体与环境一直在交互：</p> <ol> <li>智能体在环境里面获取某个状态后, 它会利用该状态输出 一个<strong>动作</strong> (action), 这个动作也称为<strong>决策</strong>(decision)。</li> <li>然后这个动作会在<strong>环境</strong>之中被执行, 环境会根据智能体采取的动作, 输出下一个<strong>状态</strong>以及当前这个动作带来的<strong>奖励</strong>。智能体的目的就是尽可能多地从环境中获取奖励。</li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter1/Untitled-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter1/Untitled-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter1/Untitled-1400.webp"/> <img src="/assets/img/RL_chapter1/Untitled.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="11-强化学习与监督学习">1.1 强化学习与监督学习</h2> <ol> <li>监督学习：监督学习（supervised learning）首先假设我们有大量被标注的数据，这些图片都要满足<strong>独立同分布</strong>，即它们之间是没有关联关系的。所以在监督学习过程中，有两个假设。 <ol> <li>输入的数据（标注的数据）都应是没有关联的。因为如果输入的数据有关联，学习器（learner）是不好学习的；</li> <li>我们告诉学习器正确的标签是什么，这样它可以通过正确的标签来修正自己的预测。</li> </ol> </li> <li>在强化学习里面，监督学习的两个假设其实都不满足。 <ol> <li>智能体得到的观测（observation）不是独立同分布的，上一帧与下一帧间其实有非常强的连续性。我们得到的数据是相关的时间序列数据，不满足独立同分布。</li> <li>另外，我们并没有立刻获得反馈，游戏没有告诉我们哪个动作是正确动作。比如我们现在把木板往右移，这只会使得球往上或者往左去一点儿，我们并不会得到立刻的反馈。因此，强化学习之所以这么困难，是因为智能体不能得到即时的反馈，然而我们依然希望智能体在这个环境里面学习。</li> </ol> </li> <li>区别总结： <ol> <li>强化学习输入的样本是序列数据，而不像监督学习里面样本都是独立的。</li> <li>学习器并没有告诉我们每一步正确的动作应该是什么，学习器需要自己去发现哪些动作可以带来 最多的奖励，只能通过不停地尝试来发现最有利的动作。</li> <li>智能体获得自己能力的过程，其实是不断地试错探索（trial-and-error exploration）的过程。探索 （exploration）和利用（exploitation）是强化学习里面非常核心的问题。其中，探索指尝试一些新的动作， 这些新的动作有可能会使我们得到更多的奖励，也有可能使我们“一无所有”；利用指采取已知的可以获得最多奖励的动作，重复执行这个动作，因为我们知道这样做可以获得一定的奖励。因此，我们需要在探 索和利用之间进行权衡，这也是在监督学习里面没有的情况。</li> <li>在强化学习过程中，没有非常强的监督者（supervisor），只有<strong>奖励信号（reward signal）</strong>，并且奖励信号是延迟的。</li> </ol> </li> </ol> <h2 id="12-强化学习的特征">1.2 强化学习的特征</h2> <ol> <li>强化学习会试错探索，它通过探索环境来获取对环境的理解。</li> <li>强化学习智能体会从环境里面获得延迟的奖励。</li> <li>在强化学习的训练过程中，时间非常重要。因为我们得到的是有时间关联的数据（sequential data）， 而不是独立同分布的数据。</li> <li>智能体的动作会影响它随后得到的数据。在训练智能体的过程中，很多时候我们也是通过正在学习的智能体与环境交互来得到数据的。所以如果在训练过程中，智能体不能保持稳定，就会使我们采集到的数据非常糟糕。我们通过数据来训练智能体，如果数据有问题，整个训练过程就会失败。所以在强化学习里面一个非常重要的问题就是，怎么让智能体的动作一直稳定地提升。</li> </ol> <h1 id="二序列决策">二、序列决策</h1> <h2 id="21-智能体和环境">2.1 智能体和环境</h2> <p>强化学习研究的问题是智能体与环境交互的问题。智能体把它的动作输出给环境，环境取得这个动作后会进行下一步，把下一步的观测与这个动作带来的奖励返还给智能体。这样的交互会产生很多观测，智能体的目的是从这些观测之中学到能最大化奖励的策略。</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter1/Untitled%201-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter1/Untitled%201-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter1/Untitled%201-1400.webp"/> <img src="/assets/img/RL_chapter1/Untitled%201.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="22-奖励">2.2 奖励</h2> <p>奖励是由环境给的一种<strong>标量的反馈信号</strong>（scalar feedback signal），这种信号可显示智能体在某一步采 取某个策略的表现如何。强化学习的目的就是最大化智能体可以获得的奖励，智能体在环境里面存在的目的就是最大化它的期望的累积奖励（expected cumulative reward）。不同的环境中，奖励也是不同的。</p> <h2 id="23-序列决策">2.3 序列决策</h2> <p>在一个强化学习环境里面，智能体的目的就是选取一系列的动作来最大化奖励，所以这些选取的动作 必须有长期的影响。但在这个过程里面，智能体的奖励其实是被延迟了的，就是我们现在选取的某一步动作，可能要等到很久后才知道这一步到底产生了什么样的影响。强化学习里面一个重要的课题就是近期奖励和远期奖励的权衡 （trade-off），研究怎么让智能体取得更多的远期奖励。在与环境的交互过程中，智能体会获得很多观测。针对每一个观测，智能体会采取一个动作，也会得到一个奖励。</p> <ol> <li> <p><strong>历史</strong>是<strong>观测、动作、奖励</strong>的序列：</p> \[H_t=o_1, a_1, r_1, \ldots, o_t, a_t, r_t\] </li> <li> <p>智能体在采取当前动作的时候会依赖于它之前得到的历史，所以我们可以把整个游戏的<strong>状态</strong>看成关于这个<strong>历史</strong>的函数：</p> \[S_t=f\left(H_t\right)\] </li> <li>观测和状态的区别：<strong>状态</strong>是对世界的完整描述，不会隐藏世界的信息。<strong>观测</strong>是对状态的部分描述，可能会遗漏一些信息。在深度强化学习中，我们几乎总是用实值的向量、矩阵或者更高阶的张量来表示状态和观。</li> <li><strong>环境</strong>有自己的函数\(S_t^e=f^e\left(H_t\right)\) 来更新状态，在<strong>智能体</strong>的内部也有一个函数\(S_t^a=f^a\left(H_t\right)\)来更新状态。</li> <li>马尔可夫决策（Markov decision process，MDP）过程：当智能体的状态与环境的状态等价的时候，即当智能体能够观察到环境的所有状态时，我们称这个环境是完全可观测的（fully observed）。在这种情况下面，强化学习通常被建模成一个马尔可夫决策过程的问题。在马尔可夫决策过程中，\(O_t=S_t^e=S_t^a\)。</li> <li> <p>部分可观测马尔可夫决策过程（partially observable Markov decision process, POMDP）：智能体得到的观测并不一定能包含环境运作的所有状态，因为在强化学习的设定里面， 环境的状态才是真正的所有状态。当智能体只能看到部分的观测，我们就称这个环境是部分可观测的（partially observed）。 在这种情况下，强化学习通常被建模成部分可观测马尔可夫决策过程的问题。部分可观测马尔可夫决策过程是马尔可夫决策过程的一种泛化。 部分可观测马尔可夫决策过程依然具有马尔可夫性质，但是假设智能体无法感知环境的状态，只能知道部分观测值。部分观测值。比如在自动驾驶中，智能体只能感知传感器采集的有限的环境信息。</p> <p>😈 部分可观测马尔可夫决策过程可以用一个七元组描述：\((S, A, T, R, \Omega, O, \gamma)\)。其中\(S\)表示状态空间，为隐变量，\(A\)为动作空间，\(T\left(s^{\prime} \mid s, a\right)\)为状态转移概率，\(R\) 为奖励函数，\(\Omega(o \mid s, a)\)为观测概率,\(o\)为观测空间，\(\gamma\)为折扣系数。</p> </li> </ol> <h1 id="三动作空间">三、动作空间</h1> <p>不同的环境允许不同种类的动作。在给定的环境中，<strong>有效动作的集合</strong>经常被称为<strong>动作空间</strong>（action space）。像雅达利游戏和围棋（Go）这样的环境有<strong>离散动作空间</strong>（discrete action space），在这个动作 空间里，智能体的动作数量是有限的。在其他环境，比如在物理世界中控制一个智能体，在这个环境中就有<strong>连续动作空间</strong>（continuous action space）。在连续动作空间中，动作是实值的向量。</p> <p>例如，走迷宫机器人如果只有往东、往南、往西、往北这 4 种移动方式，则其动作空间为离散动作空 间；如果机器人可以向 360 ◦ 中的任意角度进行移动，则其动作空间为连续动作空间。</p> <h1 id="四强化学习智能体的组成成分和类型">四、强化学习智能体的组成成分和类型</h1> <p><strong>部分可观测马尔可夫决策过程( POMDP)</strong> 是一个马尔可夫决策过程的泛化。对于一个强化学习智能体，它可能有一个或多个如下的组成成分。</p> <ul> <li><strong>策略（policy）</strong>。智能体会用策略来选取下一步的<strong>动作</strong>。</li> <li><strong>价值函数（value function）</strong>。我们用价值函数来对当前状态进行评估。价值函数用于评估智能体进入某个状态后，可以对后面的<strong>奖励</strong>带来多大的影响。价值函数值越大，说明智能体进入这个状态越有 利。</li> <li><strong>模型（model）</strong>。模型表示智能体对环境的状态进行理解，它决定了环境中世界的运行方式。 下面我们深入了解这 3 个组成部分的细节。</li> </ul> <h2 id="41-策略">4.1 策略</h2> <p>策略是智能体的<strong>动作模型</strong>，它决定了智能体的动作。它其实是一个<strong>函数</strong>，用于把输入的状态变成动作。策略可分为两种：<strong>随机性策略</strong>和<strong>确定性策略</strong>。</p> <ol> <li><strong>随机性策略（stochastic policy）</strong>就是\(\pi\) 函数，即\(\pi(a \mid s)=p\left(a_t=a \mid s_t=s\right)\)。输入一个状态 \(s\)，输出一个概率。 这个概率是智能体<strong>所有动作的概率</strong>，然后对这个概率分布进行<strong>采样</strong>，可得到智能体将采取的动作。比如可能是有 0.7 的概率往左，0.3 的概率往右，那么通过采样就可以得到智能体将采取的动作。</li> <li><strong>确定性策略（deterministic policy）</strong>就是智能体直接采取<strong>最有可能</strong>的动作，即\(a^*=\underset{a}{\arg \max } \pi(a \mid s)\)。</li> </ol> <p>😍 通常情况下，强化学习一般使用<strong>随机性策略</strong>，随机性策略有很多优点。比如，在学习时可以通过引入一定的随机性来更好地探索环境； 随机性策略的动作具有多样性，这一点在多个智能体博弈时非常重要。采用确定性策略的智能体总是对同样的状态采取相同的动作，这会导致它的策略很容易被对手预测。</p> <h2 id="42-价值函数">4.2 价值函数</h2> <ol> <li> <p>价值函数的值是对未来奖励的预测，我们用它来评估状态的好坏。 价值函数里面有一个<strong>折扣因子（discount factor）</strong>，我们希望在尽可能短的时间里面得到尽可能多的奖励。”现在的钱以后就不值钱了“。</p> \[V_\pi(s) \doteq \mathbb{E}_\pi\left[G_t \mid s_t=s\right]=\mathbb{E}_\pi\left[\sum_{k=0}^{\infty} \gamma^k r_{t+k+1} \mid s_t=s\right], \forall s\in S\] <p>期望 \(\mathbb{E}_\pi\) 的下标是\(\pi\)函数，\(\pi\)函数的值可反映在我们使用策略\(\pi\)的时候，到底可以得到多少奖励。</p> </li> <li> <p>我们还有一种价值函数： \(Q\) 函数。 \(Q\) 函数里面包含两个变量：<strong>状态</strong>和<strong>动作</strong>。其定义为</p> \[Q_\pi(s, a) \doteq \mathbb{E}_\pi\left[G_t \mid s_t=s, a_t=a\right]=\mathbb{E}_\pi\left[\sum_{k=0}^{\infty} \gamma^k r_{t+k+1} \mid s_t=s, a_t=a\right]\] <p>未来可以获得奖励的期望取决于<strong>当前的状态</strong>和<strong>当前的动作</strong>。当我们得到 \(Q\) 函数后， 进入某个状态要采取的最优动作可以通过 \(Q\) 函数得到。</p> </li> </ol> <h2 id="43-模型">4.3 模型</h2> <p>模型决定了下一步的<strong>状态</strong>。下一步的状态取决于当前的状态以及当前采取的动作。它由<strong>状态转移概率</strong>和<strong>奖励函数</strong>两个部分组成。</p> <ol> <li> <p><strong>状态转移概率</strong>即</p> \[p_{s s^{\prime}}^a=p\left(s_{t+1}=s^{\prime} \mid s_t=s, a_t=a\right)\] </li> <li> <p><strong>奖励函数</strong>是指我们在当前状态采取了某个动作，可以得到多大的奖励，即</p> </li> </ol> \[R(s, a)=\mathbb{E}\left[r_{t+1} \mid s_t=s, a_t=a\right]\] <ol> <li><strong>马尔可夫决策过程（Markov decision process）</strong>这个决策过程可视化了状态之间的转移以及采取的动作。包含<strong>策略、价值函数和模型</strong></li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter1/Untitled%202-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter1/Untitled%202-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter1/Untitled%202-1400.webp"/> <img src="/assets/img/RL_chapter1/Untitled%202.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="44-强化学习智能体的类型">4.4 强化学习智能体的类型</h2> <ol> <li>基于价值的智能体与基于策略的智能体 根据智能体学习的事物不同，我们可以把智能体进行归类。 <ol> <li><strong>基于价值的智能体（value-based agent）</strong>显式地学习价值函数，隐式地学习它的策略。策略是其从学到的价值函数里面推算出来的。</li> <li><strong>基于策略的智能体（policy-based agent）</strong>直接学习策略，我们给它一个状态，它就会输出对应动作的概率。基于策略的智能体并没有学习价值函数。</li> <li>把基于价值的智能体和基于策略的智能体结合起来就有了<strong>演员-评论员智能体（actor-critic agent）</strong>。这一类智能体把策略和价值函数都学习了，然后通过两者的交互得到最佳的动作。</li> </ol> <p>😍 Q: 基于策略和基于价值的强化学习方法有什么区别?</p> <p>A: 对于一个状态转移概率已知的马尔可夫决策过程，我们可以使用动态规划算法来求解。从决策方式来看，强化学习又可以划分为基于策略的方法和基于价值的方法。决策方式是智能体在给定状态下从动作集合中选择一个动作的依据，它是静态的，不随状态变化而变化。</p> <p>在基于策略的强化学习方法中，智能体会制定一套动作策略（确定在给定状态下需要采取何种动作），并根据这个策略进行操作。强化学习算法直接对策略进行优化，使制定的策略能够获得最大的奖励。</p> <p>而在基于价值的强化学习方法中，智能体不需要制定显式的策略，它维护一个价值表格或价值函数，并通过这个价值表格或价值函数来选取价值最大的动作。基于价值迭代的方法只能应用在不连续的、离散的环境下（如围棋或某些游戏领域），对于动作集合规模庞大、动作连续的场景（如机器人控制领域），其很难学习到较好的结果（此时基于策略迭代的方法能够根据设定的策略来选择连续的动作）。</p> <p>基于价值的强化学习算法有Q学习（Q-learning）、 Sarsa 等，而基于策略的强化学习算法有策略梯度（Policy Gradient，PG）算法等。此外，演员-评论员算法同时使用策略和价值评估来做出决策。其中，智能体会根据策略做出动作，而价值函数会对做出的动作给出价值，这样可以在原有的策略梯度算法的基础上加速学习过程，取得更好的效果。</p> </li> <li>有模型强化学习： 智能体与免模型强化学习智能体另外，我们可以通过智能体到底有没有学习<strong>环境模型</strong>来对智能体进行分类。 <ol> <li><strong>有模型（model-based）</strong>强化学习智能体通过学习<strong>状态的转移</strong>来采取动作。 </li> <li><strong>免模型（model-free）</strong>强化学习智能体没有去直接估计状态的转移，也没有得到环境的具体转移变量，它通过<strong>学习价值函数和策略函数</strong>进行决策。免模型强化学习智能体的模型里面没有环境转移的模型。</li> </ol> <p>我们可以用马尔可夫决策过程来定义强化学习任务，并将其表示为四元组 \(&lt;S,A,P,R&gt;\)，即状态集合、动作集合、状态转移函数和奖励函数。如果这个四元组中所有元素均已知，且状态集合和动作集合在有限步数内是有限集，则智能体可以对真实环境进行建模，构建一个虚拟世界来模拟真实环境中的状态和交互反应。 具体来说，当智能体知道状态转移函数 \(P(s_{t+1}∣s_t,a_t)\) 和奖励函数 \(R(s_t,a_t)\) 后，它就能知道在某一状态下执行某一动作后能带来的奖励和环境的下一状态，这样智能体就不需要在真实环境中采取动作，直接在虚拟世界中学习和规划策略即可。这种学习方法称为<strong>有模型强化学习</strong>。 五、强化学习的基本问题：规划和学习</p> </li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter1/Untitled%203-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter1/Untitled%203-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter1/Untitled%203-1400.webp"/> <img src="/assets/img/RL_chapter1/Untitled%203.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h1 id="五强化学习基本问题">五、强化学习基本问题</h1> <h2 id="51-学习和规划">5.1 学习和规划</h2> <p>学习（learning）和规划（planning）是序列决策的两个基本问题。在强化学习中，<strong>环境</strong>初始时是未知的，<strong>智能体</strong>不知道环境如何工作，它通过不断地与环境交互，逐渐改进策略。</p> <ol> <li>在<strong>规划</strong>中，环境是已知的，智能体被告知了整个环境的运作规则的详细信息。智能体能够计算出一个完美的模型，并且在不需要与环境进行任何交互的时候进行计算。智能体不需要实时地与环境交互就能知道未来环境，只需要知道当前的状态，就能够开始思考，来寻找最优解。</li> <li>在<strong>学习</strong>中，规则是确定的，我们知道选择左之后环境将会产生什么变化。我们完全可以通过已知的规则，来在内部模拟整个决策过程，无需与环境交互。 一个常用的强化学习问题解决思路是，先学习环境如何工作，也就是了解环境工作的方式，即学习得到一个模型，然后利用这个模型进行规划。</li> </ol> <h2 id="52-探索和利用">5.2 探索和利用</h2> <p>在强化学习里面，探索和利用是两个很核心的问题。</p> <ol> <li><strong>探索</strong>即我们去探索环境，通过尝试不同的动作来得到最佳的策略（带来最大奖励的策略）。</li> <li><strong>利用</strong>即我们不去尝试新的动作，而是采取已知的可以带来很大奖励的动作。</li> </ol> <p>在刚开始的时候，强化学习智能体不知道它采取了某个动作后会发生什么，所以它只能通过试错去探索，所以探索就是通过试错来理解采取的动作到底可不可以带来好的奖励。利用是指我们直接采取已知的可以带来很好奖励的动作。所以这里就面临一个权衡问题，即怎么通过牺牲一些短期的奖励来理解动作，从而学习到更好的策略。</p>]]></content><author><name></name></author><category term="Reinforcement-Learning"/><category term="intro"/><summary type="html"><![CDATA[RL Overview]]></summary></entry><entry><title type="html">第二章 Markov Decision Process (马尔可夫决策过程)</title><link href="https://gszfwsb.github.io/blog/2023/RL-lecture2/" rel="alternate" type="text/html" title="第二章 Markov Decision Process (马尔可夫决策过程)"/><published>2023-10-26T00:00:00+00:00</published><updated>2023-10-26T00:00:00+00:00</updated><id>https://gszfwsb.github.io/blog/2023/RL-lecture2</id><content type="html" xml:base="https://gszfwsb.github.io/blog/2023/RL-lecture2/"><![CDATA[<p>强化学习中，智能体与环境就是这样进行交互的，这个交互过程可以通过马尔可夫决策过程来表示，所以马尔可夫决策过程是强化学习的基本框架。</p> <p>在介绍马尔可夫决策过程之前，我们先介绍它的简化版本：马尔可夫过程（Markov process，MP）以及马尔可夫奖励过程（Markov reward process，MRP）。通过与这两种过程的比较，我们可以更容易理解马尔可夫决策过程。其次，我们会介绍马尔可夫决策过程中的<strong>策略评估（policy evaluation）</strong>，就是当给定决策后，我们怎么去计算它的价值函数。最后，我们会介绍马尔可夫决策过程的控制，具体有<strong>策略迭代（policy iteration）</strong> 和<strong>价值迭代（value iteration）</strong>两种算法。在马尔可夫决策过程中，它的环境是全部可观测的。但是很多时候环境里面有些量是不可观测的，但是这个部分观测的问题也可以转换成马尔可夫决策过程的问题。</p> <h1 id="一马尔可夫过程">一、马尔可夫过程</h1> <p>MP定义</p> <ul> <li>\(S\) 有限状态集</li> <li>\(P^a\) 动作转移模型，\(P\left(s_{t+1}=s^{\prime} \mid s_t=s\right)\)</li> <li>折扣银子 \(\gamma \in[0,1]\)</li> <li>MDP 是一个元组: \((S, P, \gamma)\)。</li> </ul> <h2 id="11-马尔可夫性质">1.1 马尔可夫性质</h2> <p>在随机过程中，马尔可夫性质 (Markov property) 是指一个随机过程在给定现在状态及所有过去状态情况下，其末来状态的条件概率分布仅依赖于<strong>当前状态</strong>。以离散随机过程为例，假设随机变量 \(X_0, X_1, \cdots, X_T\) 构成一个随机过程。这些随机变量的所有可能取值的集合被称为状态空间 (state space) 。如果 \(X_{t+1}\) 对于过去状态的条件概率分布仅是 \(X_t\) 的一个函数，则</p> \[p\left(X_{t+1}=x_{t+1} \mid X_{0: t}=x_{0: t}\right)=p\left(X_{t+1}=x_{t+1} \mid X_t=x_t\right)\] <p>其中， \(X_{0: t}\) 表示变量集合 \(X_0, X_1, \cdots, X_t ， x_{0: t}\) 为在状态空间中的状态序列 \(x_0, x_1, \cdots, x_t\) 。马尔可夫性质也可以描述为给定当前状态时，将来的状态与过去状态是<strong>条件独立</strong>的。如果某一个过程满足马尔可夫性质，那么末来的转移与过去的是独立的，它只取决于现在。马尔可夫性质是 所有马尔可夫过程的基础。</p> <h2 id="12-马尔可夫链">1.2 马尔可夫链</h2> <p>马尔可夫过程是一组具有马尔可夫性质的随机变量序列 \(s_1, \cdots, s_t\) ，其中下一个时刻的状态 \(s_{t+1}\) 只取决于当前状态 \(s_t\) 。我们设状态的历史为 \(h_t=\left\{s_1, s_2, s_3, \ldots, s_t\right\}\) ( \(h_t\) 包含了之前的所有状态)，则马尔可夫过程满足条件:</p> \[p\left(s_{t+1} \mid s_t\right)=p\left(s_{t+1} \mid h_t\right)\] <p>从当前 \(s_t\) 转移到 \(s_{t+1}\) ，它是直接就等于它之前所有的状态转移到 \(s_{t+1}\) 。<strong>离散时间的马尔可夫过程</strong>也称为<strong>马尔可夫链</strong> (Markov chain) 。马尔可夫链是最简单的马尔可夫过程，其状态是有限的。</p> <p>我们可以用状态转移矩阵 (state transition matrix) \(\boldsymbol{P}\) 来描述状态转移 \(p\left(s_{t+1}=s^{\prime} \mid s_t=s\right)\) :</p> \[\boldsymbol{P}=\left(\begin{array}{cccc} p\left(s_1 \mid s_1\right) &amp; p\left(s_2 \mid s_1\right) &amp; \ldots &amp; p\left(s_N \mid s_1\right) \\ p\left(s_1 \mid s_2\right) &amp; p\left(s_2 \mid s_2\right) &amp; \ldots &amp; p\left(s_N \mid s_2\right) \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ p\left(s_1 \mid s_N\right) &amp; p\left(s_2 \mid s_N\right) &amp; \ldots &amp; p\left(s_N \mid s_N\right) \end{array}\right)\] <p>状态转移矩阵类似于条件概率 (conditional probability)，它表示当我们知道当前我们在状态 \(s_t\) 时，到达下面所有状态的概率。所以它的每一行描述的是从一个节点到达所有其他节点的概率。</p> <h1 id="二马尔可夫奖励过程">二、马尔可夫奖励过程</h1> <p>MRP定义</p> <ul> <li>\(S\) 有限状态集</li> <li>\(P\) 转移模型，\(P\left(s_{t+1}=s^{\prime} \mid s_t=s\right)\)</li> <li>\(R\) 奖励函数 \(R\left(s_t=s, \right)=\mathbb{E}\left[r_t \mid s_t=s\right]\)</li> <li>折扣银子 \(\gamma \in[0,1]\)</li> <li>MDP 是一个元组: \((S, P, R, \gamma)\)。</li> </ul> <h2 id="21-回报和价值函数">2.1 回报和价值函数</h2> <p>马尔可夫奖励过程（Markov reward process, MRP）是<strong>马尔可夫链</strong>加上<strong>奖励函数</strong>。在马尔可夫奖励过程中，状态转移矩阵和状态都与马尔可夫链一样，只是多了<strong>奖励函数（reward function）</strong>。奖励函数\(R\)是一个期望，表示当我们到达某一个状态的时候，可以获得多大的奖励。这里另外定义了折扣因子\(\gamma\)。如果状态数是有限的，那么\(R\)可以是一个向量。</p> <ol> <li>范围（horizon）：是指一个回合的长度（每个回合最大的时间步数），它是由有限个步数决定的。</li> <li> <p>回报（return）：可以定义为奖励的逐步叠加，时刻\(t\)后的奖励序列为：</p> \[G_t=r_{t+1}+\gamma r_{t+2}+\gamma^2 r_{t+3}+\gamma^3 r_{t+4}+\ldots+\gamma^{T-t-1} r_T\] <p>其中， \(T\) 是最终时刻， \(\gamma\) 是折扣因子，越往后得到的奖励，折扣越多。这说明我们更希望得到现有的奖励，对末来的奖励要打折扣。当我们有了回 报之后，就可以定义状态的价值了，就是状态价值函数（state-value function）。对于马尔可夫奖励过程，状态价值函数被定义成回报的期望，即</p> \[\begin{aligned} V_t(s) &amp; =\mathbb{E}\left[G_t \mid s_t=s\right] \\ &amp; =\mathbb{E}\left[r_{t+1}+\gamma r_{t+2}+\gamma^2 r_{t+3}+\ldots+\gamma^{T-t-1} r_T \mid s_t=s\right] \end{aligned}\] <p>其中， \(G_t\) 是之前定义的折扣回报（discounted return) 。我们对 \(G_t\) 取了一个期望，期望就是从这个状态开始，我们可能获得多大的价值。所以期望也可以看成末来可能获得奖励的当前价值的表现，就是当我们进入某一个状态后，我们现在有多大的价值。</p> <p>😍 为什么用折扣因子？</p> <p><strong>第一</strong>，有些马尔可夫过程是带环的，它并不会终结，我们想避免无穷的奖励。</p> <p><strong>第二</strong>，我们并不能建立完美的模拟环境的模型，我们对未来的评估不一定是准确的，我们不一定完全信任模型，因为这种不确定性，所以我们对未来的评估增加一个折扣。我们想把这个不确定性表示出来，希望尽可能快地得到奖励，而不是在未来某一个点得到奖励。</p> <p><strong>第三</strong>，如果奖励是有实际价值的，我们可能更希望立刻就得到奖励，而不是后面再得到奖励（现在的钱比以后的钱更有价值）。</p> <p><strong>最后</strong>，我们也更想得到即时奖励。有些时候可以把折扣因子设为 0，我们就只关注当前的奖励。我们也可以把折扣因子设为 1，，对未来的奖励并没有打折扣，未来获得的奖励与当前获得的奖励是一样的。折扣因子可以作为强化学习智能体的一个超参数来进行调整，通过调整折扣因子，我们可以得到不同动作的智能体。</p> </li> </ol> <h2 id="22-贝尔曼方程">2.2 贝尔曼方程</h2> <p>从价值函数里面推导出<strong>贝尔曼方程（Bellman equation）</strong>，先上结论：</p> \[V(s)=\underbrace{R(s)}_{\text {即时奖励 }}+\underbrace{\gamma \sum_{s^{\prime} \in S} p\left(s^{\prime} \mid s\right) V\left(s^{\prime}\right)}_{\text {末来奖励的折扣总和 }}\] <p>其中，</p> <ul> <li>\(s^{\prime}\) 可以看成末来的所有状态，</li> <li>\(p\left(s^{\prime} \mid s\right)\) 是指从当前状态转移到末来状态的概率。</li> <li>\(V\left(s^{\prime}\right)\) 代表的是末来某一个状态的价值。我们从当前状态开始，有一定的概率去到末来的所有状态，所以我们要把 \(p\left(s^{\prime} \mid s\right)\) 写上去。我们得到了末来状态后，乘一个 \(\gamma\) ，这样就可以把末来的奖励打折扣。</li> <li>\(\gamma \sum_{s^{\prime} \in S} p\left(s^{\prime} \mid s\right) V\left(s^{\prime}\right)\) 可以看成末来奖励的折扣总和 (discounted sum of future reward)。 贝尔曼方程定义了当前状态与末来状态之间的关系。末来奖励的折扣总和加上即时奖励，就组成了贝尔曼方程。</li> <li> <p>推导过程</p> <p>可以先证明：\(\mathbb{E}\left[V\left(s_{t+1}\right) \mid s_t\right]=\mathbb{E}\left[\mathbb{E}\left[G_{t+1} \mid s_{t+1}\right] \mid s_t\right]=\mathbb{E}\left[G_{t+1} \mid s_t\right]\)。令\(s=s_t, g^{\prime}=G_{t+1}, s^{\prime}=s_{t+1}\)</p> \[\begin{aligned}\mathbb{E}\left[\mathbb{E}\left[G_{t+1} \mid s_{t+1}\right] \mid s_t\right] &amp; =\mathbb{E}\left[\mathbb{E}\left[g^{\prime} \mid s^{\prime}\right] \mid s\right] \\&amp; =\mathbb{E}\left[\sum_{g^{\prime}} g^{\prime} p\left(g^{\prime} \mid s^{\prime}\right) \mid s\right] \\&amp; =\sum_{s^{\prime}} \sum_{g^{\prime}} g^{\prime} p\left(g^{\prime} \mid s^{\prime}, s\right) p\left(s^{\prime} \mid s\right) \\&amp; =\sum_{s^{\prime}} \sum_{g^{\prime}} \frac{g^{\prime} p\left(g^{\prime} \mid s^{\prime}, s\right) p\left(s^{\prime} \mid s\right) p(s)}{p(s)} \\&amp; =\sum_{s^{\prime}} \sum_{g^{\prime}} \frac{g^{\prime} p\left(g^{\prime} \mid s^{\prime}, s\right) p\left(s^{\prime}, s\right)}{p(s)} \\&amp; =\sum_{s^{\prime}} \sum_{g^{\prime}} \frac{g^{\prime} p\left(g^{\prime}, s^{\prime}, s\right)}{p(s)} \\&amp; =\sum_{s^{\prime}} \sum_{g^{\prime}} g^{\prime} p\left(g^{\prime}, s^{\prime} \mid s\right) \\&amp; =\sum_{g^{\prime}} \sum_{s^{\prime}} g^{\prime} p\left(g^{\prime}, s^{\prime} \mid s\right) \\&amp; =\sum_{g^{\prime}} g^{\prime} p\left(g^{\prime} \mid s\right) \\&amp; =\mathbb{E}\left[g^{\prime} \mid s\right]=\mathbb{E}\left[G_{t+1} \mid s_t\right]\end{aligned}\] \[\begin{aligned}V(s) &amp; =\mathbb{E}\left[G_t \mid s_t=s\right] \\&amp; =\mathbb{E}\left[R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+\ldots \mid s_t=s\right] \\&amp; =\mathbb{E}\left[R_{t+1} \mid s_t=s\right]+\gamma \mathbb{E}\left[R_{t+2}+\gamma R_{t+3}+\gamma^2 R_{t+4}+\ldots \mid s_t=s\right] \\&amp; =R(s)+\gamma \mathbb{E}\left[G_{t+1} \mid s_t=s\right] \\&amp; =R(s)+\gamma \mathbb{E}\left[V\left(s_{t+1}\right) \mid s_t=s\right] \\&amp; =R(s)+\gamma \sum_{s^{\prime} \in S} p\left(s^{\prime} \mid s\right) V\left(s^{\prime}\right)\end{aligned}\] </li> </ul> <p>可以把贝尔曼方程写成矩阵的形式：</p> \[\left(\begin{array}{c}V\left(s_1\right) \\V\left(s_2\right) \\\vdots \\V\left(s_N\right)\end{array}\right)=\left(\begin{array}{c}R\left(s_1\right) \\R\left(s_2\right) \\\vdots \\R\left(s_N\right)\end{array}\right)+\gamma\left(\begin{array}{cccc}p\left(s_1 \mid s_1\right) &amp; p\left(s_2 \mid s_1\right) &amp; \ldots &amp; p\left(s_N \mid s_1\right) \\p\left(s_1 \mid s_2\right) &amp; p\left(s_2 \mid s_2\right) &amp; \ldots &amp; p\left(s_N \mid s_2\right) \\\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\p\left(s_1 \mid s_N\right) &amp; p\left(s_2 \mid s_N\right) &amp; \ldots &amp; p\left(s_N \mid s_N\right)\end{array}\right)\left(\begin{array}{c}V\left(s_1\right) \\V\left(s_2\right) \\\vdots \\V\left(s_N\right)\end{array}\right)\] <p>每一行来看，向量\(V\)乘状态转移矩阵里面的某一行，再加上它当前可以得到的奖励，就会得到它当前的价值。当我们把贝尔曼方程写成矩阵形式后，可以直接求解：</p> \[\begin{aligned}&amp; \boldsymbol{V}=\boldsymbol{R}+\gamma \boldsymbol{P} \boldsymbol{V} \\&amp; \boldsymbol{V}=(\boldsymbol{I}-\gamma \boldsymbol{P})^{-1} \boldsymbol{R} \end{aligned}\] <p>但是求解复杂度是\(O(N^3)\)。求解算法有<strong>蒙特卡洛</strong>、<strong>动态规划</strong>和<strong>时序差分学习</strong></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter2/Untitled-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter2/Untitled-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter2/Untitled-1400.webp"/> <img src="/assets/img/RL_chapter2/Untitled.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter2/Untitled%201-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter2/Untitled%201-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter2/Untitled%201-1400.webp"/> <img src="/assets/img/RL_chapter2/Untitled%201.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h1 id="三马尔可夫决策过程">三、马尔可夫决策过程</h1> <p>MDP定义</p> <ul> <li>\(S\) 有限状态集</li> <li>\(A\) 有限动作集</li> <li>\(P^a\) 动作转移模型，\(P\left(s_{t+1}=s^{\prime} \mid s_t=s, a_t=a\right)\)</li> <li>\(R\) 奖励函数 \(R\left(s_t=s, a_t=a\right)=\mathbb{E}\left[r_t \mid s_t=s, a_t=a\right]\)</li> <li>折扣因子 \(\gamma \in[0,1]\)</li> <li>MDP 是一个元组: \((S, A, P, R, \gamma)\)。</li> </ul> <h2 id="31-策略">3.1 策略</h2> <ol> <li> <p>策略定义了在某一个状态应该采取什么样的动作。知道当前状态后，我们可以把当前状态代入策略函数来得到一个概率，即</p> \[\pi(a \mid s)=p\left(a_t=a \mid s_t=s\right)\] <p>概率代表在所有可能的动作里面怎样采取行动，比如可能有 0.7 的概率往左走，有 0.3 的概率往右走，这是一个概率的表示。另外策略也可能是确定的，它有可能直接输出一个值，或者直接告诉我们当前应该采取什么样的动作，而不是一个动作的概率。假设概率函数是平稳的（stationary），不同时间点，我们采取的动作其实都是在对策略函数进行采样。</p> </li> <li> <p>已知马尔可夫决策过程和策略 \(\pi\) ，我们可以把马尔可夫决策过程转换成马尔可夫奖励过程。在马尔可夫决策过程里面，状态转移函\(P(s′∣s,a)\) 基于它当前的状态以及它当前的动作。因为我们现在已知策略函数，也就是已知在每一个状态下，可能采取的动作的概率，所以我们就可以直接把动作进行加和，marginalize \(a\)，这样我们就可以得到对于马尔可夫奖励过程的转移，这里就没有动作，</p> \[P_\pi\left(s^{\prime} \mid s\right)=\sum_{a \in A} \pi(a \mid s) p\left(s^{\prime} \mid s, a\right)\] <p>对于奖励函数，我们也可以把动作去掉，这样就会得到类似于马尔可夫奖励过程的奖励函数，即</p> \[r_\pi(s)=\sum_{a \in A} \pi(a \mid s) R(s, a)\] </li> </ol> <h2 id="32-同过程奖励过程的区别">3.2 同过程、奖励过程的区别</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter2/Untitled%202-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter2/Untitled%202-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter2/Untitled%202-1400.webp"/> <img src="/assets/img/RL_chapter2/Untitled%202.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ol> <li>马尔可夫过程/马尔可夫奖励过程的状态转移是直接决定的。比如当前状态是\(s\)，那么直接通过转移概率决定下一个状态是什么。</li> <li>但对于马尔可夫决策过程，它的中间多了一层动作\(a\), 即智能体在当前状态的时候，首先要决定采取某一种动作，这样我们会到达某一个黑色的节点。到达这个黑色的节点后，因为有一定的不确定性，所以当智能体当前状态以及智能体当前采取的动作决定过后，智能体进入未来的状态其实也是一个概率分布。在当前状态与未来状态转移过程中多了一层决策性，这是马尔可夫决策过程与之前的马尔可夫过程/马尔可夫奖励过程很不同的一点。在马尔可夫决策过程中，动作是由智能体决定的，智能体会采取动作来决定未来的状态转移。</li> </ol> <h2 id="33-价值函数">3.3 价值函数</h2> <p>马尔可夫决策过程中的价值函数可定义为</p> \[V_\pi(s)=\mathbb{E}_\pi\left[G_t \mid s_t=s\right]\] <p>其中，期望基于我们采取的策略。当策略决定后，我们通过对策略进行采样来得到一个期望，计算出它的价值函数。这里我们另外引入了一个 <strong>Q 函数（Q-function）</strong>。Q 函数也被称为<strong>动作价值函数（action-value function）</strong>。Q 函数定义的是在某一个状态采取某一个动作，它有可能得到的回报的一个期望，即</p> \[Q_\pi(s, a)=\mathbb{E}_\pi\left[G_t \mid s_t=s, a_t=a\right]\] <p>这里的期望其实也是基于策略函数的。所以我们需要对策略函数进行一个加和，然后得到它的价值。 对 Q 函数中的动作进行加和，就可以得到价值函数：</p> \[V_\pi(s)=\sum_{a \in A} \pi(a \mid s) Q_\pi(s, a)\] <h2 id="34-贝尔曼期望方程">3.4 贝尔曼期望方程</h2> <p>我们可以把状态价值函数和 Q 函数拆解成两个部分：即时奖励和后续状态的折扣价值（discounted value of successor state）。 通过对价值函数进行分解，我们就可以得到一个类似于之前马尔可夫奖励过程的贝尔曼方程————<strong>贝尔曼期望方程（Bellman expectation equation）</strong></p> \[V_\pi(s)=\mathbb{E}_\pi\left[r_{t+1}+\gamma V_\pi\left(s_{t+1}\right) \mid s_t=s\right]\] \[V_\pi(s)=\sum_{a \in A} \pi(a \mid s)\left(R(s, a)+\gamma \sum_{s^{\prime} \in S} p\left(s^{\prime} \mid s, a\right) V_\pi\left(s^{\prime}\right)\right)\] <ul> <li> <p>推导：</p> \[\begin{aligned}Q(s, a) &amp; =\mathbb{E}\left[G_t \mid s_t=s, a_t=a\right] \\&amp; =\mathbb{E}\left[r_{t+1}+\gamma r_{t+2}+\gamma^2 r_{t+3}+\ldots \mid s_t=s, a_t=a\right] \\&amp; =\mathbb{E}\left[r_{t+1} \mid s_t=s, a_t=a\right]+\gamma \mathbb{E}\left[r_{t+2}+\gamma r_{t+3}+\gamma^2 r_{t+4}+\ldots \mid s_t=s, a_t=a\right] \\&amp; =R(s, a)+\gamma \mathbb{E}\left[G_{t+1} \mid s_t=s, a_t=a\right] \\&amp; =R(s, a)+\gamma \mathbb{E}\left[V\left(s_{t+1}\right) \mid s_t=s, a_t=a\right] \\&amp; =R(s, a)+\gamma \sum_{s^{\prime} \in S} p\left(s^{\prime} \mid s, a\right) V\left(s^{\prime}\right)\end{aligned}\] </li> </ul> <p>对于 Q 函数，我们也可以做类似的分解，得到 Q 函数的贝尔曼期望方程：</p> \[Q_\pi(s, a)=\mathbb{E}_\pi\left[r_{t+1}+\gamma Q_\pi\left(s_{t+1}, a_{t+1}\right) \mid s_t=s, a_t=a\right]\] \[Q_\pi(s, a)=R(s, a)+\gamma \sum_{s^{\prime} \in S} p\left(s^{\prime} \mid s, a\right) \sum_{a^{\prime} \in A} \pi\left(a^{\prime} \mid s^{\prime}\right) Q_\pi\left(s^{\prime}, a^{\prime}\right)\\=R(s, a)+\gamma \sum_{s^{\prime} \in S} p\left(s^{\prime} \mid s, a\right)\textcolor{orange}{V_\pi (s')}\] <p>💡 这里可以通过V函数求出Q函数！</p> <h2 id="35-备份图">3.5 备份图</h2> <p>对于某一个状态，它的当前价值是与它的未来价值线性相关的。 我们称为<strong>备份图（backup diagram）</strong>或回溯图，因为它们所示的关系构成了更新或备份操作的基础，而这些操作是强化学习方法的核心。这些操作将价值信息从一个状态（或状态-动作对）的后继状态（或状态-动作对）转移回它。 每一个空心圆圈代表一个状态，每一个实心圆圈代表一个状态-动作对。</p> <ol> <li> <p>对于V-function：</p> \[v_\pi(s)=\sum_{a \in A} \pi(a \mid s)\left(R(s, a)+\gamma \sum_{s^{\prime} \in S} P\left(s^{\prime} \mid s, a\right) v_\pi\left(s^{\prime}\right)\right)\] </li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter2/Untitled%203-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter2/Untitled%203-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter2/Untitled%203-1400.webp"/> <img src="/assets/img/RL_chapter2/Untitled%203.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ol> <li> <p>对于Q-function</p> \[q_\pi(s, a)=R(s, a)+\gamma \sum_{s^{\prime} \in S} P\left(s^{\prime} \mid s, a\right) \sum_{a^{\prime} \in A} \pi\left(a^{\prime} \mid s^{\prime}\right) q_\pi\left(s^{\prime}, a^{\prime}\right)\] </li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter2/Untitled%204-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter2/Untitled%204-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter2/Untitled%204-1400.webp"/> <img src="/assets/img/RL_chapter2/Untitled%204.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="36-策略评估">3.6 策略评估</h2> <p>已知马尔可夫决策过程以及要采取的策略\(\pi\)，计算价值函数\(V_\pi(s)\)的过程就是<strong>策略评估（价值预测）</strong>。也就是预测我们当前采取的策略最终会产生多少价值。</p> <p>我们可以直接通过贝尔曼期望方程来得到价值函数：</p> \[V^\pi_t(s)=R_\pi(s)+\gamma \sum_{s^{\prime} \in S} p\left(s^{\prime} \mid s, \pi(s)\right) V^\pi_{t-1}\left(s^{\prime}\right)\] <p>我们可以不停用贝尔曼期望方程迭代，最后价值函数会收敛。收敛之后，价值函数的值就是每一个状态的价值。</p> <h2 id="37-预测和控制">3.7 预测和控制</h2> <p>预测 (prediction) 和控制 (control) 是马尔可夫决策过程里面的核心问题。</p> <ol> <li><strong>预测</strong>（评估一个给定的策略) : 预测是指给定一个马尔可夫决策过程以及一个策略 \(\pi\) ，计算它的<strong>价值函数</strong>，也就是<strong>计算每个状态的价值</strong>。 <ol> <li>输入是马尔可夫决策过程 \(&lt;S, A, P, R, \gamma&gt;\) 和策略 \(\pi\) ，</li> <li>输出是价值函数 \(V_\pi\) 。</li> </ol> </li> <li><strong>控制</strong> (搜索最佳策略) : 控制就是我们去<strong>寻找一个最佳的策略</strong>，然后同时<strong>输出它的最佳价值函数</strong>以及最<strong>佳策略</strong>。 <ol> <li>输入是马尔可夫决策过程 \(&lt;S, A, P, R, \gamma&gt;\) ，</li> <li>输出是<strong>最佳价值函数</strong> (optimal value function) \(V^*\)和<strong>最佳策略</strong> (optimal policy） <em>\(\pi^*\)</em> 。</li> </ol> </li> </ol> <p>😍 在马尔可夫决策过程里面，预测和控制都可以通过动态规划解决。要强调的是，这两者的区别就在于，预测问题是给定一个策略，我们要确定它的价值函数是多少。而控制问题是在没有策略的前提下，我们要确定最佳的价值函数以及对应的决策方案。实际上，这两者是递进的关系，在强化学习中，我们<strong>通过解决预测问题，进而解决控制问题</strong>。</p> <h2 id="38-预测策略评估">3.8 预测（策略评估）</h2> <p>策略评估就是给定马尔可夫决策过程和策略，评估我们可以获得多少价值，即对于当前策略，我们可以得到多大的价值。我们可以直接把<strong>贝尔曼期望备份（Bellman expectation backup）</strong> ，变成迭代的过程，反复迭代直到收敛。这个迭代过程可以看作<strong>同步备份（synchronous backup）</strong> 的过程。</p> \[V_{t+1}(s)=\sum_{a \in A} \pi(a \mid s)\left(R(s, a)+\gamma \sum_{s^{\prime} \in S} p\left(s^{\prime} \mid s, a\right) V_t\left(s^{\prime}\right)\right)\] <p>策略评估的核心思想就是把如式所示的贝尔曼期望备份反复迭代，然后得到一个收敛的价值函数的值。因为已经给定了<strong>策略函数</strong>，所以我们可以直接把它简化成一个<strong>马尔可夫奖励过程</strong>的表达形式，相当于把\(a\)去掉，即</p> \[V_{t+1}(s)=r_\pi(s)+\gamma p_\pi\left(s^{\prime} \mid s\right) V_t\left(s^{\prime}\right)\] <h2 id="39-控制">3.9 控制</h2> <p>策略评估是指给定马尔可夫决策过程和策略，我们可以估算出价值函数的值。如果我们只有马尔可夫决策过程，那么应该如何寻找最佳的策略，从而得到<strong>最佳价值函数（optimal value function）</strong>呢？</p> <p>最佳价值函数的定义为：</p> \[V^*(s)=\max _\pi V_\pi(s)\] \[\pi^*(s)=\underset{\pi}{\arg \max } V_\pi(s)\] <p>当取得最佳价值函数后，我们可以通过对 \(Q\) 函数进行最大化来得到最佳策略（这里是deterministic的！）：</p> \[\pi^*(a \mid s)= \begin{cases}1, &amp; a=\underset{a \in A}{\arg \max } Q^*(s, a) \\ 0, &amp; \text { 其他 }\end{cases}\] <p>当\(Q\)函数收敛后，因为\(Q\)函数是关于状态与动作的函数，所以如果在某个状态采取某个动作，可以使得\(Q\)函数最大化，那么这个动作就是最佳的动作。如果我们能优化出一个 \(Q\) 函数\(Q^*(s,a)\)，就可以直接在\(Q\)函数中取一个让\(Q\)函数值最大化的动作的值，就可以提取出最佳策略。</p> <p>我们可以通过策略迭代和价值迭代来解决马尔可夫决策过程的控制问题。</p> <h2 id="310-策略迭代">3.10 策略迭代</h2> <ol> <li>策略迭代由两个步骤组成：<strong>策略评估</strong>和<strong>策略改进</strong>（policy improvement）。 <ol> <li>策略评估：当前我们在优化策略\(\pi\)，在优化过程中得到一个最新的策略。我们先保证这个策略不变，然后估计它的价值，即给定当前的策略函数来估计状态价值函数。 通过<strong>贝尔曼期望方程</strong>迭代，得到价值函数\(V_{\pi_i}\)。</li> <li> <p>策略改进：得到价值函数后，我们可以进一步推算出它的 \(Q\) 函数。得到 \(Q\) 函数后，我们直接对 \(Q\) 函数进行最大化，通过在 \(Q\) 函数做一个贪心的搜索来进一步改进策略。这两个步骤一直在迭代进行。在策略迭代里面，在初始化的时候，我们有一个初始化的状态价值函数\(V\)和策略\(\pi\)，然后在这两个步骤之间迭代。</p> \[Q_{\pi_i}(s, a)=R(s, a)+\gamma \sum_{s^{\prime} \in S} p\left(s^{\prime} \mid s, a\right) V_{\pi_i}\left(s^{\prime}\right)\] <p>对于每个状态，策略改进会得到它的新一轮的策略，对于每个状态，我们取使它得到最大值的动作，即</p> \[\pi_{i+1}(s)=\underset{a}{\arg \max } Q_{\pi_i}(s, a)\] </li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> </figure></div></div> </li> </ol> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter2/Untitled%205-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter2/Untitled%205-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter2/Untitled%205-1400.webp"/> <img src="/assets/img/RL_chapter2/Untitled%205.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <p>&lt;/figure&gt;</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    &lt;/div&gt;
&lt;/div&gt;
    
上面的线就是我们当前状态价值函数的值，下面的线是策略的值。 策略迭代的过程与踢皮球一样。我们先给定当前已有的策略函数，计算它的状态价值函数。算出状态价值函数后，我们会得到一个 Q 函数。我们对Q 函数采取贪心的策略，这样就像踢皮球，“踢”回策略。然后进一步改进策略，得到一个改进的策略后，它还不是最佳的策略，我们再进行策略评估，又会得到一个新的价值函数。基于这个新的价值函数再进行 Q 函数的最大化，这样逐渐迭代，状态价值函数和策略就会收敛。
</code></pre></div></div> <ol> <li> <p>贝尔曼最优方程: 当我们一直采取 argmax 操作的时候，我们会得到一个单调的递增。通过采取这种贪心操作（argmax 操作），我们就会得到更好的或者不变的策略，而不会使价值函数变差。所以当改进停止后，我们就会得到一个最佳策略。当改进停止后，我们取让 Q 函数值最大化的动作，Q 函数就会直接变成价值函数，即</p> \[Q_\pi\left(s, \pi^{\prime}(s)\right)=\max _{a \in A} Q_\pi(s, a)=Q_\pi(s, \pi(s))=V_\pi(s)\] \[V_\pi(s)=\max _{a \in A} Q_\pi(s, a)\] <p>💡 这里因为\(V_\pi(s)=\sum_{a \in A} \pi(a \mid s) Q_\pi(s, a)\)，而\(\pi(a \mid s)\)是只有\(a=\pi^*(s)\)为1</p> <p>贝尔曼最优方程表明：最佳策略下的一个状态的价值必须等于在这个状态下采取最好动作得到的回报的期望。 当马尔可夫决策过程满足贝尔曼最优方程的时候，整个马尔可夫决策过程已经达到最佳的状态。</p> <p>只有当整个状态已经收敛后，我们得到最佳价值函数后，贝尔曼最优方程才会满足。满足贝尔曼最优方程后，我们可以采用最大化操作，即</p> \[\begin{aligned} Q^*(s, a) &amp; =R(s, a)+\gamma \sum_{s^{\prime} \in S} p\left(s^{\prime} \mid s, a\right) V^*\left(s^{\prime}\right) \\ &amp; =R(s, a)+\gamma \sum_{s^{\prime} \in S} p\left(s^{\prime} \mid s, a\right) \max _a Q^*\left(s^{\prime}, a^{\prime}\right) \end{aligned}\] </li> </ol> <h2 id="311-价值迭代">3.11 价值迭代</h2> <ol> <li> <p>最优性原理</p> <p>我们从另一个角度思考问题，动态规划的方法将优化问题分成两个部分。第一步执行的是最优的动作。之后后继的状态的每一步都按照最优的策略去做，最后的结果就是最优的。</p> </li> <li>最优性原理定理（principle of optimality theorem）： 一个策略\(π(a∣s)\) 在状态 \(s\) 达到了最优价值，也就是 \(V^π(s)=V^∗(s)\) 成立，当且仅当对于任何能够从 \(s\) 到达的 \(s'\)，都已经达到了最优价值。也就是对于所有的\(s'\)，\(V^π(s')=V^∗(s')\)  恒成立。</li> <li> <p>确认性价值迭代</p> <p>如果我们知道子问题 \(V^∗(s')\) 的最优解，就可以通过价值迭代来得到最优的\(V^∗(s)\)的解。价值迭代就是把贝尔曼最优方程当成一个更新规则来进行，即</p> \[V(s) \leftarrow \max _{a \in A}\left(R(s, a)+\gamma \sum_{s^{\prime} \in S} p\left(s^{\prime} \mid s, a\right) V\left(s^{\prime}\right)\right)\] <p>只有当整个马尔可夫决策过程已经达到最佳的状态时，式才满足。但我们可以把它转换成一个备份的等式。备份的等式就是一个迭代的等式。我们不停地迭代贝尔曼最优方程，价值函数就能逐渐趋向于最佳的价值函数，这是价值迭代算法的精髓。</p> <p>为了得到最佳的\(V^*\) ，对于每个状态的 \(V\)，我们直接通过贝尔曼最优方程进行迭代，迭代多次之后，价值函数就会收敛。这种价值迭代算法也被称为<strong>确认性价值迭代</strong>（deterministic value iteration）。</p> </li> <li>具体算法：价值迭代算法的过程如下。 <ul> <li>初始化: 令 \(k=1\)，对于所有状态 \(s ， V_0(s)=0\) 。</li> <li>对于 \(k=1: H\) ( \(H\) 是让 \(V(s)\) 收敛所需的迭代次数) <ul> <li> <p>对于所有状态 \(s\)</p> \[\begin{gathered} Q_{k+1}(s, a)=R(s, a)+\gamma \sum_{s^{\prime} \in S} p\left(s^{\prime} \mid s, a\right) V_k\left(s^{\prime}\right) \\ V_{k+1}(s)=\max_a Q_{k+1}(s, a) \end{gathered}\] </li> <li> \[k \leftarrow k+1\] </li> </ul> </li> <li> <p>在迭代后提取最优策略:</p> \[\pi(s)=\underset{a}{\arg \max }\left[R(s, a)+\gamma \sum_{s^{\prime} \in S} p\left(s^{\prime} \mid s, a\right) V_{H+1}\left(s^{\prime}\right)\right]\] </li> </ul> </li> </ol> <h2 id="312-对比策略迭代和价值迭代">3.12 对比策略迭代和价值迭代</h2> <p>这两个算法都可以解马尔可夫决策过程的控制问题。</p> <ol> <li>策略迭代分两步。 <ol> <li>首先进行策略评估，即对当前已经搜索到的策略函数进行估值，通过迭代计算出价值函数\(V\)</li> <li>得到估值后，我们进行策略改进，即把 \(Q\) 函数算出来，进行进一步改进。不断重复这两步，直到策略收敛。</li> </ol> </li> <li>价值迭代直接使用<strong>贝尔曼最优方程</strong>进行迭代，从而寻找最佳的价值函数。找到最佳价值函数后，我们再提取一次最佳策略（一旦价值函数是最优的，策略也是最优的）。</li> </ol> <h2 id="313-总结">3.13 总结</h2> <table> <thead> <tr> <th>问题</th> <th>贝尔曼方程</th> <th>算法</th> </tr> </thead> <tbody> <tr> <td>预测</td> <td>贝尔曼期望方程</td> <td>策略评估</td> </tr> <tr> <td>控制</td> <td>贝尔曼期望方程</td> <td>策略迭代</td> </tr> <tr> <td>控制</td> <td>贝尔曼最优方程</td> <td>价值迭代</td> </tr> </tbody> </table>]]></content><author><name></name></author><category term="Reinforcement-Learning"/><category term="intro"/><summary type="html"><![CDATA[RL Overview]]></summary></entry></feed>