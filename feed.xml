<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://gszfwsb.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://gszfwsb.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-03-06T14:14:33+00:00</updated><id>https://gszfwsb.github.io/feed.xml</id><title type="html">Shaobo Wang</title><subtitle>Shaobo Wang&apos;s Homepage. </subtitle><entry><title type="html">第六章 Policy Optimization State of the art (策略优化进阶篇)</title><link href="https://gszfwsb.github.io/blog/2023/RL-lecture6/" rel="alternate" type="text/html" title="第六章 Policy Optimization State of the art (策略优化进阶篇)"/><published>2023-11-01T00:00:00+00:00</published><updated>2023-11-01T00:00:00+00:00</updated><id>https://gszfwsb.github.io/blog/2023/RL-lecture6</id><content type="html" xml:base="https://gszfwsb.github.io/blog/2023/RL-lecture6/"><![CDATA[<blockquote> <p>上次介绍了策略优化的基础方法，本次介绍一些SOTA的策略优化方法 关于PG算法，有很多变种，这次会介绍。此外，策略优化的相关工作主要有两条线，</p> <ol> <li>PG → Natural PG / TRPO → ACKTR → PPO</li> <li>Q-learning → DDPG → TD3 → SAC</li> </ol> </blockquote> <h1 id="一策略梯度算法变种概览">一、策略梯度算法变种概览</h1> <ol> <li> <p>策略函数有很多种形式:</p> \[\begin{aligned}\nabla_\theta J(\theta) &amp; =\mathbb{E}_{\pi_\theta}\left[\nabla_\theta \log \pi_\theta(s, a) G_t\right] \text { - REINFORCE } \\&amp; =\mathbb{E}_{\pi_\theta}\left[\nabla_\theta \log \pi_\theta(s, a) Q^w(s, a)\right] \text { - Q Actor-Critic } \\&amp; =\mathbb{E}_{\pi_\theta}\left[\nabla_\theta \log \pi_\theta(s, a) A^w(s, a)\right] \text { - Advantage Actor-Critic } \\&amp; =\mathbb{E}_{\pi_\theta}\left[\nabla_\theta \log \pi_\theta(s, a) \delta\right] \text { - TD Actor-Critic }\end{aligned}\] </li> <li>评论员用策略评估（MC或TD）去估计\(Q^\pi(s, a), A^\pi(s, a), \text { or } V^\pi(s)\)</li> <li>两条线的工作：</li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter6/Untitled-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter6/Untitled-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter6/Untitled-1400.webp"/> <img src="/assets/img/RL_chapter6/Untitled.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h1 id="二策略优化路线pg--natural-pg--trpo--acktr--ppo">二、策略优化路线：PG → Natural PG / TRPO → ACKTR → PPO</h1> <h2 id="21-策略梯度的问题">2.1 策略梯度的问题</h2> <ol> <li> <p>采样效率很低：是on-policy learning</p> \[\nabla_\theta J(\theta)=\mathbb{E}_{a \sim \pi_\theta}\left[\nabla_\theta \log \pi_\theta(s, a) r(s, a)\right]\] </li> <li>比较大尺度的策略更新或者不太合适的步长都会摧毁训练过程： <ol> <li>和监督学习不一样，数据和学习是独立的。</li> <li>在RL中，如果步子太远，会得到比较差的策略，于是就会收集比较差的数据。</li> <li>如果得到了比较差的策略，很难从中恢复一个好的策略，这样就会影响整体的性能。</li> </ol> </li> <li>如何让训练过程更稳定？用TRPO和Natural PG</li> <li>如何让其训练类似一个off-policy优化？用TRPO中的重要性采样</li> </ol> <h2 id="22-natural-pg">2.2 Natural PG</h2> <p>PG方法是直接在参数空间中选择最陡的坡来优化，缺点在于这对策略函数十分敏感。</p> \[d^*=\nabla_\theta J(\theta)=\lim _{\epsilon \rightarrow 0} \frac{1}{\epsilon} \arg \max J(\theta+d), \text { s.t. }\|d\| \leq \epsilon\] <p>在分布空间（策略输出）中最陡的方向可以通过KL散度做约束：</p> \[d^*=\arg \max J(\theta+d) \text {, s.t. } K L\left(\pi_\theta \| \pi_{\theta+d}\right)=c\] <p>固定KL散度为一个常数\(c\)使得我们可以在参数空间中优化的速度是一个常数。KL散度可以衡量两个分布之间的差异：</p> \[K L\left(\pi_\theta \| \pi_{\theta^{\prime}}\right)=E_{\pi_\theta}\left[\log \pi_\theta\right]-E_{\pi_\theta}\left[\log \pi_{\theta^{\prime}}\right]\] <p>尽管KL散度是非对称的并且不是真正的metric，我们也可以适用。因为如果\(d\to 0\)，KL散度是渐进对称的。因此，在邻域内，KL散度是近似对称的。可以证明KL散度的二阶泰勒展开是：</p> \[K L\left(\pi_\theta \| \pi_{\theta+d}\right) \approx \frac{1}{2} d^T F d\] <p>其中<strong>F</strong>是<strong>Fisher Information Matrix，</strong>是KL散度\(E_{\pi_\theta}\left[\nabla \log \pi_\theta \nabla \log \pi_\theta^T\right]\)的二阶导。</p> <p>我们用拉格朗日乘子重新写一下上述形式，然后再借助泰勒展开近似：</p> \[d^*=\arg\max_d J(\theta +d)-\lambda (KL(\pi)\theta \| \pi_{\theta+d})-c)\\ \approx \arg\max_d J(\theta) + \nabla_\theta J(\theta)^Td - \frac12 \lambda d^TFd + \lambda c\] <p>求导我们就能得到 natural policy gradient: \(d=\frac1\lambda F^{-1}\nabla_\theta J(\theta)\)</p> <h3 id="npg的性质">NPG的性质</h3> <ol> <li> <p>second-order优化，更精确，而且和模型无关：</p> \[\theta_{t+1}=\theta_t + \alpha F^{-1} \nabla_\theta J(\theta)\] <p>其中\(F=E_{\pi_\theta}\left[\nabla \log \pi_\theta \nabla \log \pi_\theta^T\right]\)是fisher information matrix，衡量了policy distribution的曲率</p> </li> <li> <p>不管模型怎么参数化，NPG都会产生一样的策略变化，因为\(F\)是固定的。</p> </li> </ol> <h3 id="重要性采样">重要性采样</h3> <p>我们可以通过重要性采样，把PG变成一个off-policy的学习，有点类似于ELBO推导。</p> <p>Importance sampling (IS) 计算\(f(x), x \sim p(x)\)的期望，如果我们不知道\(p\)，可以通过另一个分布\(q\)来做采样，</p> \[\mathbb{E}_{x\sim p}[f(x)]=\int p(x)f(x)dx = \int q(x)\frac{p(x)}{q(x)}f(x)dx = \mathbb{E}_{x\sim q}[\frac{p(x)}{q(x)}f(x)]\] <p>这样我们可以用IS来获得我们的目标函数，其中\(\hat\pi\)就是行为策略。</p> \[J(\theta)=\mathbb{E}_{a\sim \pi_\theta}[r(s,a)]=\mathbb{E}_{a\sim \hat{\pi}}[\frac{\pi_\theta(s,a)}{\hat{\pi}(s,a)}r(s,a)]\] <p>借助了IS的思想，我们准备直接用old policy作为行为策略，因此我们可以定义一个objective function为：</p> \[\theta = \arg\max_\theta J_{\theta_{old}}(\theta)=\arg\max \mathbb{E}_t [\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}R_t]\] <table> <tbody> <tr> <td>存在一个问题：如果$$\frac{\pi_\theta(a_t</td> <td>s_t)}{\pi_{\theta_{old}}(a_t</td> <td>s_t)}$$太大，这个objective function的值也会非常大。所以有没有办法限制一下这个比值呢？比如，把这两个policy的差异变得小一些。例如，用KL divergence去度量这个距离</td> </tr> </tbody> </table> \[KL\left(\pi_{\theta_{\text {old }}}|| \pi_\theta\right)=-\sum_a \pi_{\theta_{\text {old }}}(a \mid s) \log \frac{\pi_\theta(a \mid s)}{\pi_{\theta_{\text {old }}}(a \mid s)}\] <p>这样，我们带有trust region的objective，可以写成如下的形式：</p> \[\begin{gathered}J_{\theta_{\text {old }}}(\theta)=\mathbb{E}_t\left[\frac{\pi_\theta\left(a_t \mid s_t\right)}{\pi_{\theta_{\text {old }}}\left(a_t \mid s_t\right)} R_t\right] \\\text { subject to } K L\left(\pi_{\theta_{\text {old }}}\left(. \mid s_t\right)|| \pi_\theta\left(. \mid s_t\right)\right) \leq \delta\end{gathered}\] <p>在 trust region 中，我们把参数搜索固定在一个范围内.经过一些推导和泰勒展开近似，我们有：</p> \[\begin{aligned} J_{\theta_t}(\theta) &amp; \approx g^T\left(\theta-\theta_t\right) \\ KL\left(\theta_t \| \theta\right) &amp; \approx \frac{1}{2}\left(\theta-\theta_t\right)^T H\left(\theta-\theta_t\right) \end{aligned}\] <p>其中 \(g=\nabla_\theta J_{\theta_t}(\theta),H=\nabla_\theta^2 K L\left(\theta_t \| \theta\right)\) 其中 \(\theta_t\) 是old policy parameter，因此：</p> \[\theta_{t+1}=\underset{\theta}{\arg \max } g^T\left(\theta-\theta_t\right) \text { s.t. } \frac{1}{2}\left(\theta-\theta_t\right)^T H\left(\theta-\theta_t\right) \leq \delta\] <p>可以求出解析解：</p> \[\theta_{t+1}=\theta_t+\sqrt{\frac{2 \delta}{g^T H^{-1} g}} H^{-1} g\] <ul> <li> <p>NG是fisher information matrix \(F\)下最陡峭的上山方向。</p> \[H=\nabla_\theta^2 K L\left(\pi_{\theta_t}|| \pi_\theta\right)=E_{a, s \sim \pi_{\theta_t}}\left[\nabla_\theta \log \pi_\theta(a, s) \nabla_\theta \log \pi_\theta(a, s)^T\right]\] </li> <li>学习率\(\delta\)可以被看作是选择一个normalized step size来改变policy</li> <li>亮点：任何参数更新不会影响policy network的输出。</li> </ul> <h3 id="trpo中的natural-policy-gradient">TRPO中的natural policy gradient</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter6/Untitled%201-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter6/Untitled%201-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter6/Untitled%201-1400.webp"/> <img src="/assets/img/RL_chapter6/Untitled%201.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="23-trpo">2.3 TRPO</h2> <p>有一些问题：</p> <ol> <li>FIM和逆计算是开销非常大</li> <li>TRPO通过解一个线性方程来解决这个问题。即通过估解\(Hx=g\)来解\(x=H^{-1}g\).那么可以等价于优化一个二次方程：\(\min _x \frac{1}{2} x^T H x-g^T x\)</li> <li> <p>一些解释：解决 \(Ax=b\)等价于</p> \[\begin{gathered}x=\underset{x}{\arg \max } f(x)=\frac{1}{2} x^T A x-b^T x \\\text { since } f^{\prime}(x)=A x-b=0\end{gathered}\] </li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter6/Untitled%202-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter6/Untitled%202-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter6/Untitled%202-1400.webp"/> <img src="/assets/img/RL_chapter6/Untitled%202.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>用的解法是 <strong>conjugate gradient (CG)</strong></p> <ol> <li>其实TRPO算法有一点类似于EM算法，是一类Minorize-Maximization(MM)算法，解法都是先maximize一个邻近的函数然后逼近这个local expected reward</li> <li>TRPO的问题： <ol> <li>计算FIM开销太大</li> <li>计算精确FIM需要很多次采样</li> <li>CG算法实现起来比较困难</li> </ol> </li> </ol> <h2 id="24-acktr">2.4 ACKTR</h2> <p>用 <strong>Kronecker-factored approximation (K-FAC)</strong> 来改进TRPO，具体而言就是降低计算FIM求逆的复杂度</p> \[F=E_{x \sim \pi_{\theta_t}}\left[\left(\nabla_\theta \log \pi_\theta(x)\right)^T\left(\nabla_\theta \log \pi_\theta(x)\right)\right]\] <p>把这个替换为layer-wise calculation，因为是对称的，所以可以分块矩阵计算</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter6/Untitled%203-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter6/Untitled%203-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter6/Untitled%203-1400.webp"/> <img src="/assets/img/RL_chapter6/Untitled%203.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter6/Untitled%204-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter6/Untitled%204-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter6/Untitled%204-1400.webp"/> <img src="/assets/img/RL_chapter6/Untitled%204.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="25-ppo">2.5 PPO</h2> <h3 id="基础版ppo">基础版PPO</h3> <p>其实就是把TRPO的loss改写了一下，变成拉格朗日的形式：</p> \[\max \mathbb{E}_t\left[\frac{\pi_\theta\left(a_t \mid s_t\right)}{\pi_{\theta_{\text {old }}}\left(a_t \mid s_t\right)} R_t\right] \\\text { subject to } K L\left(\pi_{\theta_{\text {old }}}\left(. \mid s_t\right)|| \pi_\theta\left(. \mid s_t\right)\right) \leq \delta\] <p>改为：</p> \[\max \mathbb{E}_t\left[\frac{\pi_\theta\left(a_t \mid s_t\right)}{\pi_{\theta_{\text {old }}}\left(a_t \mid s_t\right)} R_t\right] -\beta K L\left(\pi_{\theta_{\text {old }}}\left(. \mid s_t\right)|| \pi_\theta\left(. \mid s_t\right)\right)\] <p>这样的好处就是可以直接用SGD来优化，速度比second order快很多。</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter6/Untitled%205-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter6/Untitled%205-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter6/Untitled%205-1400.webp"/> <img src="/assets/img/RL_chapter6/Untitled%205.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="带clipping的ppo">带clipping的PPO</h3> <p>定义概率比值\(r_t(\theta)=\frac{\pi_\theta\left(a_t \mid s_t\right)}{\pi_{\theta_{\text {old }}}\left(a_t \mid s_t\right)}\)，那么我们有不同的objectives</p> <ul> <li>不带trust region的PG: \(L_t(\theta)=r_t(\theta) \hat{A_t}\)</li> <li>KL限制：\(L_t(\theta)=r_t(\theta) \hat{A_t}.\text{s.t.}, KL[\pi_{\theta_{old}},\pi_\theta]\leq \delta\)</li> <li>KL惩罚：\(L_t(\theta)=r_t(\theta) \hat{A_t}- \beta KL[\pi_{\theta_{old}},\pi_\theta]\)</li> <li>(new) 限制policy不要和old policy离开太远：\(L_t(\theta)=\min (r_t(\theta) \hat{A_t},\text{clip}(r_t(\theta),1-\epsilon,1+\epsilon)\hat{A_t})\)</li> </ul> <p>可以看作是一个正则化项：</p> <ul> <li>当advantage为正时，鼓励action增加→ \(L_t(\theta)=\min (r_t(\theta),1+\epsilon)\hat{A_t}\)</li> <li>当advantage为正时，鼓励action减少→\(L_t(\theta)=\min (r_t(\theta),1-\epsilon)\hat{A_t}\)</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter6/Untitled%206-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter6/Untitled%206-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter6/Untitled%206-1400.webp"/> <img src="/assets/img/RL_chapter6/Untitled%206.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>PPO比起TRPO稳定性可靠性更好，而且实现起来更简单。</p> <blockquote> <p>总结一下SOTA的policy优化：</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> </figure></div></div> </blockquote> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter6/Untitled%207-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter6/Untitled%207-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter6/Untitled%207-1400.webp"/> <img src="/assets/img/RL_chapter6/Untitled%207.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <p>&lt;/figure&gt;</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;/div&gt;
</code></pre></div></div> <p>&lt;/div&gt;</p> <blockquote> </blockquote> <h1 id="三价值优化路线q-learning--ddpg--td3--sac">三、价值优化路线：Q-learning → DDPG → TD3 → SAC</h1> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter6/Untitled%208-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter6/Untitled%208-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter6/Untitled%208-1400.webp"/> <img src="/assets/img/RL_chapter6/Untitled%208.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="31-deep-deterministic-policy-gradient-ddpg">3.1 <strong>Deep Deterministic Policy Gradient (DDPG)</strong></h2> <p>原始的DQN是做离散动作的，我们可以把这个拓展到连续空间么？DDPG就可以看作是一个连续版本的DQN.</p> \[\textbf{DQN}: a^*=\arg\max_a Q^*(s,a)\\ \textbf{DDPG}: a^*=\arg\max_a Q^*(s,a) \approx Q_\phi (s,\mu_\theta(s))\] <ul> <li>\(\mu_\theta(s)\)是一个deterministic的策略，直接给出一个最大化\(Q_\phi (s,\mu_\theta(s))\)的action</li> <li>动作\(a\)是连续的</li> <li>我们假设Q-function 对\(a\)是可导的</li> </ul> <p>因此，DDPG有如下objective：</p> \[\textbf{Q-target}:y(r,s',d)=r+\gamma(1-d)Q_{\phi_{targ}} (s',\mu_{\theta_{targ}}(s'))\\ \textbf{Q-function}:\min \mathbb{E}_{s,r,s',d\sim D}[Q_\phi(s,a)-y(r,s',d)]\\ \textbf{policy}: \max_\theta \mathbb{E}_{s\sim D}[Q_\phi (s,\mu_\theta(s))]\] <p>同样，DDPG也用了reply buffer和target network</p> <h2 id="32-twin-delayed-ddpg-td3">3.2 Twin Delayed DDPG (TD3)</h2> <p>DDPG的缺点：有严重的过拟合，为此TD3做了三个改进：</p> <ol> <li><strong>Clipped Double-Q Learning</strong>：TD3学习两个Q函数，用两个Q value里面小的那一个来得到target</li> <li><strong>“Delayed” Policy Updates：</strong>TD3更新策略缓慢，更新Q函数快速（两个Q函数更新一次policy再更新一次）</li> <li><strong>**</strong><strong>Target Policy Smoothing：</strong><strong>**</strong>TD3在target action里面增加噪声，让policy更难利用Q函数。</li> </ol> <p>具体而言，TD3学习两个Q函数\(Q_{\phi_1},Q_{\phi_2}\)，两个函数用一个target如下：</p> \[y(r,s',d)=r+\gamma(1-d)\min_{i=1,2}Q_{\phi_{i,targ}}(s'a_{TD3}(s'))\] <p>Target Policy Smoothing（类似于正则化）</p> \[a_{TD3}(s')=clip(\mu_{\theta,targ}(s'))+clip(\epsilon,-c,c),a_{low},a_{high}),\epsilon\sim N(0,\sigma)\] <h2 id="33-soft-actor-critic-sac">3.3 <strong>Soft Actor-Critic (SAC)</strong></h2> <p>SAC是用类似于DDPG的方法，进行stochastic policy的优化，融合了 <strong>entropy regularization</strong> （熵正则化）。具体而言，这个策略被训练来最大化 expected return和entropy之间的trade-of</p> \[\pi^*=\arg\max \mathbb{E}_{\tau\sim \pi} [\sum_t \gamma^t(R(s_t,a_t,s_{t+1})+\alpha H(\pi(\cdot | s_t)))]\] <p>value function也包含了entropy bonus</p> \[V^\pi(s)=\mathbb{E}_{\tau\sim \pi}[\sum_t \gamma^t (R(s_t,a_t,s_{t+1})+\alpha H(\pi(\cdot|s_t)))|s_0=s]\] <p>我们可以推导出类似的bellman equation：</p> \[Q^\pi(s,a)=\mathbb{E}_{s'\sim P,a'\sim \pi}[R(s,a,s')+\gamma(Q^\pi(s',a')+\alpha H(\pi(a'|s')))]\\ =\mathbb{E}_{s'\sim P,a'\sim \pi}[R(s,a,s')+\gamma(Q^\pi(s',a')-\alpha \log\pi(a'|s'))]\] <p>因此Q函数的更新可以写成：</p> \[Q^\pi(s,a)\leftarrow r+\gamma(Q^\pi(s',\hat{a}')-\alpha \log \pi(\hat{a}'|s')),\hat{a}'\sim \pi(\cdot|s')\] <p>类似TD3，也是学习两个Q函数，用clipped double-Q trick。两个Q函数都是通过mean sqaure bellman error学习的：</p> \[L(\phi_i,D)=\mathbb{E}[(Q_\phi(s,a)-y(r,s',d))^2]\\ y(r,s',d)=r+\gamma(1-d)(\min_{j=1,2}Q_{\phi_{targ,j}}(s',\hat{a}'-\alpha \log\pi_\theta(\hat{a}'|s'))),\\ \hat{a}'\sim \pi_{\theta}(\cdot|s')\] <p>policy通过maximize\(V^\pi(s)\)来学习。</p> <h3 id="重参数化">重参数化</h3> <p>接下来我们把action当作是从一个gaussian分布里面采样得到的，</p> \[\hat{a}_\theta(s,\epsilon)=tanh(\mu_\theta(s)+\sigma_\theta(s)\odot\epsilon),\epsilon\sim N(0,I)\] <p>重参数化的好处：可以让我们把对action的期望转换为一个对noise的期望——和参数无关的分布，即</p> \[\mathbb{E}_{a\sim\pi_\theta}[Q^\pi_\theta(s,a)-\alpha \log \pi_\theta(a|s)]\\=\mathbb{E}_{\epsilon\sim N}[Q^\pi_\theta(s,\hat{a}_\theta(s,\epsilon))-\alpha \log \pi_\theta(\hat{a}_\theta(s,\epsilon)|s)]\] <p>因此，policy优化公式可以写为：</p> \[\max_{\theta}\mathbb{E}_{s\sim D,\epsilon\sim N}[\min_{j=1,2}Q_{\phi_j}(s,\hat{a}_\theta(s,\epsilon))-\alpha \log \pi_\theta(\hat{a}_\theta(s,\epsilon)|s)]\] <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter6/Untitled%209-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter6/Untitled%209-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter6/Untitled%209-1400.webp"/> <img src="/assets/img/RL_chapter6/Untitled%209.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div>]]></content><author><name></name></author><category term="Reinforcement-Learning"/><category term="intro"/><summary type="html"><![CDATA[PO SOTA]]></summary></entry><entry><title type="html">第三章 Tabular methods Model-free Prediction and Control (表格型方法：免模型的预测和控制)</title><link href="https://gszfwsb.github.io/blog/2023/RL-lecture3/" rel="alternate" type="text/html" title="第三章 Tabular methods Model-free Prediction and Control (表格型方法：免模型的预测和控制)"/><published>2023-10-30T00:00:00+00:00</published><updated>2023-10-30T00:00:00+00:00</updated><id>https://gszfwsb.github.io/blog/2023/RL-lecture3</id><content type="html" xml:base="https://gszfwsb.github.io/blog/2023/RL-lecture3/"><![CDATA[<h1 id="一为什么需要免模型rl">一、为什么需要免模型RL？</h1> <ol> <li> <p>什么时候马尔可夫决策过程是已知的？</p> <p><strong>奖励和转移概率均已知，</strong>这样才可以用策略迭代和价值迭代进行求解。</p> </li> <li>策略迭代：给定一个已知的MDP，计算最优策略函数和价值函数。 <ol> <li> <p>策略评估：用贝尔曼期望方程迭代到收敛</p> \[V_{t+1}(s)=\sum_{a \in \mathcal{A}} \pi(a \mid s)(\textcolor{red}{R(s, a)}+\gamma \sum_{s^{\prime} \in \mathcal{S}} \textcolor{red}{P(s^{\prime} \mid s, a)} V_{t}(s^{\prime})\] </li> <li> <p>策略改进：用贝尔曼期望方程，并在价值函数上用贪心策略</p> \[Q_{\pi_i}(s, a)=\textcolor{red}{R(s, a)}+\gamma \sum_{s^{\prime} \in S} \textcolor{red}{P\left(s^{\prime} \mid s, a\right)} V_{\pi_i}\left(s^{\prime}\right)\\\pi_{i+1}(s)=\underset{a}{\arg \max } Q_{\pi_i}(s, a)\] </li> </ol> </li> <li>价值迭代：给定一个已知的MDP，计算最优价值函数。 <ol> <li> <p>用贝尔曼最优公式迭代</p> \[v_{i+1}(s)\leftarrow \max_{a\in \mathcal{A}}(\textcolor{red}{R(s, a)}+\gamma \sum_{s^{\prime} \in \mathcal{S}} \textcolor{red}{P(s^{\prime} \mid s, a)} v_{i}(s^{\prime})）\] </li> <li> <p>得到迭代后的最优策略：</p> \[\pi^*(s)=\underset{a}{\arg \max }\left[\textcolor{red}{R(s, a)}+\gamma \sum_{s^{\prime} \in S} \textcolor{red}{P\left(s^{\prime} \mid s, a\right)} V_{\textrm{end}}\left(s^{\prime}\right)\right]\] </li> </ol> </li> <li>知道世界如何运作的RL： <ol> <li>策略迭代和价值迭代都需要假设我们已知环境中的<strong>状态转移</strong>和<strong>奖励</strong>。</li> <li>现实情境中，MDP模型要么就是未知要么就是太大太复杂</li> </ol> </li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter3/Untitled-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter3/Untitled-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter3/Untitled-1400.webp"/> <img src="/assets/img/RL_chapter3/Untitled.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ol> <li> <p>免模型RL：通过与环境交互交互学习</p> <p>马尔可夫决策过程的模型未知或者模型很大时，我们可以使用免模型强化学习的方法。免模型强化学习方法没有获取环境的<strong>状态转移</strong>和<strong>奖励函数</strong>，而是让智能体与环境进行<strong>交互</strong>，采集大量的<strong>轨迹数据</strong>，智能体从轨迹中获取信息来改进策略，从而获得更多的奖励。</p> <p>轨迹：包括\(\{S_1,A_1,R_1,S_2,A_2,R_2,...,S_T,A_T,R_T\}\)</p> </li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter3/Untitled%201-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter3/Untitled%201-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter3/Untitled%201-1400.webp"/> <img src="/assets/img/RL_chapter3/Untitled%201.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h1 id="二免模型预测">二、免模型预测</h1> <p>在无法获取马尔可夫决策过程的模型情况下，我们可以通过<strong>蒙特卡洛方法</strong>和<strong>时序差分方法</strong>来估计某个给定策略的价值。</p> <h2 id="21-蒙特卡洛方法">2.1 蒙特卡洛方法</h2> <h3 id="211-蒙特卡洛策略评估">2.1.1 蒙特卡洛策略评估</h3> <p>蒙特卡洛方法是基于采样的方法，给定策略 \(\pi\) ，我们让智能体与环境进行交互，可以得到很多轨迹。每个轨迹都有对应的回报：</p> \[G_t=r_{t+1}+\gamma r_{t+2}+\gamma^2 r_{t+3}+\ldots \] <p>我们求出所有轨迹的回报的平均值，就可以知道某一个策略对应状态的价值，即</p> \[V_\pi(s)=\mathbb{E}_{\tau \sim \pi}\left[G_t \mid s_t=s\right]\] <ol> <li>特点： <ul> <li>蒙特卡洛仿真是指我们可以采样大量的<strong>轨迹</strong>\(\tau\)（从策略\(\pi\)采样），计算所有轨迹\(\tau\)的真实回报，然后计算平均值。</li> <li>蒙特卡洛方法使用<strong>经验平均回报</strong>（empirical mean return) 的方法来估计，它不需要马尔可夫决策过程的状态转移函数和奖励函数，并且不需要像动态规划那样用bootstrap的方法。</li> <li>此外，蒙特卡洛方法有一定的局限性，它只能用在有终止的马尔可夫决策过程中。</li> </ul> </li> <li>为了得到评估 \(V(s)\) ，我们采取了如下的步骤。 <ol> <li>在每个回合中，如果在时间步 \(t\) 状态 \(s\) 被访问了，那么 <ul> <li>状态 \(s\) 的访问数 \(N(s)\) 增加 \(1 ， N(s) \leftarrow N(s)+1\) 。</li> <li>状态 \(s\) 的总的回报 \(S(s)\) 增加 \(G_t, S(s) \leftarrow S(s)+G_{t}\)</li> <li>状态 \(s\) 的价值可以通过回报的平均来估计，即 \(V(s)=S(s) / N(s)\) 。</li> </ul> </li> <li>根据大数定律，只要我们得到足够多的轨迹，就可以趋近这个策略对应的价值函数。当 \(N(s) \rightarrow \infty\)时， \(V(s) \rightarrow V_\pi(s)\) 。</li> </ol> </li> <li> <p>具体更新时，我们可以把<strong>经验均值</strong>（empirical mean）转换成<strong>增量均值</strong>（incremental mean）的形式：</p> \[\begin{aligned}\mu_t &amp; =\frac{1}{t} \sum_{j=1}^t x_j \\&amp; =\frac{1}{t}\left(x_t+\sum_{j=1}^{t-1} x_j\right) \\&amp; =\frac{1}{t}\left(x_t+(t-1) \mu_{t-1}\right) \\&amp; =\mu_{t-1}+\frac{1}{t}\left(x_t-\mu_{t-1}\right)\end{aligned}\] </li> <li>增量均值形式的MC算法： <ul> <li>采样一轮游戏\(\left(S_1, A_1, R_1, \ldots, S_t\right)\)</li> <li> <p>对每一个状态\(S_t\)和回报\(G_t\)</p> \[\begin{aligned} &amp; N\left(S_t\right) \leftarrow N\left(S_t\right)+1 \\ &amp; v\left(S_t\right) \leftarrow v\left(S_t\right)+\frac{1}{N\left(S_t\right)}\left(G_t-v\left(S_t\right)\right) \end{aligned}\] </li> <li> <p>或者可以用running mean. 对于non-stationary问题有好处，\(\alpha\)是learning rate</p> \[v\left(S_t\right) \leftarrow v\left(S_t\right)+\alpha\left(G_t-v\left(S_t\right)\right) \] </li> </ul> </li> </ol> <h3 id="212-蒙特卡洛方法和动态规划方法">2.1.2 蒙特卡洛方法和动态规划方法</h3> <ol> <li> <p>在动态规划方法里面，我们使用了bootstrap的思想。bootstrap就是我们基于之前估计的量来估计一个量。此外，动态规划方法使用贝尔曼期望备份（Bellman expectation backup），通过上一时刻的\(V_{t}\)更新这一时刻的\(V_{t+1}\).</p> \[V_{t+1}(s) \leftarrow \sum_{a \in \mathcal{A}} \pi(a \mid s)\left(R(s, a)+\gamma \sum_{s^{\prime} \in \mathcal{S}} P\left(s^{\prime} \mid s, a\right) V_{t}\left(s^{\prime}\right)\right)\] <p>将其不停迭代，最后可以收敛。如图所示，贝尔曼期望公式有两层加和，即内部加和和外部加和，需要知道所有的状态和转移矩阵（model）计算两次期望，得到一个更新。</p> </li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter3/Untitled%202-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter3/Untitled%202-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter3/Untitled%202-1400.webp"/> <img src="/assets/img/RL_chapter3/Untitled%202.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ol> <li> <p>蒙特卡洛方法通过一个回合的经验平均回报（实际得到的奖励）来进行更新，即</p> \[V\left(s_t\right) \leftarrow V\left(s_t\right)+\alpha\left(G_{i, t}-V\left(s_t\right)\right)\] </li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter3/Untitled%203-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter3/Untitled%203-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter3/Untitled%203-1400.webp"/> <img src="/assets/img/RL_chapter3/Untitled%203.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>如图，我们使用蒙特卡洛方法得到的轨迹对应树上蓝色的轨迹，轨迹上的状态已经是决定的，采取的动作也是已经决定的。我们现在只更新这条轨迹上的所有状态，与这条轨迹没有关系的状态都不进行更新。
</code></pre></div></div> <ol> <li><strong>蒙特卡洛方法</strong>相比<strong>动态规划方法</strong>是有一些优势的。 <ol> <li>蒙特卡洛方法适用于<strong>环境未知</strong>的情况，而动态规划是<strong>有模型</strong>的方法（需要转移矩阵）。</li> <li>即使知道了环境的全部信息，计算转移概率往往也是很复杂的。</li> <li>蒙特卡洛方法只需要更新<strong>一条轨迹</strong>的状态，而动态规划方法需要更新<strong>所有的状态</strong>。状态数量很多的时候（比如100万个、200万个），我们使用动态规划方法进行迭代，速度是非常慢。</li> </ol> </li> </ol> <h2 id="22-时序差分方法">2.2 时序差分方法</h2> <p>时序差分是介于蒙特卡洛和动态规划之间的方法，它是免模型的，不需要马尔可夫决策过程的转移矩阵和奖励函数。 此外，时序差分方法可以从不完整的回合中学习，并且结合了bootstrap的思想。</p> <ol> <li>特点： <ol> <li>直接从轨迹中学习</li> <li>免模型：不需要MDP的转移概率和奖励</li> <li>通过bootstrap从<strong>不完整</strong>的轨迹中学习</li> </ol> </li> <li>时序差分方法的目的是对于某个给定的策略\(\pi\)，在线（online）地算出它的价值函数\(V_\pi\)，即一步一步地（step-by-step）算。 <ol> <li> <p>最简单的算法是一步时序差分（one-step TD），TD(0)：</p> \[V\left(s_t\right) \leftarrow V\left(s_t\right)+\alpha\left[\textcolor{red}{R_t+\gamma V\left(s_{t+1}\right)}-V\left(s_t\right)\right]\] </li> <li>\(R_{t+1}+\gamma V\left(S_{t+1}\right)\) 是TD目标</li> <li>\(\delta_t=R_{t+1}+\gamma V\left(S_{t+1}\right)-V\left(S_t\right)\)是TD error</li> </ol> <p>😍 MC和TD： 类比增量式蒙特卡洛方法，给定一个回合\(i\)，我们可以更新 \(V(s_t)\) 来逼近真实的回报，具体更新公式为:</p> \[V\left(s_t\right) \leftarrow V\left(s_t\right)+\alpha\left(\textcolor{red}{G_{i, t}}-V\left(s_t\right)\right)\] <p>回顾贝尔曼期望方程便知道原因：</p> \[\begin{aligned}V_\pi(s) &amp; =\mathbb{E}_\pi\left[G_t \mid S_t=s\right] \\&amp; =\mathbb{E}_\pi\left[\sum_{k=0}^{\infty} \gamma^k R_{t+k} \mid S_t=s\right] \\&amp; =\mathbb{E}_\pi\left[R_t+\gamma \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \mid S_t=s\right] \\&amp; =\mathbb{E}_\pi\left[R_t+\gamma V_\pi\left(S_{t+1}\right) \mid S_t=s\right]\end{aligned}\] <p><strong>蒙特卡洛方法</strong>将上式<strong>第一行</strong>作为更新的目标，而<strong>时序差分算法</strong>将上式<strong>最后一行</strong>作为更新的目标。于是，在用策略和环境交互时，每采样一步，我们就可以用时序差分算法来更新状态价值估计。时序差分算法用到了\(V(s_{t+1})\)的估计值，可以证明它最终收敛到策略\(\pi\) 的价值函数。</p> </li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter3/Untitled%204-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter3/Untitled%204-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter3/Untitled%204-1400.webp"/> <img src="/assets/img/RL_chapter3/Untitled%204.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter3/Untitled%205-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter3/Untitled%205-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter3/Untitled%205-1400.webp"/> <img src="/assets/img/RL_chapter3/Untitled%205.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>进一步比较时序差分方法和蒙特卡洛方法。

- 时序差分方法可以**在线学习**（online learning），每走一步就可以更新效率高。蒙特卡洛方法必须等游戏结束时才可以学习。
- 时序差分方法可以从**不完整序列**上进行学习。蒙特卡洛方法只能从**完整的序列**上进行学习。
- 时序差分方法可以在**连续的环境下**（没有终止）进行学习。蒙特卡洛方法只能在**有终止**的情况下学习。
- 时序差分方法利用了**马尔可夫性质**，在马尔可夫环境下有更高的学习效率。蒙特卡洛方法没有假设环境具有马尔可夫性质，利用采样的价值来估计某个状态的价值，在不是马尔可夫的环境下更加有效。
</code></pre></div></div> <ol> <li>多步时序差分（n-step TD）：之前是只往前走一步，即TD(0)。 我们可以调整步数（step），变成<strong><em>n</em>步时序差分。</strong></li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter3/Untitled%206-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter3/Untitled%206-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter3/Untitled%206-1400.webp"/> <img src="/assets/img/RL_chapter3/Untitled%206.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>这样我们就可以通过步数来调整算法需要的实际奖励和bootstrap。

$$
\begin{aligned}  n=1(\mathrm{TD}) \quad &amp; G_t^{(1)}=r_{t+1}+\gamma V\left(s_{t+1}\right) \\ n=2(\mathrm{TD(2)})) \quad &amp; G_t^{(2)}=r_{t+1}+\gamma r_{t+2}+\gamma^2 V\left(s_{t+2}\right) \\ \ldots \\ n=\infty(\mathrm{MC}) \quad &amp; G_t^{\infty} \begin{array}{l}=r_{t+1}+\gamma r_{t+2}+\ldots+\gamma^{T-t-1} r_T\end{array}\end{aligned}
$$

通过调整步数，可以进行蒙特卡洛方法和时序差分方法之间的权衡。如果$$n=\infty$$， 即整个游戏结束后，再进行更新，时序差分方法就变成了蒙特卡洛方法。*n*步时序差分可写为:

$$
G_t^n=r_{t+1}+\gamma r_{t+2}+\ldots+\gamma^{n-1} r_{t+n}+\gamma^n V\left(s_{t+n}\right)
$$

得到时序差分目标之后，我们用增量式学习 (incremental learning) 的方法来更新状态的价值:

$$
V\left(s_t\right) \leftarrow V\left(s_t\right)+\alpha\left(G_t^n-V\left(s_t\right)\right) 
$$
</code></pre></div></div> <h2 id="23-采样和bootrap">2.3 采样和bootrap</h2> <p>动态规划方法、蒙特卡洛方法以及时序差分方法的bootstrap和采样有什么联系和区别呢？</p> <h3 id="231-采样和bootrap情形">2.3.1 采样和bootrap情形</h3> <ol> <li><strong>Bootstrap是指更新时使用了估计</strong>。 <ol> <li>蒙特卡洛方法没有使用bootstrap，因为它根据实际的回报进行更新。</li> <li>动态规划方法和时序差分方法使用了bootstrap。</li> </ol> </li> <li><strong>采样是指更新时通过采样得到一个期望</strong>。 <ol> <li>蒙特卡洛方法是纯采样的方法。</li> <li>动态规划方法没有使用采样，它是直接用贝尔曼期望方程来更新状态价值的。</li> <li>时序差分方法使用了采样。时序差分目标由两部分组成，一部分是采样，一部分是bootstrap。</li> </ol> </li> </ol> <h3 id="232-dp-mc-td">2.3.2 DP, MC, TD</h3> <ol> <li> <p>动态规划方法直接计算期望，它把所有相关的状态都进行加和，即</p> \[V\left(s_t\right) \leftarrow \mathbb{E}_\pi\left[r_{t+1}+\gamma V\left(s_{t+1}\right)\right]\] </li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter3/Untitled%207-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter3/Untitled%207-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter3/Untitled%207-1400.webp"/> <img src="/assets/img/RL_chapter3/Untitled%207.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ol> <li> <p>蒙特卡洛方法在当前状态下，采取一条支路，在这条路径上进行更新，更新这条路径上的所有状态，即</p> \[V\left(s_t\right) \leftarrow V\left(s_t\right)+\alpha\left(G_t-V\left(s_t\right)\right)\] </li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter3/Untitled%208-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter3/Untitled%208-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter3/Untitled%208-1400.webp"/> <img src="/assets/img/RL_chapter3/Untitled%208.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ol> <li> <p>时序差分从当前状态开始，往前走了一步，关注的是非常局部的步骤，即</p> \[\mathrm{TD}(0): V\left(s_t\right) \leftarrow V\left(s_t\right)+\alpha\left(r_{t+1}+\gamma V\left(s_{t+1}\right)-V\left(s_t\right)\right)\] </li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter3/Untitled%209-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter3/Untitled%209-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter3/Untitled%209-1400.webp"/> <img src="/assets/img/RL_chapter3/Untitled%209.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ol> <li>如果时序差分方法需要更广度的更新，就变成了动态规划方法（因为动态规划方法是把所有状态都考虑进去来进行更新）。如果时序差分方法需要更深度的更新，就变成了蒙特卡洛方法。图 右下角是穷举搜索的方法（exhaustive search），穷举搜索的方法不仅需要很深度的信息，还需要很广度的信息。</li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter3/Untitled%2010-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter3/Untitled%2010-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter3/Untitled%2010-1400.webp"/> <img src="/assets/img/RL_chapter3/Untitled%2010.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h1 id="三免模型控制">三、免模型控制</h1> <h2 id="31-广义策略迭代">3.1 广义策略迭代</h2> <p>在我们不知道马尔可夫决策过程模型的情况下，如何优化价值函数，得到最佳的策略呢？我们可以把策略迭代进行广义的推广，使它能够兼容蒙特卡洛和时序差分的方法，即带有<strong>蒙特卡洛方法</strong>和<strong>时序差分</strong>方法的<strong>广义策略迭代（generalized policy iteration，GPI）</strong></p> <p>当我们不知道奖励函数和状态转移时，如何进行策略的优化？我们引入了广义的策略迭代的方法。 我们对策略评估部分进行修改，使用蒙特卡洛的方法代替动态规划的方法估计 \(Q\) 函数。我们首先进行策略评估，使用蒙特卡洛方法来估计策略 \(Q=Q_\pi\)，然后进行策略更新，即得到 \(Q\) 函数后，我们就可以通过贪心的方法去改进它：</p> \[\pi(s)=\underset{a}{\arg \max } Q(s, a)\] <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter3/Untitled%2011-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter3/Untitled%2011-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter3/Untitled%2011-1400.webp"/> <img src="/assets/img/RL_chapter3/Untitled%2011.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="32-蒙特卡洛方法">3.2 蒙特卡洛方法</h2> <h3 id="321-探索性开始的mc">3.2.1 探索性开始的MC</h3> <p>一个保证策略迭代收敛的<strong>假设</strong>是回合有<strong>探索性开始（exploring start）</strong>。假设每一个回合都有一个<strong>探索性开始</strong>，探索性开始保证所有的状态和动作都在无限步的执行后能被采样到，这样才能很好地进行估计。 算法通过蒙特卡洛方法产生很多轨迹，每条轨迹都可以算出它的价值。然后，我们可以通过平均的方法去估计 Q 函数。Q 函数可以看成一个表格，我们通过采样的方法把表格的每个单元值都填上，然后使用策略改进来选取更好的策略。 如何用蒙特卡洛方法来填 Q 表格是这个算法的核心。</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter3/Untitled%2012-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter3/Untitled%2012-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter3/Untitled%2012-1400.webp"/> <img src="/assets/img/RL_chapter3/Untitled%2012.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="322-带有varepsilon-greedy的贪心策略">3.2.2 带有\(\varepsilon\)-greedy的贪心策略</h3> <p>为了确保蒙特卡洛方法能够有足够的探索，我们使用了 \(\varepsilon\)-贪心探索。 \(\varepsilon\)-贪心是指我们有  \(1-\varepsilon\) 的概率会按照 Q函数来决定动作，通常 \(\varepsilon\) 就设一个很小的值， \(1-\varepsilon\) 可能是 0.9，也就是 0.9 的概率会按照Q函数来决定动作，但是我们有 0.1 的概率是随机的。通常在实现上， \(\varepsilon\) 的值会随着时间递减。在最开始的时候，因为我们还不知道哪个动作是比较好的，所以会花比较多的时间探索。接下来随着训练的次数越来越多，我们已经比较确定哪一个动作是比较好的，就会减少探索，把 \(\varepsilon\) 的值变小。主要根据 Q 函数来决定动作，比较少随机决定动作，这就是\(\varepsilon\)-贪心。</p> \[\pi(a \mid s)= \begin{cases}\epsilon /|\mathcal{A}|+1-\epsilon &amp; \text { if } a^*=\arg \max _{a \in \mathcal{A}} Q(s, a) \\ \epsilon /|\mathcal{A}| &amp; \text { otherwise }\end{cases}\] <p>当我们使用蒙特卡洛方法和\(\varepsilon\)-贪心探索的时候，可以确保价值函数是单调的、改进的。对于任何 \(\epsilon\)-贪心策略 \(\pi\)，关于 \(Q_\pi\) 的 \(\varepsilon\)-贪心策略 \(\pi'\) 都是一个改进，即 \(V_{\pi}(s)\leq V_{\pi'}(s)\)，证明过程如下：</p> \[\begin{aligned}V_\pi\left(s, \pi^{\prime}(s)\right) &amp; =\sum_{a \in A} \pi^{\prime}(a \mid s) Q_\pi(s, a) \\&amp; =\frac{\varepsilon}{|A|} \sum_{a \in A} Q_\pi(s, a)+(1-\varepsilon) \max _a Q_\pi(s, a) \\&amp; \geqslant \frac{\varepsilon}{|A|} \sum_{a \in A} Q_\pi(s, a)+(1-\varepsilon) \sum_{a \in A} \frac{\pi(a \mid s)-\frac{\varepsilon}{|A|}}{1-\varepsilon} Q_\pi(s, a) \\&amp; =\sum_{a \in A} \pi(a \mid s) Q_\pi(s, a)=V_\pi(s)\end{aligned}\] <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter3/Untitled%2013-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter3/Untitled%2013-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter3/Untitled%2013-1400.webp"/> <img src="/assets/img/RL_chapter3/Untitled%2013.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="33-时序差分方法">3.3 时序差分方法</h2> <p>与蒙特卡洛方法相比，时序差分方法有如下几个优势：<strong>低方差，能够在线学习，能够从不完整的序列中学习。</strong> 所以我们可以把时序差分方法也放到<strong>控制循环</strong>（control loop）里面去估计Q表格，再采取\(\varepsilon\)贪心探索改进。这样就可以在回合没结束的时候更新已经采集到的状态价值。</p> <h3 id="331-sarsa-on-policy-td">3.3.1 Sarsa: on-policy TD</h3> <ol> <li> <p>Sarsa：</p> <p>时序差分方法是给定一个策略，然后我们去估计它的价值函数。接着我们要考虑怎么使用时序差分方法的框架来估计Q函数，也就是 Sarsa 算法。</p> </li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter3/Untitled%2014-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter3/Untitled%2014-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter3/Untitled%2014-1400.webp"/> <img src="/assets/img/RL_chapter3/Untitled%2014.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Sarsa 所做出的改变很简单，它将原本时序差分方法更新 *V* 的过程，变成了更新 *Q*，即

$$
Q\left(S_t, A_t\right) \leftarrow Q\left(S_t, A_t\right)+\alpha\left[R_{t+1}+\gamma Q\left(S_{t+1}, A_{t+1}\right)-Q\left(S_t, A_t\right)\right]
$$

&gt; 注：prediction中我们更新V，control中我们更新Q
&gt; 
&gt; 
&gt; $$
&gt; V\left(s_t\right) \leftarrow V\left(s_t\right)+\alpha\left(R_{t+1}+\gamma V\left(s_{t+1}\right)-V\left(s_t\right)\right)
&gt; $$
&gt; 

TD目标：$$\delta_t=R_{t+1}+\gamma Q\left(S_{t+1}, A_{t+1}\right)$$
</code></pre></div></div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter3/Untitled%2015-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter3/Untitled%2015-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter3/Untitled%2015-1400.webp"/> <img src="/assets/img/RL_chapter3/Untitled%2015.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ol> <li> <p>n-step Sarsa</p> <p>Sarsa 属于单步更新算法，每执行一个动作，就会更新一次价值和策略。如果不进行单步更新，而是采取\(n\)步更新或者回合更新，即在执行<em>n</em>步之后再更新价值和策略，这样我们就得到了<strong><em>n</em> 步 Sarsa（<em>n</em>-step Sarsa）:</strong></p> \[\begin{aligned}n=1(\text { Sarsa }) q_t^{(1)}= &amp; R_{t+1}+\gamma Q\left(S_{t+1}, A_{t+1}\right) \\n=2 \quad q_t^{(2)}= &amp; R_{t+1}+\gamma R_{t+2}+\gamma^2 Q\left(S_{t+2}, A_{t+2}\right) \\&amp; \vdots &amp; \\n=\infty(M C) \quad q_t^{\infty}= &amp; R_{t+1}+\gamma R_{t+2}+\ldots+\gamma^{T-t-1} R_T\end{aligned}\] <p>n-step收益为：</p> \[q_t^{(n)}=R_{t+1}+\gamma R_{t+2}+\ldots+\gamma^{n-1} R_{t+n}+\gamma^n Q\left(S_{t+n}, A_{t+n}\right)\] <p>n-step迭代更新：</p> \[Q\left(S_t, A_t\right) \leftarrow Q\left(S_t, A_t\right)+\alpha\left(q_t^{(n)}-Q\left(S_t, A_t\right)\right)\] </li> </ol> <h3 id="332-q-learning-off-policy-td">3.3.2 Q-learning: off-policy TD</h3> <ol> <li>off-policy <ol> <li>Sarsa 是一种<strong>在线策略（on-policy）</strong>算法，它优化的是它实际执行的策略，它直接用下一步会执行的动作去优化 Q 表格。<strong>在线策略</strong>在学习的过程中，只存在一种策略，它用一种策略去做动作的选取，也用一种策略去做优化。所以 Sarsa 知道它下一步的动作有可能会跑到悬崖那边去，它就会在优化自己的策略的时候，尽可能离悬崖远一点。这样子就会保证，它下一步哪怕是有随机动作，它也还是在安全区域内。</li> <li>Q学习是一种<strong>离线策略（off-policy）</strong>算法。如图所示，<strong>离线策略</strong>在学习的过程中，有两种不同的策略：<strong>目标策略（target policy）</strong>和<strong>行为策略（behavior policy）</strong>。 <ul> <li>目标策略是我们需要去学习的策略，一般用 \(\pi\) 来表示。目标策略就像是在后方指挥战术的一个军师，它可以根据自己的经验来学习最优的策略，不需要去和环境交互。</li> <li>行为策略是探索环境的策略，一般用 <em>μ</em> 来表示。行为策略可以大胆地去探索到所有可能的轨迹，采集轨迹，采集数据，然后把采集到的数据“喂”给目标策略学习。而且“喂”给目标策略的数据中并不需要 \(a_{t+1}\)，而 Sarsa 是要有 \(a_{t+1}\) 的。行为策略像是一个战士，可以在环境里面探索所有的动作、轨迹和经验，然后把这些经验交给目标策略去学习。比如目标策略优化的时候，Q学习不会管我们下一步去往哪里探索，它只选取奖励最大的策略。</li> </ul> </li> </ol> </li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter3/Untitled%2016-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter3/Untitled%2016-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter3/Untitled%2016-1400.webp"/> <img src="/assets/img/RL_chapter3/Untitled%2016.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ol> <li>off-policy学习有很多好处。 <ol> <li>我们可以利用探索策略来学到最佳的策略，学习效率高；</li> <li>其次，off-policy学习可以让我们学习其他智能体的动作，进行模仿学习，学习人或者其他智能体产生的轨迹；</li> <li>最后，off-policy学习可以让我们重用旧的策略产生的轨迹，探索过程需要很多计算资源，这样可以节省资源。</li> </ol> </li> <li> <p>离线算法：</p> <p>Q学习有两种策略: 行为策略和目标策略。</p> <ul> <li> <p>目标策略 \(\pi\) 直接在 Q 表格上使用贪心策略，取它下一步能得到的所有状态，即</p> \[\pi\left(s_{t+1}\right)=\underset{a^{\prime}}{\arg \max } Q\left(s_{t+1}, a^{\prime}\right) \] </li> <li> <p>行为策略 \(\mu\) 可以是一个随机的策略，但我们采取 \(\varepsilon\)-贪心策略，让行为策略不至于是完全随机的，它是基于 Q 表格逐渐改进的。我们可以构造Q 学习目标， Q学习的下一个动作都是通过 argmax 操作选出来的，于是我们可得</p> \[\begin{aligned} r_{t+1}+\gamma Q\left(s_{t+1}, A^{\prime}\right) &amp; =r_{t+1}+\gamma Q\left(s_{t+1}, \underset{a^{\prime}}{\arg \max } Q\left(s_{t+1}, a^{\prime}\right)\right) \\ &amp; =r_{t+1}+\gamma \max_{a^{\prime}} Q\left(s_{t+1}, a^{\prime}\right) \end{aligned}\] <p>接着我们可以把 Q 学习更新写成增量学习的形式，时序差分目标变成了 \(r_{t+1}+\gamma \max_a Q\left(s_{t+1}, a\right)\) ，即</p> \[Q\left(s_t, a_t\right) \leftarrow Q\left(s_t, a_t\right)+\alpha\left[r_{t+1}+\gamma \max_a Q\left(s_{t+1}, a\right)-Q\left(s_t, a_t\right)\right]\] </li> </ul> </li> <li>Q-learning和Sarsa：</li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter3/Untitled%2017-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter3/Untitled%2017-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter3/Untitled%2017-1400.webp"/> <img src="/assets/img/RL_chapter3/Untitled%2017.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter3/Untitled%2018-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter3/Untitled%2018-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter3/Untitled%2018-1400.webp"/> <img src="/assets/img/RL_chapter3/Untitled%2018.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>通过备份图也可以看出差异：

- Sarsa里面A和A’都是通过一个同样的policy进行采样的，所以是在线策略
- Q-learning里面，A和A’不是同一个策略。A是探索策略（目标策略），A’是直接通过max操作得到的策略（行为策略）。
</code></pre></div></div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter3/Untitled%2019-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter3/Untitled%2019-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter3/Untitled%2019-1400.webp"/> <img src="/assets/img/RL_chapter3/Untitled%2019.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1. DP和TD的总结：
</code></pre></div></div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter3/Untitled%2020-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter3/Untitled%2020-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter3/Untitled%2020-1400.webp"/> <img src="/assets/img/RL_chapter3/Untitled%2020.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="333-on-policy和off-policy的区别">3.3.3 on-policy和off-policy的区别</h3> <p>总结一下在线策略和离线策略的区别。</p> <ul> <li>只用了一个策略 \(\pi\)，它不仅使用策略 \(\pi\) 学习，还使用策略 \(\pi\) 与环境交互产生经验。 如果策略采用 \(\epsilon\)贪心算法，它需要兼顾探索，为了兼顾探索和利用，它训练的时候会显得有点“胆小”。它在解决悬崖行走问题的时候，会尽可能地远离悬崖边，确保哪怕自己不小心探索了一点儿，也还是在安全区域内。此外，因为采用的是 \(\epsilon\)贪心 算法，策略会不断改变（\(\epsilon\) 值会不断变小），所以策略不稳定。</li> <li>有两种策略————目标策略和行为策略。 <ul> <li>行为策略探索得到的经验轨迹来优化目标策略，从而更有可能探索到最佳策略。行为策略可以采用 \(\epsilon\)贪心 算法</li> <li>目标策略采用的是贪心算法，它直接根据行为策略采集到的数据来采用最佳策略，所以不需要兼顾探索。</li> <li>off-policy是通过从行为策略\(\mu\)中的经验采样来学习目标策略\(\pi\)</li> </ul> </li> <li>我们比较一下 Q学习 和 Sarsa 的更新公式，就可以发现 Sarsa 并没有选取最大值的最大化操作。因此，Q学习是一个非常激进的方法，它希望每一步都获得最大的利益；Sarsa 则相对较为保守，它会选择一条相对安全的迭代路线。</li> </ul>]]></content><author><name></name></author><category term="Reinforcement-Learning"/><category term="intro"/><summary type="html"><![CDATA[Tabular methods]]></summary></entry><entry><title type="html">第四章 Value Function Approximation and Deep Q-learning (价值函数近似和深度Q学习)</title><link href="https://gszfwsb.github.io/blog/2023/RL-lecture4/" rel="alternate" type="text/html" title="第四章 Value Function Approximation and Deep Q-learning (价值函数近似和深度Q学习)"/><published>2023-10-30T00:00:00+00:00</published><updated>2023-10-30T00:00:00+00:00</updated><id>https://gszfwsb.github.io/blog/2023/RL-lecture4</id><content type="html" xml:base="https://gszfwsb.github.io/blog/2023/RL-lecture4/"><![CDATA[<h1 id="一函数近似">一、函数近似</h1> <h2 id="11-问题引入大规模的mdp问题如何估计价值函数">1.1 问题引入：大规模的MDP问题如何估计价值函数？</h2> <p>在面对大规模 MDP 问题时，要避免用table去表示特征（Q-tabel等），而是采用带参数的<strong>函数近似</strong>的方式去近似估计V、Q、π</p> <ol> <li>表格型方法 <ul> <li>在表格型方法中，我们是通过查表的方式去计算价值函数的</li> <li>每一个状态-动作对 \(&lt;s,a&gt;\)有一个元素\(Q(s,a)\)</li> </ul> </li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter4/Untitled-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter4/Untitled-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter4/Untitled-1400.webp"/> <img src="/assets/img/RL_chapter4/Untitled.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ol> <li>MDP的挑战： <ul> <li>太多状态和动作需要存下来</li> <li>单独学习每一个状态的价值很慢</li> </ul> </li> </ol> <h2 id="12-通过函数近似的方法来解决大规模rl">1.2 通过函数近似的方法来解决大规模RL</h2> <ol> <li>如何隐式学习或存储每个状态？需要学习的包括： <ol> <li>状态转移模型、奖励模型</li> <li>价值函数，Q函数</li> <li>策略</li> </ol> </li> <li> <p>解决方法：通过<strong>价值函数近似（VFA）</strong>来逼近。通过参数\(\mathbf{w}\)估计价值函数、Q函数和策略。</p> \[\begin{aligned}\hat{v}(s, \mathbf{w}) &amp; \approx v^\pi(s) \\\hat{q}(s, a, \mathbf{w}) &amp; \approx q^\pi(s, a) \\\hat{\pi}(a, s, \mathbf{w}) &amp; \approx \pi(a \mid s)\end{aligned}\] <ol> <li>可以从见过的状态泛化到没有见到的状态</li> <li>可以通过MC或者TD方法训练\(\mathbf{w}\)</li> </ol> </li> <li>一些近似方法：其中线性方法和神经网络是我们关注的重点 <ol> <li>特征线性组合</li> <li>神经网络</li> <li>决策树</li> <li>最近邻</li> </ol> </li> </ol> <h1 id="二预测中的价值函数近似">二、预测中的价值函数近似</h1> <h2 id="21-有oracle模型预测vfa">2.1 有Oracle/模型预测VFA</h2> <h3 id="211-基本内容">2.1.1 基本内容</h3> <ol> <li>Oracle：我们知道gt的value function \(v^\pi(s)\)，对所有状态\(s\)都是已知的。</li> <li>我们的目标是学习对\(v^\pi(s)\)的近似。</li> <li> <p>因此我们MSE来定义loss：</p> \[J(\mathbf{w})=\mathbb{E}_\pi\left[\left(v^\pi(s)-\hat{v}(s, \mathbf{w})\right)^2\right]\] </li> <li> <p>梯度下降：</p> \[\begin{aligned}\Delta \mathbf{w} &amp; =-\frac{1}{2} \alpha \nabla_{\mathbf{w}} J(\mathbf{w}) \\\mathbf{w}_{t+1} &amp; =\mathbf{w}_t+\Delta \mathbf{w}\end{aligned}\] </li> </ol> <h3 id="212-线性方法">2.1.2 线性方法</h3> <ol> <li> <p>如何通过特征向量表示状态？</p> \[\mathbf{x}(s)=\left(x_1(s), \ldots, x_n(s)\right)^T\] </li> <li> <p>价值函数——特征线性组合：</p> </li> </ol> \[\hat{v}(s, \mathbf{w})=\mathbf{x}(s)^T \mathbf{w}=\sum_{j=1}^n x_j(s) w_j\] <ol> <li>目标函数：</li> </ol> \[J(\mathbf{w})=\mathbb{E}_\pi\left[\left(v^\pi(s)-\mathbf{x}(s)^T \mathbf{w}\right)^2\right]\] \[\Delta \mathbf{w}=\alpha\left(v^\pi(s)-\hat{v}(s, \mathbf{w})\right) \mathbf{x}(s)\] <p>😍 <strong><em>Update = StepSize × PredictionError × FeatureValue</em></strong></p> <h3 id="213-通过查特征表的线性方法">2.1.3 通过查特征表的线性方法</h3> <ol> <li>是一种特殊的线性VFA</li> <li>查表的方式是通过1-hot向量实现的，特征都是0或1：</li> </ol> \[\mathbf{x}^{\text {table }}(s)=\left(\mathbf{1}\left(s=s_1\right), \ldots, \mathbf{1}\left(s=s_n\right)\right)^T\] <ol> <li>事实上价值函数的每一项恰好就是训练参数：</li> </ol> \[\hat{v}(s, \mathbf{w})=\left(\mathbf{1}\left(s=s_1\right), \ldots, \mathbf{1}\left(s=s_n\right)\right)\left(w_1, \ldots, w_n\right)^T\\\hat{v}\left(s_k, \mathbf{w}\right)=w_k\] <h2 id="22-无模型预测vfa">2.2 无模型预测VFA</h2> <p>实践中，没有办法知道所有状态的gt价值函数，没有oracle。</p> <p>回想下在无模型的预测中做的事：</p> <ol> <li>目标是去通过给定策略 \(\pi\) 估计价值函数 \(v^\pi\)</li> <li>维护一个 \(v^\pi\) 或者 \(q^\pi\) 的表格</li> <li>每回合评估更新（MC） 或 每一步评估更新（TD）</li> </ol> <h3 id="221-增量式函数近似预测算法incremental-vfa-prediction-algorithms">2.2.1 <strong>增量式函数近似预测算法</strong>（Incremental VFA Prediction Algorithms）</h3> <ol> <li> <p>在有gt价值函数的情形下，我们有：</p> \[\Delta \mathbf{w}=\alpha\left(\textcolor{red}{v^\pi(s)}-\hat{v}(s, \mathbf{w})\right) \nabla_{\mathbf{w}} \hat{v}\left(s_t, \mathbf{w}\right)\] </li> <li> <p>在真实场景下没有\(\textcolor{red}{v^\pi(s)}\)，我们需要寻求替代方案：</p> <ul> <li> <p>在MC中，我们用实际return代替</p> \[\Delta \mathbf{w}=\alpha\left(\textcolor{red}{G_t}-\hat{v}\left(s_t, \mathbf{w}\right)\right) \nabla_{\mathbf{w}} \hat{v}\left(s_t, \mathbf{w}\right)\] </li> <li> <p>在TD(0)中，我们用\(\delta_t\)代替</p> \[\Delta \mathbf{w}=\alpha\left(\textcolor{red}{R_{t+1}+\gamma \hat{v}\left(s_{t+1}, \mathbf{w}\right)}-\hat{v}\left(s_t, \mathbf{w}\right)\right) \nabla_{\mathbf{w}} \hat{v}\left(s_t, \mathbf{w}\right)\] </li> </ul> </li> </ol> <h3 id="222-基于vfa的mc预测">2.2.2 基于VFA的MC预测</h3> <ol> <li>奖励 \(G_t\) 是unbiased的，但是对于真正的value function是带有noise的，只有采样大量的\(G_t\) 才能恢复出\(v^\pi(s_t)\)，因为\(\mathbb{E}[G_t]=v^\pi(s_t)\)。</li> <li> <p>因此我们可以把 \(G_t\) 看成label，\(S_t\) 看成input，做一个监督学习任务。</p> \[&lt;S_1, G_1&gt;,&lt;S_2, G_2&gt;, \ldots,&lt;s_t, G_T&gt;\] </li> <li> <p>MC策略评估更新可以写为：</p> \[\begin{aligned} \Delta \mathbf{w} &amp; =\alpha\left(G_t-\hat{v}\left(s_t, \mathbf{w}\right)\right) \nabla_{\mathbf{w}} \hat{v}\left(s_t, \mathbf{w}\right) \\ &amp; =\alpha\left(G_t-\hat{v}\left(s_t, \mathbf{w}\right)\right) \mathbf{x}\left(s_t\right) \end{aligned}\] </li> <li>MC预测在线性/非线性VFA中都可以收敛到globel最优。</li> </ol> <h3 id="223-基于vfa的td预测">2.2.3 基于VFA的TD预测</h3> <ol> <li>TD目标\(R_{t+1}+\gamma \hat{v}\left(s_{t+1}, \mathbf{w}\right)\)是biased的，因为\(\mathbb{E}[R_{t+1}+\gamma \hat{v}\left(s_{t+1}, \mathbf{w}\right)]\neq v^\pi (s_t)\)</li> <li> <p>我们有类似的监督学习训练pair如下：</p> \[&lt;S_1, R_2+\gamma \hat{v}\left(s_2, \mathbf{w}\right)&gt;,&lt;S_2, R_3+\gamma \hat{v}\left(s_3, \mathbf{w}\right)&gt;, \ldots,&lt;S_{T-1}, R_T&gt;\] </li> <li> <p>TD(0)中的更新可以写为：</p> \[\begin{aligned}\Delta \mathbf{w} &amp; =\alpha\left(R+\gamma \hat{v}\left(s^{\prime}, \mathbf{w}\right)-\hat{v}(s, \mathbf{w})\right) \nabla_{\mathbf{w}} \hat{v}(s, \mathbf{w}) \\&amp; =\alpha\left(R+\gamma \hat{v}\left(s^{\prime}, \mathbf{w}\right)-\hat{v}(s, \mathbf{w})\right) \mathbf{x}(s)\end{aligned}\] <p>这个也叫做semi-gradient，因为我们忽略了改变权重向量\(\mathbf{w}\)对于target的效应。</p> </li> <li>线性TD(0)才可以收敛到globel最优。</li> </ol> <h1 id="三-控制中的价值函数近似">三、 控制中的价值函数近似</h1> <p>广义策略迭代：</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter4/Untitled%201-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter4/Untitled%201-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter4/Untitled%201-1400.webp"/> <img src="/assets/img/RL_chapter4/Untitled%201.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>策略评估：近似策略评估，\(\hat{q}(., ., \mathbf{w}) \approx q^\pi\)</li> <li>策略改进：\(\varepsilon\)-贪心策略改进</li> </ul> <h2 id="31-有oracle模型控制vfa">3.1 有Oracle/模型控制VFA</h2> <ol> <li> <p>近似动作价值函数：</p> \[\hat{q}(s, a, \mathbf{w}) \approx q^\pi(s, a)\] </li> <li> <p>loss function:</p> \[J(\mathbf{w})=\mathbb{E}_\pi\left[\left(q^\pi(s, a)-\hat{q}(s, a, \mathbf{w})\right)^2\right]\] </li> <li> <p>梯度求解：</p> \[\Delta \mathbf{w}=\alpha\left(q^\pi(s, a)-\hat{q}(s, a, \mathbf{w})\right) \nabla_{\mathbf{w}} \hat{q}(s, a, \mathbf{w})\] </li> </ol> <h2 id="32-无模型控制中的vfa">3.2 无模型控制中的VFA</h2> <h3 id="321-增量控制算法incremental-control-algorithm">3.2.1 增量控制算法（Incremental Control Algorithm）</h3> <p>类似的，真实情况中我们没有oracle了。</p> <ol> <li> <p>MC：</p> \[\Delta \mathbf{w}=\alpha\left(\textcolor{red}{G_t}-\hat{q}\left(s_t, a_t, \mathbf{w}\right)\right) \nabla_{\mathbf{w}} \hat{q}\left(s_t, a_t, \mathbf{w}\right)\] </li> <li> <p>Sarsa：</p> \[\Delta \mathbf{w}=\alpha\left(\textcolor{red}{R_{t+1}+\gamma \hat{q}\left(s_{t+1}, a_{t+1}, \mathbf{w}\right)}-\hat{q}\left(s_t, a_t, \mathbf{w}\right)\right) \nabla_{\mathbf{w}} \hat{q}\left(s_t, a_t, \mathbf{w}\right)\] </li> <li> <p>Q-learning：</p> \[\Delta \mathbf{w}=\alpha\left(\textcolor{red}{R_{t+1}+\gamma \max _a \hat{q}\left(s_{t+1}, a, \mathbf{w}\right)}-\hat{q}\left(s_t, a_t, \mathbf{w}\right)\right) \nabla_{\mathbf{w}} \hat{q}\left(s_t, a_t, \mathbf{w}\right)\] </li> <li> <p>Semi-gradient Sarsa算法流程：</p> </li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter4/Untitled%202-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter4/Untitled%202-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter4/Untitled%202-1400.webp"/> <img src="/assets/img/RL_chapter4/Untitled%202.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="322-收敛性分析sarsaq-learning">3.2.2 收敛性分析（Sarsa/Q-learning）</h3> <ol> <li>TD中的VFA不遵从任何一个Loss function</li> <li>这个更新包含两个近似，都会引入噪声： <ol> <li>贝尔曼备份公式中的近似</li> <li>近似价值函数的近似</li> </ol> </li> <li>一个挑战：行为策略和目标策略不同，因此价值函数的VFA可能没法收敛。</li> <li>死亡三角： <ol> <li>函数近似：近似会引入误差</li> <li>bootstrap：TD依赖于之前的估计会引入bias，MC方法避免了这个问题</li> <li>off-policy训练：采样分布和实际分布差异比较大</li> </ol> </li> <li>收敛性总结：</li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter4/Untitled%203-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter4/Untitled%203-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter4/Untitled%203-1400.webp"/> <img src="/assets/img/RL_chapter4/Untitled%203.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="323-batch强化学习">3.2.3 batch强化学习</h3> <ol> <li>想法： <ol> <li>增量梯度下降更新是简单的，但是并不是有效率的采样方式，每走一步优化一次。</li> <li>基于batch的方法会找一批中的数据。</li> </ol> </li> <li> <p>建模：假设经验包含了\(&lt;state, value&gt;\)对。</p> \[D=\{&lt;s_i,v_i&gt;\}_{t=1}^T\] <p>迭代时重复两步操作：</p> <ol> <li>随机采样一对，\(&lt;s, v^\pi&gt;\sim \mathcal{D}\)</li> <li>用梯度下降法进行优化：\(\Delta \mathbf{w}=\alpha\left(v^\pi-\hat{v}(s, \mathbf{w})\right) \nabla_{\mathbf{w}} \hat{v}(s, \mathbf{w})\)</li> </ol> <p>等价于MSE loss</p> \[\mathbf{w}^{L S}=\underset{\mathbf{w}}{\arg \min } \sum_{t=1}^T\left(v_t^\pi-\hat{v}\left(s_t, \mathbf{w}\right)\right)^2\] </li> </ol> <h1 id="三深度q网络">三、深度Q网络</h1> <ol> <li>线性VFA vs 非线性VFA <ol> <li>线性VFA通常在给定正确的特征时效果很好，但是需要人工设计特征集。</li> <li>非线性VFA不需要人工涉及特征，直接用DNN</li> </ol> </li> <li>深度强化学习 <ol> <li>DNN需要表示：价值函数、策略函数以及环境模型</li> <li>优化：SGD</li> </ol> </li> </ol> <h2 id="31-deep-q-learning-dqn">3.1 Deep Q-Learning (DQN)</h2> <ol> <li>DQN通过神经网络表示Q函数</li> <li>DQN玩雅达利游戏： <ol> <li>端到端学习\(Q(s,a)\)，直接把像素帧作为输入</li> <li>输入最近4帧状态\(s\)的像素</li> <li>输出是\(Q(s,a)\)代表18个按钮位置</li> <li>奖励是这一步的分数</li> </ol> </li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter4/Untitled%204-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter4/Untitled%204-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter4/Untitled%204-1400.webp"/> <img src="/assets/img/RL_chapter4/Untitled%204.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ol> <li>两个重要的问题： <ol> <li>样本之间的关系怎么处理？像素级别的关联很高。解决：experience replay</li> <li>target是带有noise的。解决：固定Q targets</li> </ol> </li> </ol> <h2 id="32-dqnexperience-replay">3.2 DQN：experience replay</h2> <ol> <li>为了减少样本之间的关联性，我们把 \((s_t,a_t,r_t,s_{t+1})\) 存进一个回放状态转移memory \(D\)，这是随机打乱的。希望采样出来的经验和目前状态没有很强的相关性。</li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter4/Untitled%205-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter4/Untitled%205-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter4/Untitled%205-1400.webp"/> <img src="/assets/img/RL_chapter4/Untitled%205.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ol> <li>经验回放具体步骤： <ol> <li>在数据集中采样经验：\(\left(s, a, r, s^{\prime}\right) \sim \mathcal{D}\)</li> <li>计算采样出的价值目标：\(r+\gamma \max _{a^{\prime}} \hat{Q}\left(s^{\prime}, a^{\prime}, \mathbf{w}\right)\)</li> <li> <p>通过SGD算法更新网络权重:</p> \[\Delta \mathbf{w}=\alpha\left(r+\gamma \max _{a^{\prime}} \hat{Q}\left(s^{\prime}, a^{\prime}, \mathbf{w}\right)-Q(s, a, \mathbf{w})\right) \nabla_{\mathbf{w}} \hat{Q}(s, a, \mathbf{w})\] </li> </ol> </li> <li>固定Q目标 <ol> <li>为了提高训练的稳定性，希望在多次更新时，计算target时固定其权重。</li> <li>用一组新的参数\(\mathbf{w^-}\)作为使用的权重，\(\mathbf{w}\)是我们需要更新的权重。</li> <li>固定目标的经验回放具体步骤： <ol> <li>在数据集中采样经验：\(\left(s, a, r, s^{\prime}\right) \sim \mathcal{D}\)</li> <li>计算采样出的价值目标：\(r+\gamma \max _{a^{\prime}} \hat{Q}\left(s^{\prime}, a^{\prime}, \textcolor{red}{\mathbf{w}^-} \right)\)</li> <li> <p>通过SGD算法更新网络权重:</p> \[\Delta \mathbf{w}=\alpha\left(r+\gamma \max _{a^{\prime}} \hat{Q}\left(s^{\prime}, a^{\prime}, \textcolor{red}{\mathbf{w}^{-}}\right)-Q(s, a, \mathbf{w})\right) \nabla_{\mathbf{w}} \hat{Q}(s, a, \mathbf{w})\] </li> </ol> </li> </ol> <p>😍 为什么需要固定目标？ 一开始更新的时候，对于Q的估计和Q的目标每一步都在变化。如果我们希望Q的估计可以逼近Q的目标，那么一个很好的方式就是固定Q的目标，让对Q的估计更精准。</p> </li> </ol>]]></content><author><name></name></author><category term="Reinforcement-Learning"/><category term="intro"/><summary type="html"><![CDATA[VFA and Q-learning]]></summary></entry><entry><title type="html">第五章 Policy Optimization Foundation (策略优化基础篇)</title><link href="https://gszfwsb.github.io/blog/2023/RL-lecture5/" rel="alternate" type="text/html" title="第五章 Policy Optimization Foundation (策略优化基础篇)"/><published>2023-10-30T00:00:00+00:00</published><updated>2023-10-30T00:00:00+00:00</updated><id>https://gszfwsb.github.io/blog/2023/RL-lecture5</id><content type="html" xml:base="https://gszfwsb.github.io/blog/2023/RL-lecture5/"><![CDATA[<h1 id="一基于策略的强化学习">一、基于策略的强化学习</h1> <ol> <li>基于价值的RL和基于策略的RL有何区别？ <ol> <li> <p><strong>决定性策略</strong>是直接通过贪心算法，由价值函数生成我们应该采取的动作：</p> \[a_t=\arg\max_a Q\left(a, s_t\right)\] </li> <li> <p><strong>随机性策略</strong>：我们也可以直接建模策略函数为一个参数化概率分布\(\pi_{\theta}(a\mid s)\)，参数为\(\theta\)</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> </figure></div></div> </li> </ol> </li> </ol> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter5/Untitled-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter5/Untitled-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter5/Untitled-1400.webp"/> <img src="/assets/img/RL_chapter5/Untitled.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <p>&lt;/figure&gt;</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>        &lt;/div&gt;
    &lt;/div&gt;

    
3. 对比：
    
    | 基于价值的RL | 基于策略的RL | 演员-评论员 |
    | --- | --- | --- |
    | 学习价值函数 | 没有价值函数 | 同时学策略和价值函数 |
    | 通过价值函数得到隐式的策略 | 直接学习策略 |  |
</code></pre></div></div> <ol> <li>policy-based RL特点: 有点类似监督学习 <ol> <li>优势： <ol> <li>更好的收敛性：可以保证收敛，至少是local optimum</li> <li>策略梯度在高维动作空间是更有效的</li> <li>策略梯度可以学习随机策略，但是价值函数不能。</li> </ol> </li> <li>劣势： <ol> <li>经常会收敛到局部最优解</li> <li>策略评估会有很大的方差</li> </ol> </li> </ol> </li> </ol> <h1 id="二mc策略梯度">二、MC策略梯度</h1> <h2 id="21-策略优化">2.1 策略优化</h2> <ol> <li>策略优化目标：给定一个带参 \(\theta\) 策略近似器 \(\pi_{\theta}(s,a)\)，找到最优\(\theta\)</li> <li>如何衡量策略 \(\pi_{\theta}(s,a)\) 的质量？ <ol> <li> <p>在离散（有轮数）环境中我们用<strong>开始价值</strong></p> \[J_1(\theta)=V^{\pi_\theta}\left(s_1\right)=\mathbb{E}_{\pi_\theta}\left[v_1\right]\] </li> <li> <p>在连续的环境（没有终止）我们可以用<strong>价值均值</strong>或者<strong>回报均值</strong></p> <ul> <li> <p>价值均值：</p> \[J_{a v V}(\theta)=\sum_s d^{\pi_\theta}(s) V^{\pi_\theta}(s)\] </li> <li> <p>回报均值：</p> \[J_{a v R}(\theta)=\sum_s d^{\pi_\theta}(s) \sum_a \pi_\theta(s, a) R(s, a)\] <p>其中\(\pi_\theta\)的马尔可夫分布稳态分布是\(d^{\pi_\theta}\)。</p> </li> </ul> </li> </ol> </li> <li> <p>策略价值：和我们在基于价值的RL中定义一样。</p> \[\begin{aligned}J(\theta) &amp; =\mathbb{E}_{\tau \sim \pi_\theta}\left[\sum_t R\left(s_t^\tau, a_t^\tau\right)\right] \\&amp; \approx \frac{1}{m} \sum_m \sum_t R\left(s_t^m, a_t^m\right)\end{aligned}\] <ul> <li>\(\tau\)是我们从策略函数\(\pi_\theta\)中的采样轨迹</li> <li>我们忽略折扣</li> </ul> </li> <li> <p>基于策略的RL的目标：找到参数使得\(J(\theta)\)最大化</p> \[\theta^*=\underset{\theta}{\arg \max } \mathbb{E}_{\tau \sim \pi_\theta}\left[\sum_t R\left(s_t^\tau, a_t^\tau\right)\right]\] <ul> <li>如果\(J(\theta)\)可微，我们可以用基于梯度的方法解</li> </ul> </li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter5/Untitled%201-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter5/Untitled%201-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter5/Untitled%201-1400.webp"/> <img src="/assets/img/RL_chapter5/Untitled%201.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>如果\(J(\theta)\)不可微或者难以求导，我们可以用一些不用梯度的黑盒优化方法： 交叉熵法 (CEM)</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter5/Untitled%202-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter5/Untitled%202-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter5/Untitled%202-1400.webp"/> <img src="/assets/img/RL_chapter5/Untitled%202.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>有限差分</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter5/Untitled%203-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter5/Untitled%203-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter5/Untitled%203-1400.webp"/> <img src="/assets/img/RL_chapter5/Untitled%203.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ol> <li>计算策略梯度： <ol> <li>假设策略非0时处处可微，我们可以计算梯度\(\nabla_\theta \pi_\theta(s, a)\)</li> <li> <p>一个计算trick (likelihood ratios)：优化目标就是\(\nabla_\theta \log \pi_\theta(s, a)\)</p> \[\textcolor{blue}{\begin{aligned}\nabla_\theta \pi_\theta(s, a) &amp; =\pi_\theta(s, a) \frac{\nabla_\theta \pi_\theta(s, a)}{\pi_\theta(s, a)} \\&amp; =\pi_\theta(s, a) \nabla_\theta \log \pi_\theta(s, a)\end{aligned}}\] </li> </ol> <ul> <li> <p>例1：高斯策略</p> <p>在连续动作空间，一个高斯策略的定义十分自然。其均值是状态特征的线性组合：\(\mu(s)=\phi(s)^T \theta\)。方差可以是参数化的也可以是常数。策略是连续、高斯的，即\(a \sim \mathcal{N}\left(\mu(s), \sigma^2\right)\)</p> <p>那么分数为：</p> \[\nabla_\theta \log \pi_\theta(s, a)=\frac{(a-\mu(s)) \phi(s)}{\sigma^2}\] </li> <li> <p>例2：softmax策略</p> <p>简单策略模型，用特征线性组合作为动作权重。其动作概率和权重大小指数成正比，即</p> \[\pi_\theta(s, a)=\frac{\exp ^{\phi(s, a)^T \theta}}{\sum_{a^{\prime}} \exp ^{\phi\left(s, a^{\prime}\right)^T \theta}}\] <p>那么分数为：</p> \[\nabla_\theta \log \pi_\theta(s, a)=\phi(s, a)-\mathbb{E}_{\pi_\theta}[\phi(s, .)]\] </li> </ul> </li> </ol> <h2 id="22-单步mdp的策略梯度">2.2 单步MDP的策略梯度</h2> <p>考虑单步MDP的简单情况： 从状态\(s\sim d(s)\)开始，到一个时间单位后结束，奖励为\(r=R(s,a)\)。我们用似然比例去计算策略梯度：</p> \[\begin{aligned}J(\theta) =\mathbb{E}_{\pi_\theta}[r] =\sum_{s \in \mathcal{S}} d(s) \sum_{a \in \mathcal{A}} \pi_\theta(s, a) r\end{aligned}\] <p>其梯度为：</p> \[\begin{aligned}\nabla_\theta J(\theta) &amp; =\sum_{s \in \mathcal{S}} d(s) \sum_{a \in \mathcal{A}} \pi_\theta(s, a) \nabla_\theta \log \pi_\theta(s, a) r \\&amp; =\mathbb{E}_{\pi_\theta}\left[r \nabla_\theta \log \pi_\theta(s, a)\right]\end{aligned}\] <h2 id="23-多步mdp的策略梯度">2.3 多步MDP的策略梯度</h2> <ol> <li>一回合的轨迹可以表示为：\(\tau=\left(s_0, a_0, r_1, \ldots s_{T-1}, a_{T-1}, r_T, s_T\right) \sim\left(\pi_\theta, P\left(s_{t+1} \mid s_t, a_t\right)\right)\)</li> <li>其轨迹的奖励为\(R(\tau)=\sum_{t=0}^{T-1} R\left(s_t, a_t\right)\)</li> <li> <p>策略目标为：</p> \[J(\theta)=\mathbb{E}_{\pi_\theta}\left[\sum_{t=0}^{T-1} R\left(s_t, a_t\right)\right]=\sum_\tau P(\tau;\theta) R(\tau)\] <p>其中\(P(\tau;\theta)=d(s_0)\pi_{\theta}(a_0\mid s_0)p(s_1\mid s_0,a_0)\cdots \pi_{\theta}(a_{T-1}\mid s_{T-1})p(s_T \mid s_{T-1},a_{T-1})=d\left(s_0\right) \prod_{t=0}^{T-1} \pi_\theta\left(a_t \mid s_t\right) p\left(s_{t+1} \mid s_t, a_t\right)\)表示执行策略\(\pi_\theta\)时在轨迹中的概率</p> </li> <li> <p>总体目标：max \(J(\theta)\)</p> \[\theta^*=\underset{\theta}{\arg \max } J(\theta)=\underset{\theta}{\arg \max } \sum_\tau P(\tau;\theta) R(\tau)\] </li> <li> <p>计算梯度：</p> \[\begin{aligned}\nabla_\theta J(\theta) &amp; =\nabla_\theta \sum_\tau P(\tau;\theta) R(\tau) \\&amp; =\sum_\tau \nabla_\theta P(\tau;\theta) R(\tau) \\&amp; =\sum_\tau \frac{P(\tau;\theta)}{P(\tau;\theta)} \nabla_\theta P(\tau;\theta) R(\tau) \\&amp; =\sum_\tau P(\tau;\theta) R(\tau) \frac{\nabla_\theta P(\tau;\theta)}{P(\tau;\theta)} \\&amp; =\sum_\tau P(\tau;\theta) R(\tau) \nabla_\theta \log P(\tau;\theta)\end{aligned}\] </li> <li> <p>经验估计策略，近似：</p> \[\nabla_\theta J(\theta) \approx \frac{1}{m} \sum_{i=1}^m R\left(\tau_i\right) \nabla_\theta \log P\left(\tau_i;\theta\right)\] </li> <li> <p>分解轨迹为状态和动作 \(\nabla_\theta \log P(\tau;\theta)\)</p> \[\begin{aligned}\nabla_\theta \log P(\tau;\theta) &amp; =\nabla_\theta \log \left[d\left(s_0\right) \prod_{t=0}^{T-1} \pi_\theta\left(a_t \mid s_t\right) p\left(s_{t+1} \mid s_t, a_t\right)\right] \\&amp; =\nabla_\theta\left[\log d\left(s_0\right)+\sum_{t=0}^{T-1} \log \pi_\theta\left(a_t \mid s_t\right)+\log p\left(s_{t+1} \mid s_t, a_t\right)\right] \\&amp; =\sum_{t=0}^{T-1} \nabla_\theta \log \pi_\theta\left(a_t \mid s_t\right)\end{aligned}\] \[\textcolor{blue}{\nabla_\theta J(\theta) \approx \frac{1}{m} \sum_{i=1}^m R\left(\tau_i\right) \sum_{t=0}^{T-1} \nabla_\theta \log \pi_\theta\left(a_t^i \mid s_t^i\right)}\] <p>我们不需要知道<strong>转移模型</strong>。</p> </li> </ol> <h1 id="三减少策略梯度的方差">三、减少策略梯度的方差</h1> <h2 id="31-理解梯度优化的过程">3.1 理解梯度优化的过程</h2> <p>我们考察的梯度是\(E_{\tau \sim \pi_\theta}[R(\tau)]\).更一般的，如果我们需要计算函数\(f(x)\)的期望，我们有：</p> \[\begin{aligned}\nabla_\theta \mathbb{E}_{p(x;\theta)}[f(x)] &amp; =\mathbb{E}_{p(x;\theta)}\left[f(x) \nabla_\theta \log p(x;\theta)\right] \\&amp; \approx \frac{1}{S} \sum_{s=1}^S f\left(x_s\right) \nabla_\theta \log p\left(x_s;\theta\right), \text { where } x_s \sim p(x;\theta)\end{aligned}\] <ol> <li>如何理解梯度： <ol> <li>移动分布\(p\)（优化参数\(\theta\)）使得其未来的样本\(x\)可以得到更高的分数</li> <li>向量\(f(x)\nabla_\theta \log p(x;\theta)\)把样本的log-likelihood推向一个表示它的分数。</li> </ol> </li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter5/Untitled%204-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter5/Untitled%204-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter5/Untitled%204-1400.webp"/> <img src="/assets/img/RL_chapter5/Untitled%204.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ol> <li>与极大似然比较： <ol> <li> <p>策略梯度估计：</p> \[\nabla_\theta J(\theta) \approx \frac{1}{M} \sum_{m=1}^M\left(\sum_{t=1}^T \nabla_\theta \log \pi_\theta\left(a_{t, m} \mid s_{t, m}\right)\right)\textcolor{red}{\left(\sum_{t=1}^T r\left(s_{t, m}, a_{t, m}\right)\right)}\] </li> <li> <p>极大似然估计：</p> \[\nabla_\theta J_{M L}(\theta) \approx \frac{1}{M} \sum_{m=1}^M\left(\sum_{t=1}^T \nabla_\theta \log \pi_\theta\left(a_{t, m} \mid s_{t, m}\right)\right)\] </li> </ol> <p>主要区别：<strong>给好的动作更大的可能性，给不好的动作更小的可能性</strong></p> </li> </ol> <h2 id="32-策略梯度中方差较大的问题">3.2 策略梯度中方差较大的问题</h2> <ol> <li> <p>我们的优化如下：</p> \[\nabla_\theta J(\theta) \approx \frac{1}{m} \sum_{i=1}^m R\left(\tau_i\right) \sum_{t=0}^{T-1} \nabla_\theta \log \pi_\theta\left(a_t^i \mid s_t^i\right)\] <ul> <li>尽管是Unbiased，但是很Noisy</li> <li>如何修改？ <ul> <li>用时间因果</li> <li>引入baseline</li> </ul> </li> </ul> </li> <li> <p>通过时间因果解决大方差问题：</p> <ol> <li>此前我们有：\(\nabla_\theta \mathbb{E}_\tau[R]=\mathbb{E}_\tau \left[\left(\sum_{t=0}^{T-1} r_t\right)\left(\sum_{t=0}^{T-1} \nabla_\theta \log \pi_\theta\left(a_t \mid s_t\right)\right)\right]\)</li> <li> <p>我们可以得到某一时刻的单一奖励：</p> \[\nabla_\theta \mathbb{E}_\tau\left[r_{t^{\prime}}\right]=\mathbb{E}_\tau\left[r_{t^{\prime}} \sum_{t=0}^{t^{\prime}} \nabla_\theta \log \pi_\theta\left(a_t \mid s_t\right)\right]\] </li> <li> <p>通过对\(t\)求和，我们可以得到：</p> \[\begin{aligned}\nabla_\theta J(\theta)=\nabla_\theta \mathbb{E}_{\tau \sim \pi_\theta}[R] &amp; =\mathbb{E}_\tau\left[\sum_{t^{\prime}=0}^{T-1} r_{t^{\prime}} \sum_{t=0}^{t^{\prime}} \nabla_\theta \log \pi_\theta\left(a_t \mid s_t\right)\right] \\&amp; =\mathbb{E}_\tau\left[\sum_{t=0}^{T-1} \nabla_\theta \log \pi_\theta\left(a_t \mid s_t\right) \sum_{\textcolor{red}{t^{\prime}=t}}^{T-1} r_{t^{\prime}}\right] \\&amp; =\mathbb{E}_\tau\left[\sum_{t=0}^{T-1} G_t \cdot \nabla_\theta \log \pi_\theta\left(a_t \mid s_t\right)\right]\end{aligned}\] </li> <li> <p>因此我们有：</p> \[\nabla_\theta \mathbb{E}_{\tau \sim \pi_\theta}[R]=\mathbb{E}_\tau\left[\sum_{t=0}^{T-1} G_t \cdot \nabla_\theta \log \pi_\theta\left(a_t \mid s_t\right)\right]\] <ul> <li>\(G_t=\sum_{t^{\prime}=t}^{T-1} r_{t^{\prime}}\) 是\(t\)步轨迹中的回报</li> <li>因果：时间靠后的策略不能影响时间靠前的策略</li> <li>所以我们可以得到如下的估计更新：</li> </ul> \[\nabla_\theta \mathbb{E}[R] \approx \frac{1}{m} \sum_{i=1}^m \sum_{t=0}^{T-1} G_t^{(i)} \cdot \nabla_\theta \log \pi_\theta\left(a_t^i \mid s_t^i\right)\] </li> </ol> <p>😍 例：<strong>REINFORCE: 一种MC策略梯度算法</strong></p> <p>算法简单从策略中采样多个轨迹的样本，并且同时更新\(\theta\)参数：</p> </li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter5/Untitled%205-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter5/Untitled%205-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter5/Untitled%205-1400.webp"/> <img src="/assets/img/RL_chapter5/Untitled%205.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ol> <li>通过引入baseline解决方差大的问题： <ol> <li> <p>原始更新：\(G_t=\sum_{t^{\prime}=t}^{T-1} r_{t^{\prime}}\)可能有很大的方差</p> \[\nabla_\theta \mathbb{E}_{\tau \sim \pi_\theta}[R]=\mathbb{E}_\tau\left[\sum_{t=0}^{T-1} \textcolor{red}{G_t} \cdot \nabla_\theta \log \pi_\theta\left(a_t \mid s_t\right)\right]\] </li> <li> <p>可以通过减去一个baseline来减小方差</p> \[\nabla_\theta \mathbb{E}_{\tau \sim \pi_\theta}[R]=\mathbb{E}_\tau\left[\sum_{t=0}^{T-1} \textcolor{red}{(G_t-b(s_t))} \cdot \nabla_\theta \log \pi_\theta\left(a_t \mid s_t\right)\right]\] </li> <li> <p>一个好的baseline就是期望奖励：</p> \[b\left(s_t\right)=\mathbb{E}\left[r_t+r_{t+1}+\ldots+r_{T-1}\right]\] </li> <li> <p>解释：我们优化的不再是\(G_t\)本身，而是\(G_t\)可以比baseline好多少。可以证明这样可以降低方差：</p> \[\begin{aligned}\mathbb{E}_\tau\left[\nabla_\theta \log \pi_\theta\left(a_t \mid s_t\right) b\left(s_t\right)\right] &amp; =0, \\E_\tau\left[\nabla_\theta \log \pi_\theta\left(a_t \mid s_t\right)\left(G_t-b\left(s_t\right)\right)\right] &amp; =E_\tau\left[\nabla_\theta \log \pi_\theta\left(a_t \mid s_t\right) G_t\right] \\\operatorname{Var}_\tau\left[\nabla_\theta \log \pi_\theta\left(a_t \mid s_t\right)\left(G_t-b\left(s_t\right)\right)\right] &amp; &lt;\operatorname{Var}_\tau\left[\nabla_\theta \log \pi_\theta\left(a_t \mid s_t\right) G_t\right]\end{aligned}\] </li> <li> <p>同样，我们也可以给baseline一个参数来进行学习：</p> \[\nabla_\theta \mathbb{E}_{\tau \sim \pi_\theta}[R]=\mathbb{E}_\tau\left[\sum_{t=0}^{T-1} \textcolor{red}{(G_t-b_{\mathbf{w}}(s_t))} \cdot \nabla_\theta \log \pi_\theta\left(a_t \mid s_t\right)\right]\] <p>\(\mathbf{w}\)用于训练baseline，因此我们有两套参数：\(\mathbf{w}\)和\(\theta\).</p> </li> </ol> <p>😍 带有baseline的策略梯度算法：</p> </li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter5/Untitled%206-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter5/Untitled%206-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter5/Untitled%206-1400.webp"/> <img src="/assets/img/RL_chapter5/Untitled%206.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h1 id="四演员-评论员">四、<strong>演员-评论员</strong></h1> <ol> <li> <p>除了引入baseline和时间因果，我们还可以通过引入Critic的方式来降低方差，这就引入了一个新的算法。</p> \[\nabla_\theta \mathbb{E}_{\tau \sim \pi_\theta}[R]=\mathbb{E}_\tau\left[\sum_{t=0}^{T-1} \textcolor{red}{G_t} \cdot \nabla_\theta \log \pi_\theta\left(a_t \mid s_t\right)\right]\] <ul> <li>实践上\(G_t\)是MC策略梯度的一个样本，是对\(Q^{\pi_\theta}(s_t,a_t)\)的一个unbiased noisy估计。</li> <li> <p>实际上我们可以不用\(G_t\)，而用一个评论员去估计Q函数。</p> \[Q_{\mathbf{w}}(s, a) \approx Q^{\pi_\theta}(s, a)\] \[\nabla_\theta J(\theta)=\mathbb{E}_{\pi_\theta}\left[\sum_{t=0}^{T-1} Q_{\textcolor{red}{\mathbf{w}}}\left(s_t, a_t\right) \cdot \nabla_\theta \log \pi_{\textcolor{red}\theta}\left(a_t \mid s_t\right)\right]\] </li> </ul> </li> <li> <p>上式就是Actor-Critic策略梯度：</p> <ul> <li>Actor：policy function，用于生成动作，学习参数\(\theta\)</li> <li> <p>Critic：value function，衡量动作的回报，学习参数\(\mathbf{w}\)</p> <blockquote> <p>Critic：类似于策略评估的作用</p> </blockquote> </li> </ul> </li> </ol> <h2 id="41-动作价值动作评论员算法qac">4.1 动作价值动作评论员算法(QAC)</h2> <ol> <li> <p>用一个线性价值函数近似：</p> \[Q_{\mathbf{w}}(s, a)=\psi(s, a)^T \mathbf{w}\] <p>其中，评论员用一个线性TD(0)来更新参数\(\mathbf{w}\)，二演员通过策略梯度更新参数\(\theta\).</p> </li> <li>具体算法流程：</li> <li>演员-评论员函数近似方法：</li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter5/Untitled%207-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter5/Untitled%207-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter5/Untitled%207-1400.webp"/> <img src="/assets/img/RL_chapter5/Untitled%207.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>我们可以有两个不同的函数来近似策略函数和价值函数，但是我们也可以用一个共享的网络来涉及同时得到两个函数。</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter5/Untitled%208-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter5/Untitled%208-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter5/Untitled%208-1400.webp"/> <img src="/assets/img/RL_chapter5/Untitled%208.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="42-带basline的演员-评论员算法a2c--tdac">4.2 带basline的演员-评论员算法(A2C &amp; TDAC)</h2> <ol> <li> <p>我们定义Q函数如下：</p> \[Q^{\pi, \gamma}(s, a)=\mathbb{E}_\pi\left[r_1+\gamma r_2+\ldots \mid s_1=s, a_1=a\right]\] <p>那么其实价值函数就是一个很好的baseline：</p> \[\begin{aligned}V^{\pi, \gamma}(s) &amp; =\mathbb{E}_\pi\left[r_1+\gamma r_2+\ldots \mid s_1=s\right] \\&amp; =\mathbb{E}_{a \sim \pi}\left[Q^{\pi, \gamma}(s, a)\right]\end{aligned}\] </li> <li> <p>advantage function：我们定义带有baseline（价值函数）的Q函数为advantage function，即</p> \[A^{\pi, \gamma}(s, a)=Q^{\pi, \gamma}(s, a)-V^{\pi, \gamma}(s)\] </li> <li> <p>策略梯度为：</p> \[\nabla_\theta J(\theta)=\mathbb{E}_{\pi_\theta}\left[\nabla_\theta \log \pi_\theta(s, a) A^{\pi, \gamma}(s, a)\right]\] </li> <li> <p>在策略梯度里面，我们可以用MC来估计回报，也可以用TD方法来估计。如果我们考虑一个n步的回报，我们有：</p> \[\begin{aligned}n=1(T D) &amp; G_t^{(1)}=r_{t+1}+\gamma v\left(s_{t+1}\right) \\n=2 \quad &amp; G_t^{(2)}=r_{t+1}+\gamma r_{t+2}+\gamma^2 v\left(s_{t+2}\right) \\n=\infty(M C) &amp; G_t^{(\infty)}=r_{t+1}+\gamma r_{t+2}+\ldots+\gamma^{T-t-1} r_T\end{aligned}\] <p>那么我们的advantage function估计可以写为：</p> \[\begin{aligned}&amp; \hat{A}_t^{(1)}=r_{t+1}+\gamma v\left(s_{t+1}\right)-v\left(s_t\right) \\&amp; \hat{A}_t^{(2)}=r_{t+1}+\gamma r_{t+2}+\gamma^2 v\left(s_{t+2}\right)-v\left(s_t\right) \\&amp; \hat{A}_t^{(\infty)}=r_{t+1}+\gamma r_{t+2}+\ldots+\gamma^{T-t-1} r_T-v\left(s_t\right)\end{aligned}\] <p>注意到，\(\hat{A}^{(\infty)}\)有大var，小bias，而\(\hat{A}^{(1)}\)有小var，大bias。</p> </li> <li> <p>不同时间尺度的演员：</p> <p>策略梯度可以在很多时间尺度上估计</p> \[\nabla_\theta J(\theta)=\mathbb{E}_{\pi_\theta}\left[\nabla_\theta \log \pi_\theta(s, a) A^{\pi_\theta}(s, a)\right]\] <ul> <li> <p>MC演员-评论员策略梯度用<strong>整个回报</strong>误差来优化：</p> \[\nabla_\theta J(\theta)=\alpha\left(G_t-V_\kappa\left(s_t\right)\right) \nabla_\theta \log \pi_\theta\left(s_t, a_t\right)\] </li> <li> <p>TD演员-评论员策略梯度用TD误差来优化：</p> \[\nabla_\theta J(\theta)=\alpha\left(r+\gamma V_\kappa\left(s_{t+1}\right)-V_\kappa\left(s_t\right)\right) \nabla_\theta \log \pi_\theta\left(s_t, a_t\right)\] </li> <li> <p>k步TD演员-评论员策略梯度用k步回报误差来优化：</p> \[\nabla_\theta J(\theta)=\alpha\left(\sum_{i=0}^k \gamma^i r_{t+i}+\gamma^k V_\kappa\left(s_{t+k}\right)-V_\kappa\left(s_t\right)\right) \nabla_\theta \log \pi_\theta\left(s_t, a_t\right)\] </li> </ul> <blockquote> <p>TD算法只需要优化critic的参数\(\kappa\)！</p> </blockquote> </li> </ol> <h1 id="五策略梯度小结">五、策略梯度小结</h1> <ol> <li>策略梯度可以解决不可微的情形：我们可以通过随机策略采样来结算</li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter5/Untitled%209-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter5/Untitled%209-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter5/Untitled%209-1400.webp"/> <img src="/assets/img/RL_chapter5/Untitled%209.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ol> <li>训练过程中我们会产生一些样本（图中不同分支），我们可以让不同样本尽可能得到比较好的结果（比如小的loss）。</li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter5/Untitled%2010-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter5/Untitled%2010-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter5/Untitled%2010-1400.webp"/> <img src="/assets/img/RL_chapter5/Untitled%2010.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ol> <li>策略梯度拓展：大部分SOTA的RL方法都是基于策略的。</li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter5/Untitled%2011-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter5/Untitled%2011-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter5/Untitled%2011-1400.webp"/> <img src="/assets/img/RL_chapter5/Untitled%2011.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ol> <li>不同RL流派： <ol> <li>基于价值的RL：通过DP解决RL <ol> <li>经典RL和控制论</li> <li>代表性算法：Deep Q-learning和其变种</li> <li>代表研究人员：Richard Sutton，David Silver</li> <li>代表机构：Deepmind</li> </ol> </li> <li>基于策略的RL：通过学习解决RL <ol> <li>机器学习和深度学习</li> <li>代表性算法：PG和其变种</li> <li>代表研究人员：Pieter Abeel, Sergey Levine, John Schulman</li> <li>代表机构：OpenAI, Berkeley</li> </ol> </li> </ol> </li> </ol>]]></content><author><name></name></author><category term="Reinforcement-Learning"/><category term="intro"/><summary type="html"><![CDATA[PO]]></summary></entry><entry><title type="html">第二章 Markov Decision Process (马尔可夫决策过程)</title><link href="https://gszfwsb.github.io/blog/2023/RL-lecture2/" rel="alternate" type="text/html" title="第二章 Markov Decision Process (马尔可夫决策过程)"/><published>2023-10-29T00:00:00+00:00</published><updated>2023-10-29T00:00:00+00:00</updated><id>https://gszfwsb.github.io/blog/2023/RL-lecture2</id><content type="html" xml:base="https://gszfwsb.github.io/blog/2023/RL-lecture2/"><![CDATA[<p>强化学习中，智能体与环境就是这样进行交互的，这个交互过程可以通过马尔可夫决策过程来表示，所以马尔可夫决策过程是强化学习的基本框架。</p> <p>在介绍马尔可夫决策过程之前，我们先介绍它的简化版本：马尔可夫过程（Markov process，MP）以及马尔可夫奖励过程（Markov reward process，MRP）。通过与这两种过程的比较，我们可以更容易理解马尔可夫决策过程。其次，我们会介绍马尔可夫决策过程中的<strong>策略评估（policy evaluation）</strong>，就是当给定决策后，我们怎么去计算它的价值函数。最后，我们会介绍马尔可夫决策过程的控制，具体有<strong>策略迭代（policy iteration）</strong> 和<strong>价值迭代（value iteration）</strong>两种算法。在马尔可夫决策过程中，它的环境是全部可观测的。但是很多时候环境里面有些量是不可观测的，但是这个部分观测的问题也可以转换成马尔可夫决策过程的问题。</p> <h1 id="一马尔可夫过程">一、马尔可夫过程</h1> <p>MP定义</p> <ul> <li>\(S\) 有限状态集</li> <li>\(P^a\) 动作转移模型，\(P\left(s_{t+1}=s^{\prime} \mid s_t=s\right)\)</li> <li>折扣银子 \(\gamma \in[0,1]\)</li> <li>MDP 是一个元组: \((S, P, \gamma)\)。</li> </ul> <h2 id="11-马尔可夫性质">1.1 马尔可夫性质</h2> <p>在随机过程中，马尔可夫性质 (Markov property) 是指一个随机过程在给定现在状态及所有过去状态情况下，其末来状态的条件概率分布仅依赖于<strong>当前状态</strong>。以离散随机过程为例，假设随机变量 \(X_0, X_1, \cdots, X_T\) 构成一个随机过程。这些随机变量的所有可能取值的集合被称为状态空间 (state space) 。如果 \(X_{t+1}\) 对于过去状态的条件概率分布仅是 \(X_t\) 的一个函数，则</p> \[p\left(X_{t+1}=x_{t+1} \mid X_{0: t}=x_{0: t}\right)=p\left(X_{t+1}=x_{t+1} \mid X_t=x_t\right)\] <p>其中， \(X_{0: t}\) 表示变量集合 \(X_0, X_1, \cdots, X_t ， x_{0: t}\) 为在状态空间中的状态序列 \(x_0, x_1, \cdots, x_t\) 。马尔可夫性质也可以描述为给定当前状态时，将来的状态与过去状态是<strong>条件独立</strong>的。如果某一个过程满足马尔可夫性质，那么末来的转移与过去的是独立的，它只取决于现在。马尔可夫性质是 所有马尔可夫过程的基础。</p> <h2 id="12-马尔可夫链">1.2 马尔可夫链</h2> <p>马尔可夫过程是一组具有马尔可夫性质的随机变量序列 \(s_1, \cdots, s_t\) ，其中下一个时刻的状态 \(s_{t+1}\) 只取决于当前状态 \(s_t\) 。我们设状态的历史为 \(h_t=\left\{s_1, s_2, s_3, \ldots, s_t\right\}\) ( \(h_t\) 包含了之前的所有状态)，则马尔可夫过程满足条件:</p> \[p\left(s_{t+1} \mid s_t\right)=p\left(s_{t+1} \mid h_t\right)\] <p>从当前 \(s_t\) 转移到 \(s_{t+1}\) ，它是直接就等于它之前所有的状态转移到 \(s_{t+1}\) 。<strong>离散时间的马尔可夫过程</strong>也称为<strong>马尔可夫链</strong> (Markov chain) 。马尔可夫链是最简单的马尔可夫过程，其状态是有限的。</p> <p>我们可以用状态转移矩阵 (state transition matrix) \(\boldsymbol{P}\) 来描述状态转移 \(p\left(s_{t+1}=s^{\prime} \mid s_t=s\right)\) :</p> \[\boldsymbol{P}=\left(\begin{array}{cccc} p\left(s_1 \mid s_1\right) &amp; p\left(s_2 \mid s_1\right) &amp; \ldots &amp; p\left(s_N \mid s_1\right) \\ p\left(s_1 \mid s_2\right) &amp; p\left(s_2 \mid s_2\right) &amp; \ldots &amp; p\left(s_N \mid s_2\right) \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ p\left(s_1 \mid s_N\right) &amp; p\left(s_2 \mid s_N\right) &amp; \ldots &amp; p\left(s_N \mid s_N\right) \end{array}\right)\] <p>状态转移矩阵类似于条件概率 (conditional probability)，它表示当我们知道当前我们在状态 \(s_t\) 时，到达下面所有状态的概率。所以它的每一行描述的是从一个节点到达所有其他节点的概率。</p> <h1 id="二马尔可夫奖励过程">二、马尔可夫奖励过程</h1> <p>MRP定义</p> <ul> <li>\(S\) 有限状态集</li> <li>\(P\) 转移模型，\(P\left(s_{t+1}=s^{\prime} \mid s_t=s\right)\)</li> <li>\(R\) 奖励函数 \(R\left(s_t=s, \right)=\mathbb{E}\left[r_t \mid s_t=s\right]\)</li> <li>折扣银子 \(\gamma \in[0,1]\)</li> <li>MDP 是一个元组: \((S, P, R, \gamma)\)。</li> </ul> <h2 id="21-回报和价值函数">2.1 回报和价值函数</h2> <p>马尔可夫奖励过程（Markov reward process, MRP）是<strong>马尔可夫链</strong>加上<strong>奖励函数</strong>。在马尔可夫奖励过程中，状态转移矩阵和状态都与马尔可夫链一样，只是多了<strong>奖励函数（reward function）</strong>。奖励函数\(R\)是一个期望，表示当我们到达某一个状态的时候，可以获得多大的奖励。这里另外定义了折扣因子\(\gamma\)。如果状态数是有限的，那么\(R\)可以是一个向量。</p> <ol> <li>范围（horizon）：是指一个回合的长度（每个回合最大的时间步数），它是由有限个步数决定的。</li> <li> <p>回报（return）：可以定义为奖励的逐步叠加，时刻\(t\)后的奖励序列为：</p> \[G_t=r_{t+1}+\gamma r_{t+2}+\gamma^2 r_{t+3}+\gamma^3 r_{t+4}+\ldots+\gamma^{T-t-1} r_T\] <p>其中， \(T\) 是最终时刻， \(\gamma\) 是折扣因子，越往后得到的奖励，折扣越多。这说明我们更希望得到现有的奖励，对末来的奖励要打折扣。当我们有了回 报之后，就可以定义状态的价值了，就是状态价值函数（state-value function）。对于马尔可夫奖励过程，状态价值函数被定义成回报的期望，即</p> \[\begin{aligned} V_t(s) &amp; =\mathbb{E}\left[G_t \mid s_t=s\right] \\ &amp; =\mathbb{E}\left[r_{t+1}+\gamma r_{t+2}+\gamma^2 r_{t+3}+\ldots+\gamma^{T-t-1} r_T \mid s_t=s\right] \end{aligned}\] <p>其中， \(G_t\) 是之前定义的折扣回报（discounted return) 。我们对 \(G_t\) 取了一个期望，期望就是从这个状态开始，我们可能获得多大的价值。所以期望也可以看成末来可能获得奖励的当前价值的表现，就是当我们进入某一个状态后，我们现在有多大的价值。</p> <p>😍 为什么用折扣因子？</p> <p><strong>第一</strong>，有些马尔可夫过程是带环的，它并不会终结，我们想避免无穷的奖励。</p> <p><strong>第二</strong>，我们并不能建立完美的模拟环境的模型，我们对未来的评估不一定是准确的，我们不一定完全信任模型，因为这种不确定性，所以我们对未来的评估增加一个折扣。我们想把这个不确定性表示出来，希望尽可能快地得到奖励，而不是在未来某一个点得到奖励。</p> <p><strong>第三</strong>，如果奖励是有实际价值的，我们可能更希望立刻就得到奖励，而不是后面再得到奖励（现在的钱比以后的钱更有价值）。</p> <p><strong>最后</strong>，我们也更想得到即时奖励。有些时候可以把折扣因子设为 0，我们就只关注当前的奖励。我们也可以把折扣因子设为 1，，对未来的奖励并没有打折扣，未来获得的奖励与当前获得的奖励是一样的。折扣因子可以作为强化学习智能体的一个超参数来进行调整，通过调整折扣因子，我们可以得到不同动作的智能体。</p> </li> </ol> <h2 id="22-贝尔曼方程">2.2 贝尔曼方程</h2> <p>从价值函数里面推导出<strong>贝尔曼方程（Bellman equation）</strong>，先上结论：</p> \[V(s)=\underbrace{R(s)}_{\text {即时奖励 }}+\underbrace{\gamma \sum_{s^{\prime} \in S} p\left(s^{\prime} \mid s\right) V\left(s^{\prime}\right)}_{\text {末来奖励的折扣总和 }}\] <p>其中，</p> <ul> <li>\(s^{\prime}\) 可以看成末来的所有状态，</li> <li>\(p\left(s^{\prime} \mid s\right)\) 是指从当前状态转移到末来状态的概率。</li> <li>\(V\left(s^{\prime}\right)\) 代表的是末来某一个状态的价值。我们从当前状态开始，有一定的概率去到末来的所有状态，所以我们要把 \(p\left(s^{\prime} \mid s\right)\) 写上去。我们得到了末来状态后，乘一个 \(\gamma\) ，这样就可以把末来的奖励打折扣。</li> <li>\(\gamma \sum_{s^{\prime} \in S} p\left(s^{\prime} \mid s\right) V\left(s^{\prime}\right)\) 可以看成末来奖励的折扣总和 (discounted sum of future reward)。 贝尔曼方程定义了当前状态与末来状态之间的关系。末来奖励的折扣总和加上即时奖励，就组成了贝尔曼方程。</li> <li> <p>推导过程</p> <p>可以先证明：\(\mathbb{E}\left[V\left(s_{t+1}\right) \mid s_t\right]=\mathbb{E}\left[\mathbb{E}\left[G_{t+1} \mid s_{t+1}\right] \mid s_t\right]=\mathbb{E}\left[G_{t+1} \mid s_t\right]\)。令\(s=s_t, g^{\prime}=G_{t+1}, s^{\prime}=s_{t+1}\)</p> \[\begin{aligned}\mathbb{E}\left[\mathbb{E}\left[G_{t+1} \mid s_{t+1}\right] \mid s_t\right] &amp; =\mathbb{E}\left[\mathbb{E}\left[g^{\prime} \mid s^{\prime}\right] \mid s\right] \\&amp; =\mathbb{E}\left[\sum_{g^{\prime}} g^{\prime} p\left(g^{\prime} \mid s^{\prime}\right) \mid s\right] \\&amp; =\sum_{s^{\prime}} \sum_{g^{\prime}} g^{\prime} p\left(g^{\prime} \mid s^{\prime}, s\right) p\left(s^{\prime} \mid s\right) \\&amp; =\sum_{s^{\prime}} \sum_{g^{\prime}} \frac{g^{\prime} p\left(g^{\prime} \mid s^{\prime}, s\right) p\left(s^{\prime} \mid s\right) p(s)}{p(s)} \\&amp; =\sum_{s^{\prime}} \sum_{g^{\prime}} \frac{g^{\prime} p\left(g^{\prime} \mid s^{\prime}, s\right) p\left(s^{\prime}, s\right)}{p(s)} \\&amp; =\sum_{s^{\prime}} \sum_{g^{\prime}} \frac{g^{\prime} p\left(g^{\prime}, s^{\prime}, s\right)}{p(s)} \\&amp; =\sum_{s^{\prime}} \sum_{g^{\prime}} g^{\prime} p\left(g^{\prime}, s^{\prime} \mid s\right) \\&amp; =\sum_{g^{\prime}} \sum_{s^{\prime}} g^{\prime} p\left(g^{\prime}, s^{\prime} \mid s\right) \\&amp; =\sum_{g^{\prime}} g^{\prime} p\left(g^{\prime} \mid s\right) \\&amp; =\mathbb{E}\left[g^{\prime} \mid s\right]=\mathbb{E}\left[G_{t+1} \mid s_t\right]\end{aligned}\] \[\begin{aligned}V(s) &amp; =\mathbb{E}\left[G_t \mid s_t=s\right] \\&amp; =\mathbb{E}\left[R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+\ldots \mid s_t=s\right] \\&amp; =\mathbb{E}\left[R_{t+1} \mid s_t=s\right]+\gamma \mathbb{E}\left[R_{t+2}+\gamma R_{t+3}+\gamma^2 R_{t+4}+\ldots \mid s_t=s\right] \\&amp; =R(s)+\gamma \mathbb{E}\left[G_{t+1} \mid s_t=s\right] \\&amp; =R(s)+\gamma \mathbb{E}\left[V\left(s_{t+1}\right) \mid s_t=s\right] \\&amp; =R(s)+\gamma \sum_{s^{\prime} \in S} p\left(s^{\prime} \mid s\right) V\left(s^{\prime}\right)\end{aligned}\] </li> </ul> <p>可以把贝尔曼方程写成矩阵的形式：</p> \[\left(\begin{array}{c}V\left(s_1\right) \\V\left(s_2\right) \\\vdots \\V\left(s_N\right)\end{array}\right)=\left(\begin{array}{c}R\left(s_1\right) \\R\left(s_2\right) \\\vdots \\R\left(s_N\right)\end{array}\right)+\gamma\left(\begin{array}{cccc}p\left(s_1 \mid s_1\right) &amp; p\left(s_2 \mid s_1\right) &amp; \ldots &amp; p\left(s_N \mid s_1\right) \\p\left(s_1 \mid s_2\right) &amp; p\left(s_2 \mid s_2\right) &amp; \ldots &amp; p\left(s_N \mid s_2\right) \\\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\p\left(s_1 \mid s_N\right) &amp; p\left(s_2 \mid s_N\right) &amp; \ldots &amp; p\left(s_N \mid s_N\right)\end{array}\right)\left(\begin{array}{c}V\left(s_1\right) \\V\left(s_2\right) \\\vdots \\V\left(s_N\right)\end{array}\right)\] <p>每一行来看，向量\(V\)乘状态转移矩阵里面的某一行，再加上它当前可以得到的奖励，就会得到它当前的价值。当我们把贝尔曼方程写成矩阵形式后，可以直接求解：</p> \[\begin{aligned}&amp; \boldsymbol{V}=\boldsymbol{R}+\gamma \boldsymbol{P} \boldsymbol{V} \\&amp; \boldsymbol{V}=(\boldsymbol{I}-\gamma \boldsymbol{P})^{-1} \boldsymbol{R} \end{aligned}\] <p>但是求解复杂度是\(O(N^3)\)。求解算法有<strong>蒙特卡洛</strong>、<strong>动态规划</strong>和<strong>时序差分学习</strong></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter2/Untitled-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter2/Untitled-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter2/Untitled-1400.webp"/> <img src="/assets/img/RL_chapter2/Untitled.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter2/Untitled%201-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter2/Untitled%201-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter2/Untitled%201-1400.webp"/> <img src="/assets/img/RL_chapter2/Untitled%201.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h1 id="三马尔可夫决策过程">三、马尔可夫决策过程</h1> <p>MDP定义</p> <ul> <li>\(S\) 有限状态集</li> <li>\(A\) 有限动作集</li> <li>\(P^a\) 动作转移模型，\(P\left(s_{t+1}=s^{\prime} \mid s_t=s, a_t=a\right)\)</li> <li>\(R\) 奖励函数 \(R\left(s_t=s, a_t=a\right)=\mathbb{E}\left[r_t \mid s_t=s, a_t=a\right]\)</li> <li>折扣因子 \(\gamma \in[0,1]\)</li> <li>MDP 是一个元组: \((S, A, P, R, \gamma)\)。</li> </ul> <h2 id="31-策略">3.1 策略</h2> <ol> <li> <p>策略定义了在某一个状态应该采取什么样的动作。知道当前状态后，我们可以把当前状态代入策略函数来得到一个概率，即</p> \[\pi(a \mid s)=p\left(a_t=a \mid s_t=s\right)\] <p>概率代表在所有可能的动作里面怎样采取行动，比如可能有 0.7 的概率往左走，有 0.3 的概率往右走，这是一个概率的表示。另外策略也可能是确定的，它有可能直接输出一个值，或者直接告诉我们当前应该采取什么样的动作，而不是一个动作的概率。假设概率函数是平稳的（stationary），不同时间点，我们采取的动作其实都是在对策略函数进行采样。</p> </li> <li> <p>已知马尔可夫决策过程和策略 \(\pi\) ，我们可以把马尔可夫决策过程转换成马尔可夫奖励过程。在马尔可夫决策过程里面，状态转移函\(P(s′∣s,a)\) 基于它当前的状态以及它当前的动作。因为我们现在已知策略函数，也就是已知在每一个状态下，可能采取的动作的概率，所以我们就可以直接把动作进行加和，marginalize \(a\)，这样我们就可以得到对于马尔可夫奖励过程的转移，这里就没有动作，</p> \[P_\pi\left(s^{\prime} \mid s\right)=\sum_{a \in A} \pi(a \mid s) p\left(s^{\prime} \mid s, a\right)\] <p>对于奖励函数，我们也可以把动作去掉，这样就会得到类似于马尔可夫奖励过程的奖励函数，即</p> \[r_\pi(s)=\sum_{a \in A} \pi(a \mid s) R(s, a)\] </li> </ol> <h2 id="32-同过程奖励过程的区别">3.2 同过程、奖励过程的区别</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter2/Untitled%202-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter2/Untitled%202-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter2/Untitled%202-1400.webp"/> <img src="/assets/img/RL_chapter2/Untitled%202.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ol> <li>马尔可夫过程/马尔可夫奖励过程的状态转移是直接决定的。比如当前状态是\(s\)，那么直接通过转移概率决定下一个状态是什么。</li> <li>但对于马尔可夫决策过程，它的中间多了一层动作\(a\), 即智能体在当前状态的时候，首先要决定采取某一种动作，这样我们会到达某一个黑色的节点。到达这个黑色的节点后，因为有一定的不确定性，所以当智能体当前状态以及智能体当前采取的动作决定过后，智能体进入未来的状态其实也是一个概率分布。在当前状态与未来状态转移过程中多了一层决策性，这是马尔可夫决策过程与之前的马尔可夫过程/马尔可夫奖励过程很不同的一点。在马尔可夫决策过程中，动作是由智能体决定的，智能体会采取动作来决定未来的状态转移。</li> </ol> <h2 id="33-价值函数">3.3 价值函数</h2> <p>马尔可夫决策过程中的价值函数可定义为</p> \[V_\pi(s)=\mathbb{E}_\pi\left[G_t \mid s_t=s\right]\] <p>其中，期望基于我们采取的策略。当策略决定后，我们通过对策略进行采样来得到一个期望，计算出它的价值函数。这里我们另外引入了一个 <strong>Q 函数（Q-function）</strong>。Q 函数也被称为<strong>动作价值函数（action-value function）</strong>。Q 函数定义的是在某一个状态采取某一个动作，它有可能得到的回报的一个期望，即</p> \[Q_\pi(s, a)=\mathbb{E}_\pi\left[G_t \mid s_t=s, a_t=a\right]\] <p>这里的期望其实也是基于策略函数的。所以我们需要对策略函数进行一个加和，然后得到它的价值。 对 Q 函数中的动作进行加和，就可以得到价值函数：</p> \[V_\pi(s)=\sum_{a \in A} \pi(a \mid s) Q_\pi(s, a)\] <h2 id="34-贝尔曼期望方程">3.4 贝尔曼期望方程</h2> <p>我们可以把状态价值函数和 Q 函数拆解成两个部分：即时奖励和后续状态的折扣价值（discounted value of successor state）。 通过对价值函数进行分解，我们就可以得到一个类似于之前马尔可夫奖励过程的贝尔曼方程————<strong>贝尔曼期望方程（Bellman expectation equation）</strong></p> \[V_\pi(s)=\mathbb{E}_\pi\left[r_{t+1}+\gamma V_\pi\left(s_{t+1}\right) \mid s_t=s\right]\] \[V_\pi(s)=\sum_{a \in A} \pi(a \mid s)\left(R(s, a)+\gamma \sum_{s^{\prime} \in S} p\left(s^{\prime} \mid s, a\right) V_\pi\left(s^{\prime}\right)\right)\] <ul> <li> <p>推导：</p> \[\begin{aligned}Q(s, a) &amp; =\mathbb{E}\left[G_t \mid s_t=s, a_t=a\right] \\&amp; =\mathbb{E}\left[r_{t+1}+\gamma r_{t+2}+\gamma^2 r_{t+3}+\ldots \mid s_t=s, a_t=a\right] \\&amp; =\mathbb{E}\left[r_{t+1} \mid s_t=s, a_t=a\right]+\gamma \mathbb{E}\left[r_{t+2}+\gamma r_{t+3}+\gamma^2 r_{t+4}+\ldots \mid s_t=s, a_t=a\right] \\&amp; =R(s, a)+\gamma \mathbb{E}\left[G_{t+1} \mid s_t=s, a_t=a\right] \\&amp; =R(s, a)+\gamma \mathbb{E}\left[V\left(s_{t+1}\right) \mid s_t=s, a_t=a\right] \\&amp; =R(s, a)+\gamma \sum_{s^{\prime} \in S} p\left(s^{\prime} \mid s, a\right) V\left(s^{\prime}\right)\end{aligned}\] </li> </ul> <p>对于 Q 函数，我们也可以做类似的分解，得到 Q 函数的贝尔曼期望方程：</p> \[Q_\pi(s, a)=\mathbb{E}_\pi\left[r_{t+1}+\gamma Q_\pi\left(s_{t+1}, a_{t+1}\right) \mid s_t=s, a_t=a\right]\] \[Q_\pi(s, a)=R(s, a)+\gamma \sum_{s^{\prime} \in S} p\left(s^{\prime} \mid s, a\right) \sum_{a^{\prime} \in A} \pi\left(a^{\prime} \mid s^{\prime}\right) Q_\pi\left(s^{\prime}, a^{\prime}\right)\\=R(s, a)+\gamma \sum_{s^{\prime} \in S} p\left(s^{\prime} \mid s, a\right)\textcolor{orange}{V_\pi (s')}\] <p>💡 这里可以通过V函数求出Q函数！</p> <h2 id="35-备份图">3.5 备份图</h2> <p>对于某一个状态，它的当前价值是与它的未来价值线性相关的。 我们称为<strong>备份图（backup diagram）</strong>或回溯图，因为它们所示的关系构成了更新或备份操作的基础，而这些操作是强化学习方法的核心。这些操作将价值信息从一个状态（或状态-动作对）的后继状态（或状态-动作对）转移回它。 每一个空心圆圈代表一个状态，每一个实心圆圈代表一个状态-动作对。</p> <ol> <li> <p>对于V-function：</p> \[v_\pi(s)=\sum_{a \in A} \pi(a \mid s)\left(R(s, a)+\gamma \sum_{s^{\prime} \in S} P\left(s^{\prime} \mid s, a\right) v_\pi\left(s^{\prime}\right)\right)\] </li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter2/Untitled%203-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter2/Untitled%203-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter2/Untitled%203-1400.webp"/> <img src="/assets/img/RL_chapter2/Untitled%203.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ol> <li> <p>对于Q-function</p> \[q_\pi(s, a)=R(s, a)+\gamma \sum_{s^{\prime} \in S} P\left(s^{\prime} \mid s, a\right) \sum_{a^{\prime} \in A} \pi\left(a^{\prime} \mid s^{\prime}\right) q_\pi\left(s^{\prime}, a^{\prime}\right)\] </li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter2/Untitled%204-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter2/Untitled%204-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter2/Untitled%204-1400.webp"/> <img src="/assets/img/RL_chapter2/Untitled%204.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="36-策略评估">3.6 策略评估</h2> <p>已知马尔可夫决策过程以及要采取的策略\(\pi\)，计算价值函数\(V_\pi(s)\)的过程就是<strong>策略评估（价值预测）</strong>。也就是预测我们当前采取的策略最终会产生多少价值。</p> <p>我们可以直接通过贝尔曼期望方程来得到价值函数：</p> \[V^\pi_t(s)=R_\pi(s)+\gamma \sum_{s^{\prime} \in S} p\left(s^{\prime} \mid s, \pi(s)\right) V^\pi_{t-1}\left(s^{\prime}\right)\] <p>我们可以不停用贝尔曼期望方程迭代，最后价值函数会收敛。收敛之后，价值函数的值就是每一个状态的价值。</p> <h2 id="37-预测和控制">3.7 预测和控制</h2> <p>预测 (prediction) 和控制 (control) 是马尔可夫决策过程里面的核心问题。</p> <ol> <li><strong>预测</strong>（评估一个给定的策略) : 预测是指给定一个马尔可夫决策过程以及一个策略 \(\pi\) ，计算它的<strong>价值函数</strong>，也就是<strong>计算每个状态的价值</strong>。 <ol> <li>输入是马尔可夫决策过程 \(&lt;S, A, P, R, \gamma&gt;\) 和策略 \(\pi\) ，</li> <li>输出是价值函数 \(V_\pi\) 。</li> </ol> </li> <li><strong>控制</strong> (搜索最佳策略) : 控制就是我们去<strong>寻找一个最佳的策略</strong>，然后同时<strong>输出它的最佳价值函数</strong>以及最<strong>佳策略</strong>。 <ol> <li>输入是马尔可夫决策过程 \(&lt;S, A, P, R, \gamma&gt;\) ，</li> <li>输出是<strong>最佳价值函数</strong> (optimal value function) \(V^*\)和<strong>最佳策略</strong> (optimal policy） <em>\(\pi^*\)</em> 。</li> </ol> </li> </ol> <p>😍 在马尔可夫决策过程里面，预测和控制都可以通过动态规划解决。要强调的是，这两者的区别就在于，预测问题是给定一个策略，我们要确定它的价值函数是多少。而控制问题是在没有策略的前提下，我们要确定最佳的价值函数以及对应的决策方案。实际上，这两者是递进的关系，在强化学习中，我们<strong>通过解决预测问题，进而解决控制问题</strong>。</p> <h2 id="38-预测策略评估">3.8 预测（策略评估）</h2> <p>策略评估就是给定马尔可夫决策过程和策略，评估我们可以获得多少价值，即对于当前策略，我们可以得到多大的价值。我们可以直接把<strong>贝尔曼期望备份（Bellman expectation backup）</strong> ，变成迭代的过程，反复迭代直到收敛。这个迭代过程可以看作<strong>同步备份（synchronous backup）</strong> 的过程。</p> \[V_{t+1}(s)=\sum_{a \in A} \pi(a \mid s)\left(R(s, a)+\gamma \sum_{s^{\prime} \in S} p\left(s^{\prime} \mid s, a\right) V_t\left(s^{\prime}\right)\right)\] <p>策略评估的核心思想就是把如式所示的贝尔曼期望备份反复迭代，然后得到一个收敛的价值函数的值。因为已经给定了<strong>策略函数</strong>，所以我们可以直接把它简化成一个<strong>马尔可夫奖励过程</strong>的表达形式，相当于把\(a\)去掉，即</p> \[V_{t+1}(s)=r_\pi(s)+\gamma p_\pi\left(s^{\prime} \mid s\right) V_t\left(s^{\prime}\right)\] <h2 id="39-控制">3.9 控制</h2> <p>策略评估是指给定马尔可夫决策过程和策略，我们可以估算出价值函数的值。如果我们只有马尔可夫决策过程，那么应该如何寻找最佳的策略，从而得到<strong>最佳价值函数（optimal value function）</strong>呢？</p> <p>最佳价值函数的定义为：</p> \[V^*(s)=\max _\pi V_\pi(s)\] \[\pi^*(s)=\underset{\pi}{\arg \max } V_\pi(s)\] <p>当取得最佳价值函数后，我们可以通过对 \(Q\) 函数进行最大化来得到最佳策略（这里是deterministic的！）：</p> \[\pi^*(a \mid s)= \begin{cases}1, &amp; a=\underset{a \in A}{\arg \max } Q^*(s, a) \\ 0, &amp; \text { 其他 }\end{cases}\] <p>当\(Q\)函数收敛后，因为\(Q\)函数是关于状态与动作的函数，所以如果在某个状态采取某个动作，可以使得\(Q\)函数最大化，那么这个动作就是最佳的动作。如果我们能优化出一个 \(Q\) 函数\(Q^*(s,a)\)，就可以直接在\(Q\)函数中取一个让\(Q\)函数值最大化的动作的值，就可以提取出最佳策略。</p> <p>我们可以通过策略迭代和价值迭代来解决马尔可夫决策过程的控制问题。</p> <h2 id="310-策略迭代">3.10 策略迭代</h2> <ol> <li>策略迭代由两个步骤组成：<strong>策略评估</strong>和<strong>策略改进</strong>（policy improvement）。 <ol> <li>策略评估：当前我们在优化策略\(\pi\)，在优化过程中得到一个最新的策略。我们先保证这个策略不变，然后估计它的价值，即给定当前的策略函数来估计状态价值函数。 通过<strong>贝尔曼期望方程</strong>迭代，得到价值函数\(V_{\pi_i}\)。</li> <li> <p>策略改进：得到价值函数后，我们可以进一步推算出它的 \(Q\) 函数。得到 \(Q\) 函数后，我们直接对 \(Q\) 函数进行最大化，通过在 \(Q\) 函数做一个贪心的搜索来进一步改进策略。这两个步骤一直在迭代进行。在策略迭代里面，在初始化的时候，我们有一个初始化的状态价值函数\(V\)和策略\(\pi\)，然后在这两个步骤之间迭代。</p> \[Q_{\pi_i}(s, a)=R(s, a)+\gamma \sum_{s^{\prime} \in S} p\left(s^{\prime} \mid s, a\right) V_{\pi_i}\left(s^{\prime}\right)\] <p>对于每个状态，策略改进会得到它的新一轮的策略，对于每个状态，我们取使它得到最大值的动作，即</p> \[\pi_{i+1}(s)=\underset{a}{\arg \max } Q_{\pi_i}(s, a)\] </li> </ol> </li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter2/Untitled%205-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter2/Untitled%205-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter2/Untitled%205-1400.webp"/> <img src="/assets/img/RL_chapter2/Untitled%205.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>上面的线就是我们当前状态价值函数的值，下面的线是策略的值。 策略迭代的过程与踢皮球一样。我们先给定当前已有的策略函数，计算它的状态价值函数。算出状态价值函数后，我们会得到一个 Q 函数。我们对Q 函数采取贪心的策略，这样就像踢皮球，“踢”回策略。然后进一步改进策略，得到一个改进的策略后，它还不是最佳的策略，我们再进行策略评估，又会得到一个新的价值函数。基于这个新的价值函数再进行 Q 函数的最大化，这样逐渐迭代，状态价值函数和策略就会收敛。</p> <ol> <li> <p>贝尔曼最优方程: 当我们一直采取 argmax 操作的时候，我们会得到一个单调的递增。通过采取这种贪心操作（argmax 操作），我们就会得到更好的或者不变的策略，而不会使价值函数变差。所以当改进停止后，我们就会得到一个最佳策略。当改进停止后，我们取让 Q 函数值最大化的动作，Q 函数就会直接变成价值函数，即</p> \[Q_\pi\left(s, \pi^{\prime}(s)\right)=\max _{a \in A} Q_\pi(s, a)=Q_\pi(s, \pi(s))=V_\pi(s)\] \[V_\pi(s)=\max _{a \in A} Q_\pi(s, a)\] <p>💡 这里因为\(V_\pi(s)=\sum_{a \in A} \pi(a \mid s) Q_\pi(s, a)\)，而\(\pi(a \mid s)\)是只有\(a=\pi^*(s)\)为1</p> <p>贝尔曼最优方程表明：最佳策略下的一个状态的价值必须等于在这个状态下采取最好动作得到的回报的期望。 当马尔可夫决策过程满足贝尔曼最优方程的时候，整个马尔可夫决策过程已经达到最佳的状态。</p> <p>只有当整个状态已经收敛后，我们得到最佳价值函数后，贝尔曼最优方程才会满足。满足贝尔曼最优方程后，我们可以采用最大化操作，即</p> \[\begin{aligned} Q^*(s, a) &amp; =R(s, a)+\gamma \sum_{s^{\prime} \in S} p\left(s^{\prime} \mid s, a\right) V^*\left(s^{\prime}\right) \\ &amp; =R(s, a)+\gamma \sum_{s^{\prime} \in S} p\left(s^{\prime} \mid s, a\right) \max _a Q^*\left(s^{\prime}, a^{\prime}\right) \end{aligned}\] </li> </ol> <h2 id="311-价值迭代">3.11 价值迭代</h2> <ol> <li> <p>最优性原理</p> <p>我们从另一个角度思考问题，动态规划的方法将优化问题分成两个部分。第一步执行的是最优的动作。之后后继的状态的每一步都按照最优的策略去做，最后的结果就是最优的。</p> </li> <li>最优性原理定理（principle of optimality theorem）： 一个策略\(π(a∣s)\) 在状态 \(s\) 达到了最优价值，也就是 \(V^π(s)=V^∗(s)\) 成立，当且仅当对于任何能够从 \(s\) 到达的 \(s'\)，都已经达到了最优价值。也就是对于所有的\(s'\)，\(V^π(s')=V^∗(s')\)  恒成立。</li> <li> <p>确认性价值迭代</p> <p>如果我们知道子问题 \(V^∗(s')\) 的最优解，就可以通过价值迭代来得到最优的\(V^∗(s)\)的解。价值迭代就是把贝尔曼最优方程当成一个更新规则来进行，即</p> \[V(s) \leftarrow \max _{a \in A}\left(R(s, a)+\gamma \sum_{s^{\prime} \in S} p\left(s^{\prime} \mid s, a\right) V\left(s^{\prime}\right)\right)\] <p>只有当整个马尔可夫决策过程已经达到最佳的状态时，式才满足。但我们可以把它转换成一个备份的等式。备份的等式就是一个迭代的等式。我们不停地迭代贝尔曼最优方程，价值函数就能逐渐趋向于最佳的价值函数，这是价值迭代算法的精髓。</p> <p>为了得到最佳的\(V^*\) ，对于每个状态的 \(V\)，我们直接通过贝尔曼最优方程进行迭代，迭代多次之后，价值函数就会收敛。这种价值迭代算法也被称为<strong>确认性价值迭代</strong>（deterministic value iteration）。</p> </li> <li>具体算法：价值迭代算法的过程如下。 <ul> <li>初始化: 令 \(k=1\)，对于所有状态 \(s ， V_0(s)=0\) 。</li> <li>对于 \(k=1: H\) ( \(H\) 是让 \(V(s)\) 收敛所需的迭代次数) <ul> <li> <p>对于所有状态 \(s\)</p> \[\begin{gathered} Q_{k+1}(s, a)=R(s, a)+\gamma \sum_{s^{\prime} \in S} p\left(s^{\prime} \mid s, a\right) V_k\left(s^{\prime}\right) \\ V_{k+1}(s)=\max_a Q_{k+1}(s, a) \end{gathered}\] </li> <li> \[k \leftarrow k+1\] </li> </ul> </li> <li> <p>在迭代后提取最优策略:</p> \[\pi(s)=\underset{a}{\arg \max }\left[R(s, a)+\gamma \sum_{s^{\prime} \in S} p\left(s^{\prime} \mid s, a\right) V_{H+1}\left(s^{\prime}\right)\right]\] </li> </ul> </li> </ol> <h2 id="312-对比策略迭代和价值迭代">3.12 对比策略迭代和价值迭代</h2> <p>这两个算法都可以解马尔可夫决策过程的控制问题。</p> <ol> <li>策略迭代分两步。 <ol> <li>首先进行策略评估，即对当前已经搜索到的策略函数进行估值，通过迭代计算出价值函数\(V\)</li> <li>得到估值后，我们进行策略改进，即把 \(Q\) 函数算出来，进行进一步改进。不断重复这两步，直到策略收敛。</li> </ol> </li> <li>价值迭代直接使用<strong>贝尔曼最优方程</strong>进行迭代，从而寻找最佳的价值函数。找到最佳价值函数后，我们再提取一次最佳策略（一旦价值函数是最优的，策略也是最优的）。</li> </ol> <h2 id="313-总结">3.13 总结</h2> <table> <thead> <tr> <th>问题</th> <th>贝尔曼方程</th> <th>算法</th> </tr> </thead> <tbody> <tr> <td>预测</td> <td>贝尔曼期望方程</td> <td>策略评估</td> </tr> <tr> <td>控制</td> <td>贝尔曼期望方程</td> <td>策略迭代</td> </tr> <tr> <td>控制</td> <td>贝尔曼最优方程</td> <td>价值迭代</td> </tr> </tbody> </table>]]></content><author><name></name></author><category term="Reinforcement-Learning"/><category term="intro"/><summary type="html"><![CDATA[MDP]]></summary></entry><entry><title type="html">第一章 Overview (课程概括与RL基础)</title><link href="https://gszfwsb.github.io/blog/2023/RL-lecture1/" rel="alternate" type="text/html" title="第一章 Overview (课程概括与RL基础)"/><published>2023-10-26T00:00:00+00:00</published><updated>2023-10-26T00:00:00+00:00</updated><id>https://gszfwsb.github.io/blog/2023/RL-lecture1</id><content type="html" xml:base="https://gszfwsb.github.io/blog/2023/RL-lecture1/"><![CDATA[<h1 id="一强化学习概述">一、强化学习概述</h1> <p>什么是强化学习：强化学习 (reinforcement learning, RL) 讨论的问题是<strong>智能体 (agent)</strong> 怎么在复杂、不确定的<strong>环境</strong> (environment) 里面去最大化它能获得的<strong>奖励</strong>。如图所示, 强化学习由两部分组成: 智能体和环境。在强化学习过程中, 智能体与环境一直在交互：</p> <ol> <li>智能体在环境里面获取某个状态后, 它会利用该状态输出 一个<strong>动作</strong> (action), 这个动作也称为<strong>决策</strong>(decision)。</li> <li>然后这个动作会在<strong>环境</strong>之中被执行, 环境会根据智能体采取的动作, 输出下一个<strong>状态</strong>以及当前这个动作带来的<strong>奖励</strong>。智能体的目的就是尽可能多地从环境中获取奖励。</li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter1/Untitled-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter1/Untitled-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter1/Untitled-1400.webp"/> <img src="/assets/img/RL_chapter1/Untitled.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="11-强化学习与监督学习">1.1 强化学习与监督学习</h2> <ol> <li>监督学习：监督学习（supervised learning）首先假设我们有大量被标注的数据，这些图片都要满足<strong>独立同分布</strong>，即它们之间是没有关联关系的。所以在监督学习过程中，有两个假设。 <ol> <li>输入的数据（标注的数据）都应是没有关联的。因为如果输入的数据有关联，学习器（learner）是不好学习的；</li> <li>我们告诉学习器正确的标签是什么，这样它可以通过正确的标签来修正自己的预测。</li> </ol> </li> <li>在强化学习里面，监督学习的两个假设其实都不满足。 <ol> <li>智能体得到的观测（observation）不是独立同分布的，上一帧与下一帧间其实有非常强的连续性。我们得到的数据是相关的时间序列数据，不满足独立同分布。</li> <li>另外，我们并没有立刻获得反馈，游戏没有告诉我们哪个动作是正确动作。比如我们现在把木板往右移，这只会使得球往上或者往左去一点儿，我们并不会得到立刻的反馈。因此，强化学习之所以这么困难，是因为智能体不能得到即时的反馈，然而我们依然希望智能体在这个环境里面学习。</li> </ol> </li> <li>区别总结： <ol> <li>强化学习输入的样本是序列数据，而不像监督学习里面样本都是独立的。</li> <li>学习器并没有告诉我们每一步正确的动作应该是什么，学习器需要自己去发现哪些动作可以带来 最多的奖励，只能通过不停地尝试来发现最有利的动作。</li> <li>智能体获得自己能力的过程，其实是不断地试错探索（trial-and-error exploration）的过程。探索 （exploration）和利用（exploitation）是强化学习里面非常核心的问题。其中，探索指尝试一些新的动作， 这些新的动作有可能会使我们得到更多的奖励，也有可能使我们“一无所有”；利用指采取已知的可以获得最多奖励的动作，重复执行这个动作，因为我们知道这样做可以获得一定的奖励。因此，我们需要在探 索和利用之间进行权衡，这也是在监督学习里面没有的情况。</li> <li>在强化学习过程中，没有非常强的监督者（supervisor），只有<strong>奖励信号（reward signal）</strong>，并且奖励信号是延迟的。</li> </ol> </li> </ol> <h2 id="12-强化学习的特征">1.2 强化学习的特征</h2> <ol> <li>强化学习会试错探索，它通过探索环境来获取对环境的理解。</li> <li>强化学习智能体会从环境里面获得延迟的奖励。</li> <li>在强化学习的训练过程中，时间非常重要。因为我们得到的是有时间关联的数据（sequential data）， 而不是独立同分布的数据。</li> <li>智能体的动作会影响它随后得到的数据。在训练智能体的过程中，很多时候我们也是通过正在学习的智能体与环境交互来得到数据的。所以如果在训练过程中，智能体不能保持稳定，就会使我们采集到的数据非常糟糕。我们通过数据来训练智能体，如果数据有问题，整个训练过程就会失败。所以在强化学习里面一个非常重要的问题就是，怎么让智能体的动作一直稳定地提升。</li> </ol> <h1 id="二序列决策">二、序列决策</h1> <h2 id="21-智能体和环境">2.1 智能体和环境</h2> <p>强化学习研究的问题是智能体与环境交互的问题。智能体把它的动作输出给环境，环境取得这个动作后会进行下一步，把下一步的观测与这个动作带来的奖励返还给智能体。这样的交互会产生很多观测，智能体的目的是从这些观测之中学到能最大化奖励的策略。</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter1/Untitled%201-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter1/Untitled%201-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter1/Untitled%201-1400.webp"/> <img src="/assets/img/RL_chapter1/Untitled%201.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="22-奖励">2.2 奖励</h2> <p>奖励是由环境给的一种<strong>标量的反馈信号</strong>（scalar feedback signal），这种信号可显示智能体在某一步采 取某个策略的表现如何。强化学习的目的就是最大化智能体可以获得的奖励，智能体在环境里面存在的目的就是最大化它的期望的累积奖励（expected cumulative reward）。不同的环境中，奖励也是不同的。</p> <h2 id="23-序列决策">2.3 序列决策</h2> <p>在一个强化学习环境里面，智能体的目的就是选取一系列的动作来最大化奖励，所以这些选取的动作 必须有长期的影响。但在这个过程里面，智能体的奖励其实是被延迟了的，就是我们现在选取的某一步动作，可能要等到很久后才知道这一步到底产生了什么样的影响。强化学习里面一个重要的课题就是近期奖励和远期奖励的权衡 （trade-off），研究怎么让智能体取得更多的远期奖励。在与环境的交互过程中，智能体会获得很多观测。针对每一个观测，智能体会采取一个动作，也会得到一个奖励。</p> <ol> <li> <p><strong>历史</strong>是<strong>观测、动作、奖励</strong>的序列：</p> \[H_t=o_1, a_1, r_1, \ldots, o_t, a_t, r_t\] </li> <li> <p>智能体在采取当前动作的时候会依赖于它之前得到的历史，所以我们可以把整个游戏的<strong>状态</strong>看成关于这个<strong>历史</strong>的函数：</p> \[S_t=f\left(H_t\right)\] </li> <li>观测和状态的区别：<strong>状态</strong>是对世界的完整描述，不会隐藏世界的信息。<strong>观测</strong>是对状态的部分描述，可能会遗漏一些信息。在深度强化学习中，我们几乎总是用实值的向量、矩阵或者更高阶的张量来表示状态和观。</li> <li><strong>环境</strong>有自己的函数\(S_t^e=f^e\left(H_t\right)\) 来更新状态，在<strong>智能体</strong>的内部也有一个函数\(S_t^a=f^a\left(H_t\right)\)来更新状态。</li> <li>马尔可夫决策（Markov decision process，MDP）过程：当智能体的状态与环境的状态等价的时候，即当智能体能够观察到环境的所有状态时，我们称这个环境是完全可观测的（fully observed）。在这种情况下面，强化学习通常被建模成一个马尔可夫决策过程的问题。在马尔可夫决策过程中，\(O_t=S_t^e=S_t^a\)。</li> <li> <p>部分可观测马尔可夫决策过程（partially observable Markov decision process, POMDP）：智能体得到的观测并不一定能包含环境运作的所有状态，因为在强化学习的设定里面， 环境的状态才是真正的所有状态。当智能体只能看到部分的观测，我们就称这个环境是部分可观测的（partially observed）。 在这种情况下，强化学习通常被建模成部分可观测马尔可夫决策过程的问题。部分可观测马尔可夫决策过程是马尔可夫决策过程的一种泛化。 部分可观测马尔可夫决策过程依然具有马尔可夫性质，但是假设智能体无法感知环境的状态，只能知道部分观测值。部分观测值。比如在自动驾驶中，智能体只能感知传感器采集的有限的环境信息。</p> <p>😈 部分可观测马尔可夫决策过程可以用一个七元组描述：\((S, A, T, R, \Omega, O, \gamma)\)。其中\(S\)表示状态空间，为隐变量，\(A\)为动作空间，\(T\left(s^{\prime} \mid s, a\right)\)为状态转移概率，\(R\) 为奖励函数，\(\Omega(o \mid s, a)\)为观测概率,\(o\)为观测空间，\(\gamma\)为折扣系数。</p> </li> </ol> <h1 id="三动作空间">三、动作空间</h1> <p>不同的环境允许不同种类的动作。在给定的环境中，<strong>有效动作的集合</strong>经常被称为<strong>动作空间</strong>（action space）。像雅达利游戏和围棋（Go）这样的环境有<strong>离散动作空间</strong>（discrete action space），在这个动作 空间里，智能体的动作数量是有限的。在其他环境，比如在物理世界中控制一个智能体，在这个环境中就有<strong>连续动作空间</strong>（continuous action space）。在连续动作空间中，动作是实值的向量。</p> <p>例如，走迷宫机器人如果只有往东、往南、往西、往北这 4 种移动方式，则其动作空间为离散动作空 间；如果机器人可以向 360 ◦ 中的任意角度进行移动，则其动作空间为连续动作空间。</p> <h1 id="四强化学习智能体的组成成分和类型">四、强化学习智能体的组成成分和类型</h1> <p><strong>部分可观测马尔可夫决策过程( POMDP)</strong> 是一个马尔可夫决策过程的泛化。对于一个强化学习智能体，它可能有一个或多个如下的组成成分。</p> <ul> <li><strong>策略（policy）</strong>。智能体会用策略来选取下一步的<strong>动作</strong>。</li> <li><strong>价值函数（value function）</strong>。我们用价值函数来对当前状态进行评估。价值函数用于评估智能体进入某个状态后，可以对后面的<strong>奖励</strong>带来多大的影响。价值函数值越大，说明智能体进入这个状态越有 利。</li> <li><strong>模型（model）</strong>。模型表示智能体对环境的状态进行理解，它决定了环境中世界的运行方式。 下面我们深入了解这 3 个组成部分的细节。</li> </ul> <h2 id="41-策略">4.1 策略</h2> <p>策略是智能体的<strong>动作模型</strong>，它决定了智能体的动作。它其实是一个<strong>函数</strong>，用于把输入的状态变成动作。策略可分为两种：<strong>随机性策略</strong>和<strong>确定性策略</strong>。</p> <ol> <li><strong>随机性策略（stochastic policy）</strong>就是\(\pi\) 函数，即\(\pi(a \mid s)=p\left(a_t=a \mid s_t=s\right)\)。输入一个状态 \(s\)，输出一个概率。 这个概率是智能体<strong>所有动作的概率</strong>，然后对这个概率分布进行<strong>采样</strong>，可得到智能体将采取的动作。比如可能是有 0.7 的概率往左，0.3 的概率往右，那么通过采样就可以得到智能体将采取的动作。</li> <li><strong>确定性策略（deterministic policy）</strong>就是智能体直接采取<strong>最有可能</strong>的动作，即\(a^*=\underset{a}{\arg \max } \pi(a \mid s)\)。</li> </ol> <p>😍 通常情况下，强化学习一般使用<strong>随机性策略</strong>，随机性策略有很多优点。比如，在学习时可以通过引入一定的随机性来更好地探索环境； 随机性策略的动作具有多样性，这一点在多个智能体博弈时非常重要。采用确定性策略的智能体总是对同样的状态采取相同的动作，这会导致它的策略很容易被对手预测。</p> <h2 id="42-价值函数">4.2 价值函数</h2> <ol> <li> <p>价值函数的值是对未来奖励的预测，我们用它来评估状态的好坏。 价值函数里面有一个<strong>折扣因子（discount factor）</strong>，我们希望在尽可能短的时间里面得到尽可能多的奖励。”现在的钱以后就不值钱了“。</p> \[V_\pi(s) \doteq \mathbb{E}_\pi\left[G_t \mid s_t=s\right]=\mathbb{E}_\pi\left[\sum_{k=0}^{\infty} \gamma^k r_{t+k+1} \mid s_t=s\right], \forall s\in S\] <p>期望 \(\mathbb{E}_\pi\) 的下标是\(\pi\)函数，\(\pi\)函数的值可反映在我们使用策略\(\pi\)的时候，到底可以得到多少奖励。</p> </li> <li> <p>我们还有一种价值函数： \(Q\) 函数。 \(Q\) 函数里面包含两个变量：<strong>状态</strong>和<strong>动作</strong>。其定义为</p> \[Q_\pi(s, a) \doteq \mathbb{E}_\pi\left[G_t \mid s_t=s, a_t=a\right]=\mathbb{E}_\pi\left[\sum_{k=0}^{\infty} \gamma^k r_{t+k+1} \mid s_t=s, a_t=a\right]\] <p>未来可以获得奖励的期望取决于<strong>当前的状态</strong>和<strong>当前的动作</strong>。当我们得到 \(Q\) 函数后， 进入某个状态要采取的最优动作可以通过 \(Q\) 函数得到。</p> </li> </ol> <h2 id="43-模型">4.3 模型</h2> <p>模型决定了下一步的<strong>状态</strong>。下一步的状态取决于当前的状态以及当前采取的动作。它由<strong>状态转移概率</strong>和<strong>奖励函数</strong>两个部分组成。</p> <ol> <li> <p><strong>状态转移概率</strong>即</p> \[p_{s s^{\prime}}^a=p\left(s_{t+1}=s^{\prime} \mid s_t=s, a_t=a\right)\] </li> <li> <p><strong>奖励函数</strong>是指我们在当前状态采取了某个动作，可以得到多大的奖励，即</p> </li> </ol> \[R(s, a)=\mathbb{E}\left[r_{t+1} \mid s_t=s, a_t=a\right]\] <ol> <li><strong>马尔可夫决策过程（Markov decision process）</strong>这个决策过程可视化了状态之间的转移以及采取的动作。包含<strong>策略、价值函数和模型</strong></li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter1/Untitled%202-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter1/Untitled%202-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter1/Untitled%202-1400.webp"/> <img src="/assets/img/RL_chapter1/Untitled%202.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="44-强化学习智能体的类型">4.4 强化学习智能体的类型</h2> <ol> <li>基于价值的智能体与基于策略的智能体 根据智能体学习的事物不同，我们可以把智能体进行归类。 <ol> <li><strong>基于价值的智能体（value-based agent）</strong>显式地学习价值函数，隐式地学习它的策略。策略是其从学到的价值函数里面推算出来的。</li> <li><strong>基于策略的智能体（policy-based agent）</strong>直接学习策略，我们给它一个状态，它就会输出对应动作的概率。基于策略的智能体并没有学习价值函数。</li> <li>把基于价值的智能体和基于策略的智能体结合起来就有了<strong>演员-评论员智能体（actor-critic agent）</strong>。这一类智能体把策略和价值函数都学习了，然后通过两者的交互得到最佳的动作。</li> </ol> <p>😍 Q: 基于策略和基于价值的强化学习方法有什么区别?</p> <p>A: 对于一个状态转移概率已知的马尔可夫决策过程，我们可以使用动态规划算法来求解。从决策方式来看，强化学习又可以划分为基于策略的方法和基于价值的方法。决策方式是智能体在给定状态下从动作集合中选择一个动作的依据，它是静态的，不随状态变化而变化。</p> <p>在基于策略的强化学习方法中，智能体会制定一套动作策略（确定在给定状态下需要采取何种动作），并根据这个策略进行操作。强化学习算法直接对策略进行优化，使制定的策略能够获得最大的奖励。</p> <p>而在基于价值的强化学习方法中，智能体不需要制定显式的策略，它维护一个价值表格或价值函数，并通过这个价值表格或价值函数来选取价值最大的动作。基于价值迭代的方法只能应用在不连续的、离散的环境下（如围棋或某些游戏领域），对于动作集合规模庞大、动作连续的场景（如机器人控制领域），其很难学习到较好的结果（此时基于策略迭代的方法能够根据设定的策略来选择连续的动作）。</p> <p>基于价值的强化学习算法有Q学习（Q-learning）、 Sarsa 等，而基于策略的强化学习算法有策略梯度（Policy Gradient，PG）算法等。此外，演员-评论员算法同时使用策略和价值评估来做出决策。其中，智能体会根据策略做出动作，而价值函数会对做出的动作给出价值，这样可以在原有的策略梯度算法的基础上加速学习过程，取得更好的效果。</p> </li> <li>有模型强化学习： 智能体与免模型强化学习智能体另外，我们可以通过智能体到底有没有学习<strong>环境模型</strong>来对智能体进行分类。 <ol> <li><strong>有模型（model-based）</strong>强化学习智能体通过学习<strong>状态的转移</strong>来采取动作。 </li> <li><strong>免模型（model-free）</strong>强化学习智能体没有去直接估计状态的转移，也没有得到环境的具体转移变量，它通过<strong>学习价值函数和策略函数</strong>进行决策。免模型强化学习智能体的模型里面没有环境转移的模型。</li> </ol> <p>我们可以用马尔可夫决策过程来定义强化学习任务，并将其表示为四元组 \(&lt;S,A,P,R&gt;\)，即状态集合、动作集合、状态转移函数和奖励函数。如果这个四元组中所有元素均已知，且状态集合和动作集合在有限步数内是有限集，则智能体可以对真实环境进行建模，构建一个虚拟世界来模拟真实环境中的状态和交互反应。 具体来说，当智能体知道状态转移函数 \(P(s_{t+1}∣s_t,a_t)\) 和奖励函数 \(R(s_t,a_t)\) 后，它就能知道在某一状态下执行某一动作后能带来的奖励和环境的下一状态，这样智能体就不需要在真实环境中采取动作，直接在虚拟世界中学习和规划策略即可。这种学习方法称为<strong>有模型强化学习</strong>。 五、强化学习的基本问题：规划和学习</p> </li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter1/Untitled%203-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter1/Untitled%203-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter1/Untitled%203-1400.webp"/> <img src="/assets/img/RL_chapter1/Untitled%203.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h1 id="五强化学习基本问题">五、强化学习基本问题</h1> <h2 id="51-学习和规划">5.1 学习和规划</h2> <p>学习（learning）和规划（planning）是序列决策的两个基本问题。在强化学习中，<strong>环境</strong>初始时是未知的，<strong>智能体</strong>不知道环境如何工作，它通过不断地与环境交互，逐渐改进策略。</p> <ol> <li>在<strong>规划</strong>中，环境是已知的，智能体被告知了整个环境的运作规则的详细信息。智能体能够计算出一个完美的模型，并且在不需要与环境进行任何交互的时候进行计算。智能体不需要实时地与环境交互就能知道未来环境，只需要知道当前的状态，就能够开始思考，来寻找最优解。</li> <li>在<strong>学习</strong>中，规则是确定的，我们知道选择左之后环境将会产生什么变化。我们完全可以通过已知的规则，来在内部模拟整个决策过程，无需与环境交互。 一个常用的强化学习问题解决思路是，先学习环境如何工作，也就是了解环境工作的方式，即学习得到一个模型，然后利用这个模型进行规划。</li> </ol> <h2 id="52-探索和利用">5.2 探索和利用</h2> <p>在强化学习里面，探索和利用是两个很核心的问题。</p> <ol> <li><strong>探索</strong>即我们去探索环境，通过尝试不同的动作来得到最佳的策略（带来最大奖励的策略）。</li> <li><strong>利用</strong>即我们不去尝试新的动作，而是采取已知的可以带来很大奖励的动作。</li> </ol> <p>在刚开始的时候，强化学习智能体不知道它采取了某个动作后会发生什么，所以它只能通过试错去探索，所以探索就是通过试错来理解采取的动作到底可不可以带来好的奖励。利用是指我们直接采取已知的可以带来很好奖励的动作。所以这里就面临一个权衡问题，即怎么通过牺牲一些短期的奖励来理解动作，从而学习到更好的策略。</p>]]></content><author><name></name></author><category term="Reinforcement-Learning"/><category term="intro"/><summary type="html"><![CDATA[RL Overview]]></summary></entry><entry><title type="html">Statistical Learning</title><link href="https://gszfwsb.github.io/blog/2022/STL/" rel="alternate" type="text/html" title="Statistical Learning"/><published>2022-05-01T00:00:00+00:00</published><updated>2022-05-01T00:00:00+00:00</updated><id>https://gszfwsb.github.io/blog/2022/STL</id><content type="html" xml:base="https://gszfwsb.github.io/blog/2022/STL/"><![CDATA[<p>STATS_LEARNING course note</p>]]></content><author><name></name></author><summary type="html"><![CDATA[CS7335 2022 course note]]></summary></entry></feed>