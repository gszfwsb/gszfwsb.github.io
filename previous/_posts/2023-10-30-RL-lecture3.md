---
layout: post
title: 第三章 Tabular methods Model-free Prediction and Control (表格型方法：免模型的预测和控制)
date: 2023-10-30
description: Tabular methods
tags: intro
categories: Reinforcement-Learning
giscus_comments: true
related_posts: true
toc:
  beginning: true
---


# 一、为什么需要免模型RL？

1. 什么时候马尔可夫决策过程是已知的？
    
    **奖励和转移概率均已知，**这样才可以用策略迭代和价值迭代进行求解。
    
2. 策略迭代：给定一个已知的MDP，计算最优策略函数和价值函数。
    1. 策略评估：用贝尔曼期望方程迭代到收敛
        
        $$
        V_{t+1}(s)=\sum_{a \in \mathcal{A}} \pi(a \mid s)(\textcolor{red}{R(s, a)}+\gamma \sum_{s^{\prime} \in \mathcal{S}} \textcolor{red}{P(s^{\prime} \mid s, a)} V_{t}(s^{\prime})
        $$
        
    2. 策略改进：用贝尔曼期望方程，并在价值函数上用贪心策略
        
        $$
        Q_{\pi_i}(s, a)=\textcolor{red}{R(s, a)}+\gamma \sum_{s^{\prime} \in S} \textcolor{red}{P\left(s^{\prime} \mid s, a\right)} V_{\pi_i}\left(s^{\prime}\right)\\\pi_{i+1}(s)=\underset{a}{\arg \max } Q_{\pi_i}(s, a)
        $$
        
3. 价值迭代：给定一个已知的MDP，计算最优价值函数。
    1. 用贝尔曼最优公式迭代
        
        $$
        v_{i+1}(s)\leftarrow \max_{a\in \mathcal{A}}(\textcolor{red}{R(s, a)}+\gamma \sum_{s^{\prime} \in \mathcal{S}} \textcolor{red}{P(s^{\prime} \mid s, a)} v_{i}(s^{\prime})）
        $$
        
    2. 得到迭代后的最优策略：
        
        $$
        \pi^*(s)=\underset{a}{\arg \max }\left[\textcolor{red}{R(s, a)}+\gamma \sum_{s^{\prime} \in S} \textcolor{red}{P\left(s^{\prime} \mid s, a\right)} V_{\textrm{end}}\left(s^{\prime}\right)\right]
        $$
        
4. 知道世界如何运作的RL：
    1. 策略迭代和价值迭代都需要假设我们已知环境中的**状态转移**和**奖励**。
    2. 现实情境中，MDP模型要么就是未知要么就是太大太复杂
    
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% include figure.html path="assets/img/RL_chapter3/Untitled.png" class="rounded z-depth-1" style="height: 200px;" %}
    </div>
</div>

    
5. 免模型RL：通过与环境交互交互学习
    
    马尔可夫决策过程的模型未知或者模型很大时，我们可以使用免模型强化学习的方法。免模型强化学习方法没有获取环境的**状态转移**和**奖励函数**，而是让智能体与环境进行**交互**，采集大量的**轨迹数据**，智能体从轨迹中获取信息来改进策略，从而获得更多的奖励。
    
    轨迹：包括$$\{S_1,A_1,R_1,S_2,A_2,R_2,...,S_T,A_T,R_T\}$$
    
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% include figure.html path="assets/img/RL_chapter3/Untitled 1.png" class="rounded z-depth-1" style="height: 200px;" %}
    </div>
</div>
    

# 二、免模型预测

在无法获取马尔可夫决策过程的模型情况下，我们可以通过**蒙特卡洛方法**和**时序差分方法**来估计某个给定策略的价值。

## 2.1 蒙特卡洛方法

### 2.1.1 蒙特卡洛策略评估

蒙特卡洛方法是基于采样的方法，给定策略 $$\pi$$ ，我们让智能体与环境进行交互，可以得到很多轨迹。每个轨迹都有对应的回报：

$$
G_t=r_{t+1}+\gamma r_{t+2}+\gamma^2 r_{t+3}+\ldots 
$$

我们求出所有轨迹的回报的平均值，就可以知道某一个策略对应状态的价值，即

$$
V_\pi(s)=\mathbb{E}_{\tau \sim \pi}\left[G_t \mid s_t=s\right]

$$

1. 特点：
    - 蒙特卡洛仿真是指我们可以采样大量的**轨迹**$$\tau$$（从策略$$\pi$$采样），计算所有轨迹$$\tau$$的真实回报，然后计算平均值。
    - 蒙特卡洛方法使用**经验平均回报**（empirical mean return) 的方法来估计，它不需要马尔可夫决策过程的状态转移函数和奖励函数，并且不需要像动态规划那样用bootstrap的方法。
    - 此外，蒙特卡洛方法有一定的局限性，它只能用在有终止的马尔可夫决策过程中。
2. 为了得到评估 $$V(s)$$ ，我们采取了如下的步骤。
    1. 在每个回合中，如果在时间步 $$t$$ 状态 $$s$$ 被访问了，那么
        - 状态 $$s$$ 的访问数 $$N(s)$$ 增加 $$1 ， N(s) \leftarrow N(s)+1$$ 。
        - 状态 $$s$$ 的总的回报 $$S(s)$$ 增加 $$G_t, S(s) \leftarrow S(s)+G_{t}$$
        - 状态 $$s$$  的价值可以通过回报的平均来估计，即 $$V(s)=S(s) / N(s)$$ 。
    2. 根据大数定律，只要我们得到足够多的轨迹，就可以趋近这个策略对应的价值函数。当 $$N(s) \rightarrow \infty$$时， $$V(s) \rightarrow V_\pi(s)$$ 。
3. 具体更新时，我们可以把**经验均值**（empirical mean）转换成**增量均值**（incremental mean）的形式：
    
    $$
    \begin{aligned}\mu_t & =\frac{1}{t} \sum_{j=1}^t x_j \\& =\frac{1}{t}\left(x_t+\sum_{j=1}^{t-1} x_j\right) \\& =\frac{1}{t}\left(x_t+(t-1) \mu_{t-1}\right) \\& =\mu_{t-1}+\frac{1}{t}\left(x_t-\mu_{t-1}\right)\end{aligned}
    $$
    
4. 增量均值形式的MC算法：
    - 采样一轮游戏$$\left(S_1, A_1, R_1, \ldots, S_t\right)$$
    - 对每一个状态$$S_t$$和回报$$G_t$$
        
        $$
        \begin{aligned}
        & N\left(S_t\right) \leftarrow N\left(S_t\right)+1 \\
        & v\left(S_t\right) \leftarrow v\left(S_t\right)+\frac{1}{N\left(S_t\right)}\left(G_t-v\left(S_t\right)\right)
        \end{aligned}
        $$
        
    - 或者可以用running mean. 对于non-stationary问题有好处，$$\alpha$$是learning rate
        
        $$
        v\left(S_t\right) \leftarrow v\left(S_t\right)+\alpha\left(G_t-v\left(S_t\right)\right) 
        $$
        

### 2.1.2 蒙特卡洛方法和动态规划方法

1. 在动态规划方法里面，我们使用了bootstrap的思想。bootstrap就是我们基于之前估计的量来估计一个量。此外，动态规划方法使用贝尔曼期望备份（Bellman expectation backup），通过上一时刻的$$V_{t}$$更新这一时刻的$$V_{t+1}$$.
    
    $$
    V_{t+1}(s) \leftarrow \sum_{a \in \mathcal{A}} \pi(a \mid s)\left(R(s, a)+\gamma \sum_{s^{\prime} \in \mathcal{S}} P\left(s^{\prime} \mid s, a\right) V_{t}\left(s^{\prime}\right)\right)
    $$
    
    将其不停迭代，最后可以收敛。如图所示，贝尔曼期望公式有两层加和，即内部加和和外部加和，需要知道所有的状态和转移矩阵（model）计算两次期望，得到一个更新。
    
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% include figure.html path="assets/img/RL_chapter3/Untitled 2.png" class="rounded z-depth-1" style="height: 200px;" %}
    </div>
</div>
    

1. 蒙特卡洛方法通过一个回合的经验平均回报（实际得到的奖励）来进行更新，即
    
    $$
    V\left(s_t\right) \leftarrow V\left(s_t\right)+\alpha\left(G_{i, t}-V\left(s_t\right)\right)
    $$
    
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% include figure.html path="assets/img/RL_chapter3/Untitled 3.png" class="rounded z-depth-1" style="height: 200px;" %}
    </div>
</div>
    
    如图，我们使用蒙特卡洛方法得到的轨迹对应树上蓝色的轨迹，轨迹上的状态已经是决定的，采取的动作也是已经决定的。我们现在只更新这条轨迹上的所有状态，与这条轨迹没有关系的状态都不进行更新。
    
2. **蒙特卡洛方法**相比**动态规划方法**是有一些优势的。
    1. 蒙特卡洛方法适用于**环境未知**的情况，而动态规划是**有模型**的方法（需要转移矩阵）。
    2. 即使知道了环境的全部信息，计算转移概率往往也是很复杂的。
    3. 蒙特卡洛方法只需要更新**一条轨迹**的状态，而动态规划方法需要更新**所有的状态**。状态数量很多的时候（比如100万个、200万个），我们使用动态规划方法进行迭代，速度是非常慢。

## 2.2 时序差分方法

时序差分是介于蒙特卡洛和动态规划之间的方法，它是免模型的，不需要马尔可夫决策过程的转移矩阵和奖励函数。 此外，时序差分方法可以从不完整的回合中学习，并且结合了bootstrap的思想。

1. 特点：
    1. 直接从轨迹中学习
    2. 免模型：不需要MDP的转移概率和奖励
    3. 通过bootstrap从**不完整**的轨迹中学习
2. 时序差分方法的目的是对于某个给定的策略$$\pi$$，在线（online）地算出它的价值函数$$V_\pi$$，即一步一步地（step-by-step）算。 
    1. 最简单的算法是一步时序差分（one-step TD），TD(0)：
        
        $$
        V\left(s_t\right) \leftarrow V\left(s_t\right)+\alpha\left[\textcolor{red}{R_t+\gamma V\left(s_{t+1}\right)}-V\left(s_t\right)\right]
        $$
        
    2. $$R_{t+1}+\gamma V\left(S_{t+1}\right)$$ 是TD目标
    3. $$\delta_t=R_{t+1}+\gamma V\left(S_{t+1}\right)-V\left(S_t\right)$$是TD error
    
    😍 MC和TD：
    类比增量式蒙特卡洛方法，给定一个回合$$i$$，我们可以更新 $$V(s_t)$$ 来逼近真实的回报，具体更新公式为:
    
    $$
    V\left(s_t\right) \leftarrow V\left(s_t\right)+\alpha\left(\textcolor{red}{G_{i, t}}-V\left(s_t\right)\right)
    $$
    
    回顾贝尔曼期望方程便知道原因：
    
    $$
    \begin{aligned}V_\pi(s) & =\mathbb{E}_\pi\left[G_t \mid S_t=s\right] \\& =\mathbb{E}_\pi\left[\sum_{k=0}^{\infty} \gamma^k R_{t+k} \mid S_t=s\right] \\& =\mathbb{E}_\pi\left[R_t+\gamma \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \mid S_t=s\right] \\& =\mathbb{E}_\pi\left[R_t+\gamma V_\pi\left(S_{t+1}\right) \mid S_t=s\right]\end{aligned}
    $$
    
    **蒙特卡洛方法**将上式**第一行**作为更新的目标，而**时序差分算法**将上式**最后一行**作为更新的目标。于是，在用策略和环境交互时，每采样一步，我们就可以用时序差分算法来更新状态价值估计。时序差分算法用到了$$V(s_{t+1})$$的估计值，可以证明它最终收敛到策略$$\pi$$ 的价值函数。
    
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% include figure.html path="assets/img/RL_chapter3/Untitled 4.png" class="rounded z-depth-1" style="height: 200px;" %}
    </div>
</div>
    
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% include figure.html path="assets/img/RL_chapter3/Untitled 5.png" class="rounded z-depth-1" style="height: 200px;" %}
    </div>
</div>
    
    进一步比较时序差分方法和蒙特卡洛方法。
    
    - 时序差分方法可以**在线学习**（online learning），每走一步就可以更新效率高。蒙特卡洛方法必须等游戏结束时才可以学习。
    - 时序差分方法可以从**不完整序列**上进行学习。蒙特卡洛方法只能从**完整的序列**上进行学习。
    - 时序差分方法可以在**连续的环境下**（没有终止）进行学习。蒙特卡洛方法只能在**有终止**的情况下学习。
    - 时序差分方法利用了**马尔可夫性质**，在马尔可夫环境下有更高的学习效率。蒙特卡洛方法没有假设环境具有马尔可夫性质，利用采样的价值来估计某个状态的价值，在不是马尔可夫的环境下更加有效。
    
3. 多步时序差分（n-step TD）：之前是只往前走一步，即TD(0)。 我们可以调整步数（step），变成***n*步时序差分。**
    
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% include figure.html path="assets/img/RL_chapter3/Untitled 6.png" class="rounded z-depth-1" style="height: 200px;" %}
    </div>
</div>
    
    这样我们就可以通过步数来调整算法需要的实际奖励和bootstrap。
    
    $$
    \begin{aligned}  n=1(\mathrm{TD}) \quad & G_t^{(1)}=r_{t+1}+\gamma V\left(s_{t+1}\right) \\ n=2(\mathrm{TD(2)})) \quad & G_t^{(2)}=r_{t+1}+\gamma r_{t+2}+\gamma^2 V\left(s_{t+2}\right) \\ \ldots \\ n=\infty(\mathrm{MC}) \quad & G_t^{\infty} \begin{array}{l}=r_{t+1}+\gamma r_{t+2}+\ldots+\gamma^{T-t-1} r_T\end{array}\end{aligned}
    $$
    
    通过调整步数，可以进行蒙特卡洛方法和时序差分方法之间的权衡。如果$$n=\infty$$， 即整个游戏结束后，再进行更新，时序差分方法就变成了蒙特卡洛方法。*n*步时序差分可写为:
    
    $$
    G_t^n=r_{t+1}+\gamma r_{t+2}+\ldots+\gamma^{n-1} r_{t+n}+\gamma^n V\left(s_{t+n}\right)
    $$
    
    得到时序差分目标之后，我们用增量式学习 (incremental learning) 的方法来更新状态的价值:
    
    $$
    V\left(s_t\right) \leftarrow V\left(s_t\right)+\alpha\left(G_t^n-V\left(s_t\right)\right) 
    $$
    

## 2.3 采样和bootrap

动态规划方法、蒙特卡洛方法以及时序差分方法的bootstrap和采样有什么联系和区别呢？

### 2.3.1 采样和bootrap情形

1. **Bootstrap是指更新时使用了估计**。
    1. 蒙特卡洛方法没有使用bootstrap，因为它根据实际的回报进行更新。 
    2. 动态规划方法和时序差分方法使用了bootstrap。
2. **采样是指更新时通过采样得到一个期望**。
    1.  蒙特卡洛方法是纯采样的方法。 
    2. 动态规划方法没有使用采样，它是直接用贝尔曼期望方程来更新状态价值的。 
    3. 时序差分方法使用了采样。时序差分目标由两部分组成，一部分是采样，一部分是bootstrap。

### 2.3.2 DP, MC, TD

1. 动态规划方法直接计算期望，它把所有相关的状态都进行加和，即
    
    $$
    V\left(s_t\right) \leftarrow \mathbb{E}_\pi\left[r_{t+1}+\gamma V\left(s_{t+1}\right)\right]
    $$
    
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% include figure.html path="assets/img/RL_chapter3/Untitled 7.png" class="rounded z-depth-1" style="height: 200px;" %}
    </div>
</div>
    
2. 蒙特卡洛方法在当前状态下，采取一条支路，在这条路径上进行更新，更新这条路径上的所有状态，即
    
    $$
    V\left(s_t\right) \leftarrow V\left(s_t\right)+\alpha\left(G_t-V\left(s_t\right)\right)
    $$
    
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% include figure.html path="assets/img/RL_chapter3/Untitled 8.png" class="rounded z-depth-1" style="height: 200px;" %}
    </div>
</div>
    
3. 时序差分从当前状态开始，往前走了一步，关注的是非常局部的步骤，即
    
    $$
    \mathrm{TD}(0): V\left(s_t\right) \leftarrow V\left(s_t\right)+\alpha\left(r_{t+1}+\gamma V\left(s_{t+1}\right)-V\left(s_t\right)\right)
    $$
    
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% include figure.html path="assets/img/RL_chapter3/Untitled 9.png" class="rounded z-depth-1" style="height: 200px;" %}
    </div>
</div>
    

1. 如果时序差分方法需要更广度的更新，就变成了动态规划方法（因为动态规划方法是把所有状态都考虑进去来进行更新）。如果时序差分方法需要更深度的更新，就变成了蒙特卡洛方法。图 右下角是穷举搜索的方法（exhaustive search），穷举搜索的方法不仅需要很深度的信息，还需要很广度的信息。
    
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% include figure.html path="assets/img/RL_chapter3/Untitled 10.png" class="rounded z-depth-1" style="height: 200px;" %}
    </div>
</div>
    

# 三、免模型控制

## 3.1 广义策略迭代

在我们不知道马尔可夫决策过程模型的情况下，如何优化价值函数，得到最佳的策略呢？我们可以把策略迭代进行广义的推广，使它能够兼容蒙特卡洛和时序差分的方法，即带有**蒙特卡洛方法**和**时序差分**方法的**广义策略迭代（generalized policy iteration，GPI）**

当我们不知道奖励函数和状态转移时，如何进行策略的优化？我们引入了广义的策略迭代的方法。 我们对策略评估部分进行修改，使用蒙特卡洛的方法代替动态规划的方法估计 $$Q$$ 函数。我们首先进行策略评估，使用蒙特卡洛方法来估计策略 $$Q=Q_\pi$$，然后进行策略更新，即得到 $$Q$$ 函数后，我们就可以通过贪心的方法去改进它：

$$
\pi(s)=\underset{a}{\arg \max } Q(s, a)
$$

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% include figure.html path="assets/img/RL_chapter3/Untitled 11.png" class="rounded z-depth-1" style="height: 200px;" %}
    </div>
</div>

## 3.2 蒙特卡洛方法

### 3.2.1 探索性开始的MC

一个保证策略迭代收敛的**假设**是回合有**探索性开始（exploring start）**。假设每一个回合都有一个**探索性开始**，探索性开始保证所有的状态和动作都在无限步的执行后能被采样到，这样才能很好地进行估计。 算法通过蒙特卡洛方法产生很多轨迹，每条轨迹都可以算出它的价值。然后，我们可以通过平均的方法去估计 Q 函数。Q 函数可以看成一个表格，我们通过采样的方法把表格的每个单元值都填上，然后使用策略改进来选取更好的策略。 如何用蒙特卡洛方法来填 Q 表格是这个算法的核心。

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% include figure.html path="assets/img/RL_chapter3/Untitled 12.png" class="rounded z-depth-1" style="height: 200px;" %}
    </div>
</div>

### 3.2.2 带有$$\varepsilon$$-greedy的贪心策略

为了确保蒙特卡洛方法能够有足够的探索，我们使用了 $$\varepsilon$$-贪心探索。 $$\varepsilon$$-贪心是指我们有  $$1-\varepsilon$$ 的概率会按照 Q函数来决定动作，通常 $$\varepsilon$$ 就设一个很小的值， $$1-\varepsilon$$ 可能是 0.9，也就是 0.9 的概率会按照Q函数来决定动作，但是我们有 0.1 的概率是随机的。通常在实现上， $$\varepsilon$$ 的值会随着时间递减。在最开始的时候，因为我们还不知道哪个动作是比较好的，所以会花比较多的时间探索。接下来随着训练的次数越来越多，我们已经比较确定哪一个动作是比较好的，就会减少探索，把 $$\varepsilon$$ 的值变小。主要根据 Q 函数来决定动作，比较少随机决定动作，这就是$$\varepsilon$$-贪心。

$$
\pi(a \mid s)= \begin{cases}\epsilon /|\mathcal{A}|+1-\epsilon & \text { if } a^*=\arg \max _{a \in \mathcal{A}} Q(s, a) \\ \epsilon /|\mathcal{A}| & \text { otherwise }\end{cases}
$$

当我们使用蒙特卡洛方法和$$\varepsilon$$-贪心探索的时候，可以确保价值函数是单调的、改进的。对于任何 $$\epsilon$$-贪心策略 $$\pi$$，关于 $$Q_\pi$$ 的 $$\varepsilon$$-贪心策略 $$\pi'$$ 都是一个改进，即 $$V_{\pi}(s)\leq V_{\pi'}(s)$$，证明过程如下：

$$
\begin{aligned}V_\pi\left(s, \pi^{\prime}(s)\right) & =\sum_{a \in A} \pi^{\prime}(a \mid s) Q_\pi(s, a) \\& =\frac{\varepsilon}{|A|} \sum_{a \in A} Q_\pi(s, a)+(1-\varepsilon) \max _a Q_\pi(s, a) \\& \geqslant \frac{\varepsilon}{|A|} \sum_{a \in A} Q_\pi(s, a)+(1-\varepsilon) \sum_{a \in A} \frac{\pi(a \mid s)-\frac{\varepsilon}{|A|}}{1-\varepsilon} Q_\pi(s, a) \\& =\sum_{a \in A} \pi(a \mid s) Q_\pi(s, a)=V_\pi(s)\end{aligned}
$$

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% include figure.html path="assets/img/RL_chapter3/Untitled 13.png" class="rounded z-depth-1" style="height: 200px;" %}
    </div>
</div>

## 3.3 时序差分方法

与蒙特卡洛方法相比，时序差分方法有如下几个优势：**低方差，能够在线学习，能够从不完整的序列中学习。** 所以我们可以把时序差分方法也放到**控制循环**（control loop）里面去估计Q表格，再采取$$\varepsilon$$贪心探索改进。这样就可以在回合没结束的时候更新已经采集到的状态价值。

### 3.3.1 Sarsa: on-policy TD

1. Sarsa：
    
    时序差分方法是给定一个策略，然后我们去估计它的价值函数。接着我们要考虑怎么使用时序差分方法的框架来估计Q函数，也就是 Sarsa 算法。
    
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% include figure.html path="assets/img/RL_chapter3/Untitled 14.png" class="rounded z-depth-1" style="height: 200px;" %}
    </div>
</div>
    
    Sarsa 所做出的改变很简单，它将原本时序差分方法更新 *V* 的过程，变成了更新 *Q*，即
    
    $$
    Q\left(S_t, A_t\right) \leftarrow Q\left(S_t, A_t\right)+\alpha\left[R_{t+1}+\gamma Q\left(S_{t+1}, A_{t+1}\right)-Q\left(S_t, A_t\right)\right]
    $$
    
    > 注：prediction中我们更新V，control中我们更新Q
    > 
    > 
    > $$
    > V\left(s_t\right) \leftarrow V\left(s_t\right)+\alpha\left(R_{t+1}+\gamma V\left(s_{t+1}\right)-V\left(s_t\right)\right)
    > $$
    > 
    
    TD目标：$$\delta_t=R_{t+1}+\gamma Q\left(S_{t+1}, A_{t+1}\right)$$
    
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% include figure.html path="assets/img/RL_chapter3/Untitled 15.png" class="rounded z-depth-1" style="height: 200px;" %}
    </div>
</div>
    
2. n-step Sarsa
    
    Sarsa 属于单步更新算法，每执行一个动作，就会更新一次价值和策略。如果不进行单步更新，而是采取$$n$$步更新或者回合更新，即在执行*n*步之后再更新价值和策略，这样我们就得到了***n* 步 Sarsa（*n*-step Sarsa）:**
    
    $$
    \begin{aligned}n=1(\text { Sarsa }) q_t^{(1)}= & R_{t+1}+\gamma Q\left(S_{t+1}, A_{t+1}\right) \\n=2 \quad q_t^{(2)}= & R_{t+1}+\gamma R_{t+2}+\gamma^2 Q\left(S_{t+2}, A_{t+2}\right) \\& \vdots & \\n=\infty(M C) \quad q_t^{\infty}= & R_{t+1}+\gamma R_{t+2}+\ldots+\gamma^{T-t-1} R_T\end{aligned}
    $$
    
    n-step收益为：
    
    $$
    q_t^{(n)}=R_{t+1}+\gamma R_{t+2}+\ldots+\gamma^{n-1} R_{t+n}+\gamma^n Q\left(S_{t+n}, A_{t+n}\right)
    $$
    
    n-step迭代更新：
    
    $$
    Q\left(S_t, A_t\right) \leftarrow Q\left(S_t, A_t\right)+\alpha\left(q_t^{(n)}-Q\left(S_t, A_t\right)\right)
    $$
    

### 3.3.2 Q-learning: off-policy TD

1. off-policy
    1. Sarsa 是一种**在线策略（on-policy）**算法，它优化的是它实际执行的策略，它直接用下一步会执行的动作去优化 Q 表格。**在线策略**在学习的过程中，只存在一种策略，它用一种策略去做动作的选取，也用一种策略去做优化。所以 Sarsa 知道它下一步的动作有可能会跑到悬崖那边去，它就会在优化自己的策略的时候，尽可能离悬崖远一点。这样子就会保证，它下一步哪怕是有随机动作，它也还是在安全区域内。
    2. Q学习是一种**离线策略（off-policy）**算法。如图所示，**离线策略**在学习的过程中，有两种不同的策略：**目标策略（target policy）**和**行为策略（behavior policy）**。 
        - 目标策略是我们需要去学习的策略，一般用 $$\pi$$ 来表示。目标策略就像是在后方指挥战术的一个军师，它可以根据自己的经验来学习最优的策略，不需要去和环境交互。
        - 行为策略是探索环境的策略，一般用 *μ* 来表示。行为策略可以大胆地去探索到所有可能的轨迹，采集轨迹，采集数据，然后把采集到的数据“喂”给目标策略学习。而且“喂”给目标策略的数据中并不需要 $$a_{t+1}$$，而 Sarsa 是要有 $$a_{t+1}$$ 的。行为策略像是一个战士，可以在环境里面探索所有的动作、轨迹和经验，然后把这些经验交给目标策略去学习。比如目标策略优化的时候，Q学习不会管我们下一步去往哪里探索，它只选取奖励最大的策略。
            
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% include figure.html path="assets/img/RL_chapter3/Untitled 16.png" class="rounded z-depth-1" style="height: 200px;" %}
    </div>
</div>
            

1. off-policy学习有很多好处。
    1. 我们可以利用探索策略来学到最佳的策略，学习效率高； 
    2. 其次，off-policy学习可以让我们学习其他智能体的动作，进行模仿学习，学习人或者其他智能体产生的轨迹； 
    3. 最后，off-policy学习可以让我们重用旧的策略产生的轨迹，探索过程需要很多计算资源，这样可以节省资源。
2. 离线算法：
    
    Q学习有两种策略: 行为策略和目标策略。
    
    - 目标策略 $$\pi$$ 直接在 Q 表格上使用贪心策略，取它下一步能得到的所有状态，即
        
        $$
        \pi\left(s_{t+1}\right)=\underset{a^{\prime}}{\arg \max } Q\left(s_{t+1}, a^{\prime}\right) 
        $$
        
    - 行为策略 $$\mu$$ 可以是一个随机的策略，但我们采取 $$\varepsilon$$-贪心策略，让行为策略不至于是完全随机的，它是基于 Q 表格逐渐改进的。我们可以构造Q 学习目标， Q学习的下一个动作都是通过 argmax 操作选出来的，于是我们可得
        
        $$
        \begin{aligned}
        r_{t+1}+\gamma Q\left(s_{t+1}, A^{\prime}\right) & =r_{t+1}+\gamma Q\left(s_{t+1}, \underset{a^{\prime}}{\arg \max } Q\left(s_{t+1}, a^{\prime}\right)\right) \\
        & =r_{t+1}+\gamma \max_{a^{\prime}} Q\left(s_{t+1}, a^{\prime}\right)
        \end{aligned}
        $$
        
        接着我们可以把 Q 学习更新写成增量学习的形式，时序差分目标变成了 $$r_{t+1}+\gamma \max_a Q\left(s_{t+1}, a\right)$$ ，即
        
        $$
        Q\left(s_t, a_t\right) \leftarrow Q\left(s_t, a_t\right)+\alpha\left[r_{t+1}+\gamma \max_a Q\left(s_{t+1}, a\right)-Q\left(s_t, a_t\right)\right]
        $$
        
3. Q-learning和Sarsa：
    
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% include figure.html path="assets/img/RL_chapter3/Untitled 17.png" class="rounded z-depth-1" style="height: 200px;" %}
    </div>
</div>
    
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% include figure.html path="assets/img/RL_chapter3/Untitled 18.png" class="rounded z-depth-1" style="height: 200px;" %}
    </div>
</div>
    
    通过备份图也可以看出差异：
    
    - Sarsa里面A和A’都是通过一个同样的policy进行采样的，所以是在线策略
    - Q-learning里面，A和A’不是同一个策略。A是探索策略（目标策略），A’是直接通过max操作得到的策略（行为策略）。
    
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% include figure.html path="assets/img/RL_chapter3/Untitled 19.png" class="rounded z-depth-1" style="height: 200px;" %}
    </div>
</div>
    
    1. DP和TD的总结：
        
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% include figure.html path="assets/img/RL_chapter3/Untitled 20.png" class="rounded z-depth-1" style="height: 200px;" %}
    </div>
</div>
        

### 3.3.3 on-policy和off-policy的区别

总结一下在线策略和离线策略的区别。

- 只用了一个策略 $$\pi$$，它不仅使用策略 $$\pi$$ 学习，还使用策略 $$\pi$$ 与环境交互产生经验。 如果策略采用 $$\epsilon$$贪心算法，它需要兼顾探索，为了兼顾探索和利用，它训练的时候会显得有点“胆小”。它在解决悬崖行走问题的时候，会尽可能地远离悬崖边，确保哪怕自己不小心探索了一点儿，也还是在安全区域内。此外，因为采用的是 $$\epsilon$$贪心 算法，策略会不断改变（$$\epsilon$$ 值会不断变小），所以策略不稳定。
- 有两种策略————目标策略和行为策略。
    - 行为策略探索得到的经验轨迹来优化目标策略，从而更有可能探索到最佳策略。行为策略可以采用 $$\epsilon$$贪心 算法
    - 目标策略采用的是贪心算法，它直接根据行为策略采集到的数据来采用最佳策略，所以不需要兼顾探索。
    - off-policy是通过从行为策略$$\mu$$中的经验采样来学习目标策略$$\pi$$
- 我们比较一下 Q学习 和 Sarsa 的更新公式，就可以发现 Sarsa 并没有选取最大值的最大化操作。因此，Q学习是一个非常激进的方法，它希望每一步都获得最大的利益；Sarsa 则相对较为保守，它会选择一条相对安全的迭代路线。