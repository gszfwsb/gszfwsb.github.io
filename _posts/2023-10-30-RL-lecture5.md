---
layout: post
title: 第五章 Policy Optimization Foundation (策略优化基础篇)
date: 2023-10-30
description: PO
tags: intro
categories: Reinforcement-Learning
giscus_comments: true
related_posts: true
toc:
  beginning: true
---

# 一、基于策略的强化学习

1. 基于价值的RL和基于策略的RL有何区别？
    1. **决定性策略**是直接通过贪心算法，由价值函数生成我们应该采取的动作：
        
        $$
        a_t=\arg \max _a Q\left(a, s_t\right)
        $$
        
    2. **随机性策略**：我们也可以直接建模策略函数为一个参数化概率分布$$\pi_{\theta}(a|s)$$，参数为$$\theta$$
        
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% include figure.html path="assets/img/RL_chapter5/Untitled.png" class="rounded z-depth-1" style="height: 200px;" %}
    </div>
</div>

        
    3. 对比：
        
        
        | 基于价值的RL | 基于策略的RL | 演员-评论员 |
        | --- | --- | --- |
        | 学习价值函数 | 没有价值函数 | 同时学策略和价值函数 |
        | 通过价值函数得到隐式的策略 | 直接学习策略 |  |
2. policy-based RL特点: 有点类似监督学习
    1. 优势：
        1. 更好的收敛性：可以保证收敛，至少是local optimum
        2. 策略梯度在高维动作空间是更有效的
        3. 策略梯度可以学习随机策略，但是价值函数不能。
    2. 劣势：
        1. 经常会收敛到局部最优解
        2. 策略评估会有很大的方差

# 二、MC策略梯度

## 2.1 策略优化

1. 策略优化目标：给定一个带参 $$\theta$$ 策略近似器 $$\pi_{\theta}(s,a)$$，找到最优$$\theta$$
2. 如何衡量策略 $$\pi_{\theta}(s,a)$$ 的质量？
    1. 在离散（有轮数）环境中我们用**开始价值**
        
        $$
        J_1(\theta)=V^{\pi_\theta}\left(s_1\right)=\mathbb{E}_{\pi_\theta}\left[v_1\right]
        $$
        
    2. 在连续的环境（没有终止）我们可以用**价值均值**或者**回报均值**
        - 价值均值：
            
            $$
            J_{a v V}(\theta)=\sum_s d^{\pi_\theta}(s) V^{\pi_\theta}(s)
            $$
            
        - 回报均值：
            
            $$
            J_{a v R}(\theta)=\sum_s d^{\pi_\theta}(s) \sum_a \pi_\theta(s, a) R(s, a)
            $$
            
            其中$$\pi_\theta$$的马尔可夫分布稳态分布是$$d^{\pi_\theta}$$。
            
3. 策略价值：和我们在基于价值的RL中定义一样。
    
    $$
    \begin{aligned}J(\theta) & =\mathbb{E}_{\tau \sim \pi_\theta}\left[\sum_t R\left(s_t^\tau, a_t^\tau\right)\right] \\& \approx \frac{1}{m} \sum_m \sum_t R\left(s_t^m, a_t^m\right)\end{aligned}
    $$
    
    - $$\tau$$是我们从策略函数$$\pi_\theta$$中的采样轨迹
    - 我们忽略折扣
4. 基于策略的RL的目标：找到参数使得$$J(\theta)$$最大化
    
    $$
    \theta^*=\underset{\theta}{\arg \max } \mathbb{E}_{\tau \sim \pi_\theta}\left[\sum_t R\left(s_t^\tau, a_t^\tau\right)\right]
    $$
    
    - 如果$$J(\theta)$$可微，我们可以用基于梯度的方法解
        
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% include figure.html path="assets/img/RL_chapter5/Untitled 1.png" class="rounded z-depth-1" style="height: 200px;" %}
    </div>
</div>

        
    - 如果$$J(\theta)$$不可微或者难以求导，我们可以用一些不用梯度的黑盒优化方法：
        - 交叉熵法 (CEM)
            
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% include figure.html path="assets/img/RL_chapter5/Untitled 2.png" class="rounded z-depth-1" style="height: 200px;" %}
    </div>
</div>

            
        - 有限差分
            
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% include figure.html path="assets/img/RL_chapter5/Untitled 3.png" class="rounded z-depth-1" style="height: 200px;" %}
    </div>
</div>

            
5. 计算策略梯度：
    1. 假设策略非0时处处可微，我们可以计算梯度$$\nabla_\theta \pi_\theta(s, a)$$
    2. 一个计算trick (likelihood ratios)：优化目标就是$$\nabla_\theta \log \pi_\theta(s, a)$$
        
        $$
        \textcolor{blue}{\begin{aligned}\nabla_\theta \pi_\theta(s, a) & =\pi_\theta(s, a) \frac{\nabla_\theta \pi_\theta(s, a)}{\pi_\theta(s, a)} \\& =\pi_\theta(s, a) \nabla_\theta \log \pi_\theta(s, a)\end{aligned}}
        $$
        
    - 例1：高斯策略
        
        在连续动作空间，一个高斯策略的定义十分自然。其均值是状态特征的线性组合：$$\mu(s)=\phi(s)^T \theta$$。方差可以是参数化的也可以是常数。策略是连续、高斯的，即$$a \sim \mathcal{N}\left(\mu(s), \sigma^2\right)$$
        
        那么分数为：
        
        $$
        \nabla_\theta \log \pi_\theta(s, a)=\frac{(a-\mu(s)) \phi(s)}{\sigma^2}
        $$
        
    - 例2：softmax策略
        
        简单策略模型，用特征线性组合作为动作权重。其动作概率和权重大小指数成正比，即
        
        $$
        \pi_\theta(s, a)=\frac{\exp ^{\phi(s, a)^T \theta}}{\sum_{a^{\prime}} \exp ^{\phi\left(s, a^{\prime}\right)^T \theta}}
        $$
        
        那么分数为：
        
        $$
        \nabla_\theta \log \pi_\theta(s, a)=\phi(s, a)-\mathbb{E}_{\pi_\theta}[\phi(s, .)]
        $$
        

## 2.2 单步MDP的策略梯度

考虑单步MDP的简单情况： 从状态$$s\sim d(s)$$开始，到一个时间单位后结束，奖励为$$r=R(s,a)$$。我们用似然比例去计算策略梯度：

$$
\begin{aligned}J(\theta)  =\mathbb{E}_{\pi_\theta}[r]  =\sum_{s \in \mathcal{S}} d(s) \sum_{a \in \mathcal{A}} \pi_\theta(s, a) r\end{aligned}
$$

其梯度为：

$$
\begin{aligned}\nabla_\theta J(\theta) & =\sum_{s \in \mathcal{S}} d(s) \sum_{a \in \mathcal{A}} \pi_\theta(s, a) \nabla_\theta \log \pi_\theta(s, a) r \\& =\mathbb{E}_{\pi_\theta}\left[r \nabla_\theta \log \pi_\theta(s, a)\right]\end{aligned}
$$

## 2.3 多步MDP的策略梯度

1. 一回合的轨迹可以表示为：$$\tau=\left(s_0, a_0, r_1, \ldots s_{T-1}, a_{T-1}, r_T, s_T\right) \sim\left(\pi_\theta, P\left(s_{t+1} \mid s_t, a_t\right)\right)$$
2. 其轨迹的奖励为$$R(\tau)=\sum_{t=0}^{T-1} R\left(s_t, a_t\right)$$
3. 策略目标为：
    
    $$
    J(\theta)=\mathbb{E}_{\pi_\theta}\left[\sum_{t=0}^{T-1} R\left(s_t, a_t\right)\right]=\sum_\tau P(\tau ; \theta) R(\tau)
    $$
    
    其中$$P(\tau ; \theta)=d(s_0)\pi_{\theta}(a_0|s_0)p(s_1|s_0,a_0)\cdots \pi_{\theta}(a_{T-1}|s_{T-1})p(s_T|s_{T-1},a_{T-1})=d\left(s_0\right) \prod_{t=0}^{T-1} \pi_\theta\left(a_t \mid s_t\right) p\left(s_{t+1} \mid s_t, a_t\right)$$表示执行策略$$\pi_\theta$$时在轨迹中的概率
    
4. 总体目标：max $$J(\theta)$$
    
    $$
    \theta^*=\underset{\theta}{\arg \max } J(\theta)=\underset{\theta}{\arg \max } \sum_\tau P(\tau ; \theta) R(\tau)
    $$
    
5. 计算梯度：
    
    $$
    \begin{aligned}\nabla_\theta J(\theta) & =\nabla_\theta \sum_\tau P(\tau ; \theta) R(\tau) \\& =\sum_\tau \nabla_\theta P(\tau ; \theta) R(\tau) \\& =\sum_\tau \frac{P(\tau ; \theta)}{P(\tau ; \theta)} \nabla_\theta P(\tau ; \theta) R(\tau) \\& =\sum_\tau P(\tau ; \theta) R(\tau) \frac{\nabla_\theta P(\tau ; \theta)}{P(\tau ; \theta)} \\& =\sum_\tau P(\tau ; \theta) R(\tau) \nabla_\theta \log P(\tau ; \theta)\end{aligned}
    $$
    
6. 经验估计策略，近似：
    
    $$
    \nabla_\theta J(\theta) \approx \frac{1}{m} \sum_{i=1}^m R\left(\tau_i\right) \nabla_\theta \log P\left(\tau_i ; \theta\right)
    $$
    
7. 分解轨迹为状态和动作 $$\nabla_\theta \log P(\tau ; \theta)$$ 
    
    $$
    \begin{aligned}\nabla_\theta \log P(\tau ; \theta) & =\nabla_\theta \log \left[d\left(s_0\right) \prod_{t=0}^{T-1} \pi_\theta\left(a_t \mid s_t\right) p\left(s_{t+1} \mid s_t, a_t\right)\right] \\& =\nabla_\theta\left[\log d\left(s_0\right)+\sum_{t=0}^{T-1} \log \pi_\theta\left(a_t \mid s_t\right)+\log p\left(s_{t+1} \mid s_t, a_t\right)\right] \\& =\sum_{t=0}^{T-1} \nabla_\theta \log \pi_\theta\left(a_t \mid s_t\right)\end{aligned}
    $$
    
    $$
    \textcolor{blue}{\nabla_\theta J(\theta) \approx \frac{1}{m} \sum_{i=1}^m R\left(\tau_i\right) \sum_{t=0}^{T-1} \nabla_\theta \log \pi_\theta\left(a_t^i \mid s_t^i\right)}
    $$
    
    我们不需要知道**转移模型**。
    

# 三、减少策略梯度的方差

## 3.1 理解梯度优化的过程

我们考察的梯度是$$E_{\tau \sim \pi_\theta}[R(\tau)]$$.更一般的，如果我们需要计算函数$$f(x)$$的期望，我们有：

$$
\begin{aligned}\nabla_\theta \mathbb{E}_{p(x ; \theta)}[f(x)] & =\mathbb{E}_{p(x ; \theta)}\left[f(x) \nabla_\theta \log p(x ; \theta)\right] \\& \approx \frac{1}{S} \sum_{s=1}^S f\left(x_s\right) \nabla_\theta \log p\left(x_s ; \theta\right), \text { where } x_s \sim p(x ; \theta)\end{aligned}
$$

1. 如何理解梯度：
    1. 移动分布$$p$$（优化参数$$\theta$$）使得其未来的样本$$x$$可以得到更高的分数
    2. 向量$$f(x)\nabla_\theta \log p(x;\theta)$$把样本的log-likelihood推向一个表示它的分数。
        
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% include figure.html path="assets/img/RL_chapter5/Untitled 4.png" class="rounded z-depth-1" style="height: 200px;" %}
    </div>
</div>

        
2. 与极大似然比较：
    1. 策略梯度估计：
        
        $$
        \nabla_\theta J(\theta) \approx \frac{1}{M} \sum_{m=1}^M\left(\sum_{t=1}^T \nabla_\theta \log \pi_\theta\left(a_{t, m} \mid s_{t, m}\right)\right)\textcolor{red}{\left(\sum_{t=1}^T r\left(s_{t, m}, a_{t, m}\right)\right)}
        $$
        
    2. 极大似然估计：
        
        $$
        \nabla_\theta J_{M L}(\theta) \approx \frac{1}{M} \sum_{m=1}^M\left(\sum_{t=1}^T \nabla_\theta \log \pi_\theta\left(a_{t, m} \mid s_{t, m}\right)\right)
        $$
        
    
    主要区别：**给好的动作更大的可能性，给不好的动作更小的可能性**
    

## 3.2 策略梯度中方差较大的问题

1. 我们的优化如下：
    
    $$
    \nabla_\theta J(\theta) \approx \frac{1}{m} \sum_{i=1}^m R\left(\tau_i\right) \sum_{t=0}^{T-1} \nabla_\theta \log \pi_\theta\left(a_t^i \mid s_t^i\right)
    $$
    
    - 尽管是Unbiased，但是很Noisy
    - 如何修改？
        - 用时间因果
        - 引入baseline
2. 通过时间因果解决大方差问题：
    1. 此前我们有：$$\nabla_\theta \mathbb{E}_\tau[R]=\mathbb{E}_\tau \left[\left(\sum_{t=0}^{T-1} r_t\right)\left(\sum_{t=0}^{T-1} \nabla_\theta \log \pi_\theta\left(a_t \mid s_t\right)\right)\right]$$
    2. 我们可以得到某一时刻的单一奖励：
        
        $$
        \nabla_\theta \mathbb{E}_\tau\left[r_{t^{\prime}}\right]=\mathbb{E}_\tau\left[r_{t^{\prime}} \sum_{t=0}^{t^{\prime}} \nabla_\theta \log \pi_\theta\left(a_t \mid s_t\right)\right]
        $$
        
    3. 通过对$$t$$求和，我们可以得到：
        
        $$
        \begin{aligned}\nabla_\theta J(\theta)=\nabla_\theta \mathbb{E}_{\tau \sim \pi_\theta}[R] & =\mathbb{E}_\tau\left[\sum_{t^{\prime}=0}^{T-1} r_{t^{\prime}} \sum_{t=0}^{t^{\prime}} \nabla_\theta \log \pi_\theta\left(a_t \mid s_t\right)\right] \\& =\mathbb{E}_\tau\left[\sum_{t=0}^{T-1} \nabla_\theta \log \pi_\theta\left(a_t \mid s_t\right) \sum_{\textcolor{red}{t^{\prime}=t}}^{T-1} r_{t^{\prime}}\right] \\& =\mathbb{E}_\tau\left[\sum_{t=0}^{T-1} G_t \cdot \nabla_\theta \log \pi_\theta\left(a_t \mid s_t\right)\right]\end{aligned}
        $$
        
    4. 因此我们有：
        
        $$
        \nabla_\theta \mathbb{E}_{\tau \sim \pi_\theta}[R]=\mathbb{E}_\tau\left[\sum_{t=0}^{T-1} G_t \cdot \nabla_\theta \log \pi_\theta\left(a_t \mid s_t\right)\right]
        $$
        
        - $$G_t=\sum_{t^{\prime}=t}^{T-1} r_{t^{\prime}}$$ 是$$t$$步轨迹中的回报
        - 因果：时间靠后的策略不能影响时间靠前的策略
        - 所以我们可以得到如下的估计更新：
        
        $$
        \nabla_\theta \mathbb{E}[R] \approx \frac{1}{m} \sum_{i=1}^m \sum_{t=0}^{T-1} G_t^{(i)} \cdot \nabla_\theta \log \pi_\theta\left(a_t^i \mid s_t^i\right)
        $$
        
    
    <aside>
    😍 例：**REINFORCE: 一种MC策略梯度算法**
    
    算法简单从策略中采样多个轨迹的样本，并且同时更新$$\theta$$参数：
    
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% include figure.html path="assets/img/RL_chapter5/Untitled 5.png" class="rounded z-depth-1" style="height: 200px;" %}
    </div>
</div>

    
    </aside>
    
3. 通过引入baseline解决方差大的问题：
    1. 原始更新：$$G_t=\sum_{t^{\prime}=t}^{T-1} r_{t^{\prime}}$$可能有很大的方差
        
        $$
        \nabla_\theta \mathbb{E}_{\tau \sim \pi_\theta}[R]=\mathbb{E}_\tau\left[\sum_{t=0}^{T-1} \textcolor{red}{G_t} \cdot \nabla_\theta \log \pi_\theta\left(a_t \mid s_t\right)\right]
        $$
        
    2. 可以通过减去一个baseline来减小方差
        
        $$
        \nabla_\theta \mathbb{E}_{\tau \sim \pi_\theta}[R]=\mathbb{E}_\tau\left[\sum_{t=0}^{T-1} \textcolor{red}{(G_t-b(s_t))} \cdot \nabla_\theta \log \pi_\theta\left(a_t \mid s_t\right)\right]
        $$
        
    3. 一个好的baseline就是期望奖励：
        
        $$
        b\left(s_t\right)=\mathbb{E}\left[r_t+r_{t+1}+\ldots+r_{T-1}\right]
        $$
        
    4. 解释：我们优化的不再是$$G_t$$本身，而是$$G_t$$可以比baseline好多少。可以证明这样可以降低方差：
        
        $$
        \begin{aligned}\mathbb{E}_\tau\left[\nabla_\theta \log \pi_\theta\left(a_t \mid s_t\right) b\left(s_t\right)\right] & =0, \\E_\tau\left[\nabla_\theta \log \pi_\theta\left(a_t \mid s_t\right)\left(G_t-b\left(s_t\right)\right)\right] & =E_\tau\left[\nabla_\theta \log \pi_\theta\left(a_t \mid s_t\right) G_t\right] \\\operatorname{Var}_\tau\left[\nabla_\theta \log \pi_\theta\left(a_t \mid s_t\right)\left(G_t-b\left(s_t\right)\right)\right] & <\operatorname{Var}_\tau\left[\nabla_\theta \log \pi_\theta\left(a_t \mid s_t\right) G_t\right]\end{aligned}
        $$
        
    5. 同样，我们也可以给baseline一个参数来进行学习：
        
        $$
        \nabla_\theta \mathbb{E}_{\tau \sim \pi_\theta}[R]=\mathbb{E}_\tau\left[\sum_{t=0}^{T-1} \textcolor{red}{(G_t-b_{\mathbf{w}}(s_t))} \cdot \nabla_\theta \log \pi_\theta\left(a_t \mid s_t\right)\right]
        $$
        
        $$\mathbf{w}$$用于训练baseline，因此我们有两套参数：$$\mathbf{w}$$和$$\theta$$.
        
    
    <aside>
    😍 带有baseline的策略梯度算法：
    
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% include figure.html path="assets/img/RL_chapter5/Untitled 6.png" class="rounded z-depth-1" style="height: 200px;" %}
    </div>
</div>

    
    </aside>
    

# 四、**演员-评论员**

1. 除了引入baseline和时间因果，我们还可以通过引入Critic的方式来降低方差，这就引入了一个新的算法。
    
    $$
    \nabla_\theta \mathbb{E}_{\tau \sim \pi_\theta}[R]=\mathbb{E}_\tau\left[\sum_{t=0}^{T-1} \textcolor{red}{G_t} \cdot \nabla_\theta \log \pi_\theta\left(a_t \mid s_t\right)\right]
    $$
    
    - 实践上$$G_t$$是MC策略梯度的一个样本，是对$$Q^{\pi_\theta}(s_t,a_t)$$的一个unbiased noisy估计。
    - 实际上我们可以不用$$G_t$$，而用一个评论员去估计Q函数。
        
        $$
        Q_{\mathbf{w}}(s, a) \approx Q^{\pi_\theta}(s, a)
        $$
        
        $$
        \nabla_\theta J(\theta)=\mathbb{E}_{\pi_\theta}\left[\sum_{t=0}^{T-1} Q_{\textcolor{red}{\mathbf{w}}}\left(s_t, a_t\right) \cdot \nabla_\theta \log \pi_{\textcolor{red}\theta}\left(a_t \mid s_t\right)\right]
        $$
        
2. 上式就是Actor-Critic策略梯度：
    - Actor：policy function，用于生成动作，学习参数$$\theta$$
    - Critic：value function，衡量动作的回报，学习参数$$\mathbf{w}$$
        
        > Critic：类似于策略评估的作用
        > 

## 4.1 动作价值动作评论员算法(QAC)

1. 用一个线性价值函数近似：
    
    $$
    Q_{\mathbf{w}}(s, a)=\psi(s, a)^T \mathbf{w}
    $$
    
    其中，评论员用一个线性TD(0)来更新参数$$\mathbf{w}$$，二演员通过策略梯度更新参数$$\theta$$.
    
2. 具体算法流程：
3. 演员-评论员函数近似方法：
    
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% include figure.html path="assets/img/RL_chapter5/Untitled 7.png" class="rounded z-depth-1" style="height: 200px;" %}
    </div>
</div>


我们可以有两个不同的函数来近似策略函数和价值函数，但是我们也可以用一个共享的网络来涉及同时得到两个函数。

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% include figure.html path="assets/img/RL_chapter5/Untitled 8.png" class="rounded z-depth-1" style="height: 200px;" %}
    </div>
</div>


## 4.2 带basline的演员-评论员算法(A2C & TDAC)

1. 我们定义Q函数如下：
    
    $$
    Q^{\pi, \gamma}(s, a)=\mathbb{E}_\pi\left[r_1+\gamma r_2+\ldots \mid s_1=s, a_1=a\right]
    $$
    
    那么其实价值函数就是一个很好的baseline：
    
    $$
    \begin{aligned}V^{\pi, \gamma}(s) & =\mathbb{E}_\pi\left[r_1+\gamma r_2+\ldots \mid s_1=s\right] \\& =\mathbb{E}_{a \sim \pi}\left[Q^{\pi, \gamma}(s, a)\right]\end{aligned}
    $$
    
2. advantage function：我们定义带有baseline（价值函数）的Q函数为advantage function，即
    
    $$
    A^{\pi, \gamma}(s, a)=Q^{\pi, \gamma}(s, a)-V^{\pi, \gamma}(s)
    $$
    
3. 策略梯度为：
    
    $$
    \nabla_\theta J(\theta)=\mathbb{E}_{\pi_\theta}\left[\nabla_\theta \log \pi_\theta(s, a) A^{\pi, \gamma}(s, a)\right]
    $$
    
4. 在策略梯度里面，我们可以用MC来估计回报，也可以用TD方法来估计。如果我们考虑一个n步的回报，我们有：
    
    $$
    \begin{aligned}n=1(T D) & G_t^{(1)}=r_{t+1}+\gamma v\left(s_{t+1}\right) \\n=2 \quad & G_t^{(2)}=r_{t+1}+\gamma r_{t+2}+\gamma^2 v\left(s_{t+2}\right) \\n=\infty(M C) & G_t^{(\infty)}=r_{t+1}+\gamma r_{t+2}+\ldots+\gamma^{T-t-1} r_T\end{aligned}
    $$
    
    那么我们的advantage function估计可以写为：
    
    $$
    \begin{aligned}& \hat{A}_t^{(1)}=r_{t+1}+\gamma v\left(s_{t+1}\right)-v\left(s_t\right) \\& \hat{A}_t^{(2)}=r_{t+1}+\gamma r_{t+2}+\gamma^2 v\left(s_{t+2}\right)-v\left(s_t\right) \\& \hat{A}_t^{(\infty)}=r_{t+1}+\gamma r_{t+2}+\ldots+\gamma^{T-t-1} r_T-v\left(s_t\right)\end{aligned}
    $$
    
    注意到，$$\hat{A}^{(\infty)}$$有大var，小bias，而$$\hat{A}^{(1)}$$有小var，大bias。
    
5. 不同时间尺度的演员：
    
    策略梯度可以在很多时间尺度上估计
    
    $$
    \nabla_\theta J(\theta)=\mathbb{E}_{\pi_\theta}\left[\nabla_\theta \log \pi_\theta(s, a) A^{\pi_\theta}(s, a)\right]
    $$
    
    - MC演员-评论员策略梯度用**整个回报**误差来优化：
        
        $$
        \nabla_\theta J(\theta)=\alpha\left(G_t-V_\kappa\left(s_t\right)\right) \nabla_\theta \log \pi_\theta\left(s_t, a_t\right)
        $$
        
    - TD演员-评论员策略梯度用TD误差来优化：
        
        $$
        \nabla_\theta J(\theta)=\alpha\left(r+\gamma V_\kappa\left(s_{t+1}\right)-V_\kappa\left(s_t\right)\right) \nabla_\theta \log \pi_\theta\left(s_t, a_t\right)
        $$
        
    - k步TD演员-评论员策略梯度用k步回报误差来优化：
        
        $$
        \nabla_\theta J(\theta)=\alpha\left(\sum_{i=0}^k \gamma^i r_{t+i}+\gamma^k V_\kappa\left(s_{t+k}\right)-V_\kappa\left(s_t\right)\right) \nabla_\theta \log \pi_\theta\left(s_t, a_t\right)
        $$
        
    
    > TD算法只需要优化critic的参数$$\kappa$$！
    > 

# 五、策略梯度小结

1. 策略梯度可以解决不可微的情形：我们可以通过随机策略采样来结算
    
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% include figure.html path="assets/img/RL_chapter5/Untitled 9.png" class="rounded z-depth-1" style="height: 200px;" %}
    </div>
</div>

    
2. 训练过程中我们会产生一些样本（图中不同分支），我们可以让不同样本尽可能得到比较好的结果（比如小的loss）。
    
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% include figure.html path="assets/img/RL_chapter5/Untitled 10.png" class="rounded z-depth-1" style="height: 200px;" %}
    </div>
</div>

    

3. 策略梯度拓展：大部分SOTA的RL方法都是基于策略的。

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% include figure.html path="assets/img/RL_chapter5/Untitled 11.png" class="rounded z-depth-1" style="height: 200px;" %}
    </div>
</div>


4. 不同RL流派：
    1. 基于价值的RL：通过DP解决RL
        1. 经典RL和控制论
        2. 代表性算法：Deep Q-learning和其变种
        3. 代表研究人员：Richard Sutton，David Silver
        4. 代表机构：Deepmind
    2. 基于策略的RL：通过学习解决RL
        1. 机器学习和深度学习
        2. 代表性算法：PG和其变种
        3. 代表研究人员：Pieter Abeel, Sergey Levine, John Schulman
        4. 代表机构：OpenAI, Berkeley