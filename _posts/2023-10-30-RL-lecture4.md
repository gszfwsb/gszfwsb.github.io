---
layout: post
title: 第四章 Value Function Approximation and Deep Q-learning (价值函数近似和深度Q学习)
date: 2023-10-30
description: VFA and Q-learning
tags: intro
categories: Reinforcement-Learning
giscus_comments: true
related_posts: true
toc:
  beginning: true
---


# 一、函数近似

## 1.1 问题引入：大规模的MDP问题如何估计价值函数？

在面对大规模 MDP 问题时，要避免用table去表示特征（Q-tabel等），而是采用带参数的**函数近似**的方式去近似估计V、Q、π

1. 表格型方法
    - 在表格型方法中，我们是通过查表的方式去计算价值函数的
    - 每一个状态-动作对 $$<s,a>$$有一个元素$$Q(s,a)$$
        
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% include figure.html path="assets/img/RL_chapter4/Untitled.png" class="rounded z-depth-1" style="height: 200px;" %}
    </div>
</div>
        
2. MDP的挑战：
    - 太多状态和动作需要存下来
    - 单独学习每一个状态的价值很慢

## 1.2 通过函数近似的方法来解决大规模RL

1. 如何隐式学习或存储每个状态？需要学习的包括：
    1. 状态转移模型、奖励模型
    2. 价值函数，Q函数
    3. 策略
2. 解决方法：通过**价值函数近似（VFA）**来逼近。通过参数$$\mathbf{w}$$估计价值函数、Q函数和策略。
    
    $$
    \begin{aligned}\hat{v}(s, \mathbf{w}) & \approx v^\pi(s) \\\hat{q}(s, a, \mathbf{w}) & \approx q^\pi(s, a) \\\hat{\pi}(a, s, \mathbf{w}) & \approx \pi(a \mid s)\end{aligned}
    $$
    
    1. 可以从见过的状态泛化到没有见到的状态
    2. 可以通过MC或者TD方法训练$$\mathbf{w}$$
3. 一些近似方法：其中线性方法和神经网络是我们关注的重点
    1. 特征线性组合
    2. 神经网络
    3. 决策树
    4. 最近邻

# 二、预测中的价值函数近似

## 2.1 有Oracle/模型预测VFA

### 2.1.1 基本内容

1. Oracle：我们知道gt的value function $$v^\pi(s)$$，对所有状态$$s$$都是已知的。
2. 我们的目标是学习对$$v^\pi(s)$$的近似。
3. 因此我们MSE来定义loss：
    
    $$
    J(\mathbf{w})=\mathbb{E}_\pi\left[\left(v^\pi(s)-\hat{v}(s, \mathbf{w})\right)^2\right]
    $$
    
4. 梯度下降：
    
    $$
    \begin{aligned}\Delta \mathbf{w} & =-\frac{1}{2} \alpha \nabla_{\mathbf{w}} J(\mathbf{w}) \\\mathbf{w}_{t+1} & =\mathbf{w}_t+\Delta \mathbf{w}\end{aligned}
    $$
    

### 2.1.2 线性方法

1. 如何通过特征向量表示状态？
    
    $$
    \mathbf{x}(s)=\left(x_1(s), \ldots, x_n(s)\right)^T
    $$
    
2. 价值函数——特征线性组合：

$$
\hat{v}(s, \mathbf{w})=\mathbf{x}(s)^T \mathbf{w}=\sum_{j=1}^n x_j(s) w_j
$$

1. 目标函数：

$$
J(\mathbf{w})=\mathbb{E}_\pi\left[\left(v^\pi(s)-\mathbf{x}(s)^T \mathbf{w}\right)^2\right]
$$

$$
\Delta \mathbf{w}=\alpha\left(v^\pi(s)-\hat{v}(s, \mathbf{w})\right) \mathbf{x}(s)
$$

😍 ***Update = StepSize × PredictionError × FeatureValue***


### 2.1.3 通过查特征表的线性方法

1. 是一种特殊的线性VFA
2. 查表的方式是通过1-hot向量实现的，特征都是0或1：

$$
\mathbf{x}^{\text {table }}(s)=\left(\mathbf{1}\left(s=s_1\right), \ldots, \mathbf{1}\left(s=s_n\right)\right)^T
$$

1. 事实上价值函数的每一项恰好就是训练参数：

$$
\hat{v}(s, \mathbf{w})=\left(\mathbf{1}\left(s=s_1\right), \ldots, \mathbf{1}\left(s=s_n\right)\right)\left(w_1, \ldots, w_n\right)^T\\\hat{v}\left(s_k, \mathbf{w}\right)=w_k
$$

## 2.2 无模型预测VFA

实践中，没有办法知道所有状态的gt价值函数，没有oracle。

回想下在无模型的预测中做的事：

1. 目标是去通过给定策略 $$\pi$$ 估计价值函数 $$v^\pi$$ 
2. 维护一个 $$v^\pi$$ 或者 $$q^\pi$$ 的表格
3. 每回合评估更新（MC） 或 每一步评估更新（TD）

### 2.2.1 **增量式函数近似预测算法**（Incremental VFA Prediction Algorithms）

1. 在有gt价值函数的情形下，我们有：
    
    $$
    \Delta \mathbf{w}=\alpha\left(\textcolor{red}{v^\pi(s)}-\hat{v}(s, \mathbf{w})\right) \nabla_{\mathbf{w}} \hat{v}\left(s_t, \mathbf{w}\right)
    $$
    
2. 在真实场景下没有$$\textcolor{red}{v^\pi(s)}$$，我们需要寻求替代方案：
    - 在MC中，我们用实际return代替
        
        $$
        \Delta \mathbf{w}=\alpha\left(\textcolor{red}{G_t}-\hat{v}\left(s_t, \mathbf{w}\right)\right) \nabla_{\mathbf{w}} \hat{v}\left(s_t, \mathbf{w}\right)
        $$
        
    - 在TD(0)中，我们用$$\delta_t$$代替
        
        $$
        \Delta \mathbf{w}=\alpha\left(\textcolor{red}{R_{t+1}+\gamma \hat{v}\left(s_{t+1}, \mathbf{w}\right)}-\hat{v}\left(s_t, \mathbf{w}\right)\right) \nabla_{\mathbf{w}} \hat{v}\left(s_t, \mathbf{w}\right)
        $$
        

### 2.2.2 基于VFA的MC预测

1. 奖励 $$G_t$$  是unbiased的，但是对于真正的value function是带有noise的，只有采样大量的$$G_t$$ 才能恢复出$$v^\pi(s_t)$$，因为$$\mathbb{E}[G_t]=v^\pi(s_t)$$。
2. 因此我们可以把 $$G_t$$ 看成label，$$S_t$$ 看成input，做一个监督学习任务。
    
    $$
    <S_1, G_1>,<S_2, G_2>, \ldots,<s_t, G_T> 
    $$
    
3. MC策略评估更新可以写为：
    
    $$
    \begin{aligned}
    \Delta \mathbf{w} & =\alpha\left(G_t-\hat{v}\left(s_t, \mathbf{w}\right)\right) \nabla_{\mathbf{w}} \hat{v}\left(s_t, \mathbf{w}\right) \\
    & =\alpha\left(G_t-\hat{v}\left(s_t, \mathbf{w}\right)\right) \mathbf{x}\left(s_t\right)
    \end{aligned}
    $$
    
4. MC预测在线性/非线性VFA中都可以收敛到globel最优。

### 2.2.3 基于VFA的TD预测

1. TD目标$$R_{t+1}+\gamma \hat{v}\left(s_{t+1}, \mathbf{w}\right)$$是biased的，因为$$\mathbb{E}[R_{t+1}+\gamma \hat{v}\left(s_{t+1}, \mathbf{w}\right)]\neq v^\pi (s_t)$$
2. 我们有类似的监督学习训练pair如下：
    
    $$
    <S_1, R_2+\gamma \hat{v}\left(s_2, \mathbf{w}\right)>,<S_2, R_3+\gamma \hat{v}\left(s_3, \mathbf{w}\right)>, \ldots,<S_{T-1}, R_T>
    $$
    
3. TD(0)中的更新可以写为：
    
    $$
    \begin{aligned}\Delta \mathbf{w} & =\alpha\left(R+\gamma \hat{v}\left(s^{\prime}, \mathbf{w}\right)-\hat{v}(s, \mathbf{w})\right) \nabla_{\mathbf{w}} \hat{v}(s, \mathbf{w}) \\& =\alpha\left(R+\gamma \hat{v}\left(s^{\prime}, \mathbf{w}\right)-\hat{v}(s, \mathbf{w})\right) \mathbf{x}(s)\end{aligned}
    $$
    
    这个也叫做semi-gradient，因为我们忽略了改变权重向量$$\mathbf{w}$$对于target的效应。
    
4. 线性TD(0)才可以收敛到globel最优。

# 三、 控制中的价值函数近似

广义策略迭代：

        
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% include figure.html path="assets/img/RL_chapter4/Untitled 1.png" class="rounded z-depth-1" style="height: 200px;" %}
    </div>
</div>

- 策略评估：近似策略评估，$$\hat{q}(., ., \mathbf{w}) \approx q^\pi$$
- 策略改进：$$\varepsilon$$-贪心策略改进

## 3.1 有Oracle/模型控制VFA

1. 近似动作价值函数：
    
    $$
    \hat{q}(s, a, \mathbf{w}) \approx q^\pi(s, a)
    $$
    
2. loss function:
    
    $$
    J(\mathbf{w})=\mathbb{E}_\pi\left[\left(q^\pi(s, a)-\hat{q}(s, a, \mathbf{w})\right)^2\right]
    $$
    
3. 梯度求解：
    
    $$
    \Delta \mathbf{w}=\alpha\left(q^\pi(s, a)-\hat{q}(s, a, \mathbf{w})\right) \nabla_{\mathbf{w}} \hat{q}(s, a, \mathbf{w})
    $$
    

## 3.2 无模型控制中的VFA

### 3.2.1 增量控制算法（Incremental Control Algorithm）

类似的，真实情况中我们没有oracle了。

1. MC：
    
    $$
    \Delta \mathbf{w}=\alpha\left(\textcolor{red}{G_t}-\hat{q}\left(s_t, a_t, \mathbf{w}\right)\right) \nabla_{\mathbf{w}} \hat{q}\left(s_t, a_t, \mathbf{w}\right)
    $$
    
2. Sarsa：
    
    $$
    \Delta \mathbf{w}=\alpha\left(\textcolor{red}{R_{t+1}+\gamma \hat{q}\left(s_{t+1}, a_{t+1}, \mathbf{w}\right)}-\hat{q}\left(s_t, a_t, \mathbf{w}\right)\right) \nabla_{\mathbf{w}} \hat{q}\left(s_t, a_t, \mathbf{w}\right)
    $$
    
3. Q-learning：
    
    $$
    \Delta \mathbf{w}=\alpha\left(\textcolor{red}{R_{t+1}+\gamma \max _a \hat{q}\left(s_{t+1}, a, \mathbf{w}\right)}-\hat{q}\left(s_t, a_t, \mathbf{w}\right)\right) \nabla_{\mathbf{w}} \hat{q}\left(s_t, a_t, \mathbf{w}\right)
    $$
    
4. Semi-gradient Sarsa算法流程：
    
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% include figure.html path="assets/img/RL_chapter4/Untitled 2.png" class="rounded z-depth-1" style="height: 200px;" %}
    </div>
</div>
    

### 3.2.2 收敛性分析（Sarsa/Q-learning）

1. TD中的VFA不遵从任何一个Loss function
2. 这个更新包含两个近似，都会引入噪声：
    1. 贝尔曼备份公式中的近似
    2. 近似价值函数的近似
3. 一个挑战：行为策略和目标策略不同，因此价值函数的VFA可能没法收敛。
4. 死亡三角：
    1. 函数近似：近似会引入误差
    2. bootstrap：TD依赖于之前的估计会引入bias，MC方法避免了这个问题
    3. off-policy训练：采样分布和实际分布差异比较大
5. 收敛性总结：
    
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% include figure.html path="assets/img/RL_chapter4/Untitled 3.png" class="rounded z-depth-1" style="height: 200px;" %}
    </div>
</div>
    

### 3.2.3 batch强化学习

1. 想法：
    1. 增量梯度下降更新是简单的，但是并不是有效率的采样方式，每走一步优化一次。
    2. 基于batch的方法会找一批中的数据。
2. 建模：假设经验包含了$$<state, value>$$对。
    
    $$
    D=\{<s_i,v_i>\}_{t=1}^T
    $$
    
    迭代时重复两步操作： 
    
    1. 随机采样一对，$$<s, v^\pi>\sim \mathcal{D}$$
    2. 用梯度下降法进行优化：$$\Delta \mathbf{w}=\alpha\left(v^\pi-\hat{v}(s, \mathbf{w})\right) \nabla_{\mathbf{w}} \hat{v}(s, \mathbf{w})$$
    
    等价于MSE loss
    
    $$
    \mathbf{w}^{L S}=\underset{\mathbf{w}}{\arg \min } \sum_{t=1}^T\left(v_t^\pi-\hat{v}\left(s_t, \mathbf{w}\right)\right)^2
    $$
    

# 三、深度Q网络

1. 线性VFA vs 非线性VFA
    1. 线性VFA通常在给定正确的特征时效果很好，但是需要人工设计特征集。
    2. 非线性VFA不需要人工涉及特征，直接用DNN
2. 深度强化学习
    1. DNN需要表示：价值函数、策略函数以及环境模型
    2. 优化：SGD

## 3.1 Deep Q-Learning (DQN)

1. DQN通过神经网络表示Q函数
2. DQN玩雅达利游戏：
    1. 端到端学习$$Q(s,a)$$，直接把像素帧作为输入
    2. 输入最近4帧状态$$s$$的像素
    3. 输出是$$Q(s,a)$$代表18个按钮位置
    4. 奖励是这一步的分数
    
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% include figure.html path="assets/img/RL_chapter4/Untitled 4.png" class="rounded z-depth-1" style="height: 200px;" %}
    </div>
</div>
    
3. 两个重要的问题：
    1. 样本之间的关系怎么处理？像素级别的关联很高。解决：experience replay
    2. target是带有noise的。解决：固定Q targets

## 3.2 DQN：experience replay

1. 为了减少样本之间的关联性，我们把 $$(s_t,a_t,r_t,s_{t+1})$$ 存进一个回放状态转移memory $$D$$，这是随机打乱的。希望采样出来的经验和目前状态没有很强的相关性。
    
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% include figure.html path="assets/img/RL_chapter4/Untitled 5.png" class="rounded z-depth-1" style="height: 200px;" %}
    </div>
</div>
    
2. 经验回放具体步骤：
    1. 在数据集中采样经验：$$\left(s, a, r, s^{\prime}\right) \sim \mathcal{D}$$
    2. 计算采样出的价值目标：$$r+\gamma \max _{a^{\prime}} \hat{Q}\left(s^{\prime}, a^{\prime}, \mathbf{w}\right)$$
    3. 通过SGD算法更新网络权重:
        
        $$
        \Delta \mathbf{w}=\alpha\left(r+\gamma \max _{a^{\prime}} \hat{Q}\left(s^{\prime}, a^{\prime}, \mathbf{w}\right)-Q(s, a, \mathbf{w})\right) \nabla_{\mathbf{w}} \hat{Q}(s, a, \mathbf{w})
        $$
        
3. 固定Q目标
    1. 为了提高训练的稳定性，希望在多次更新时，计算target时固定其权重。
    2. 用一组新的参数$$\mathbf{w^-}$$作为使用的权重，$$\mathbf{w}$$是我们需要更新的权重。
    3. 固定目标的经验回放具体步骤：
        1. 在数据集中采样经验：$$\left(s, a, r, s^{\prime}\right) \sim \mathcal{D}$$
        2. 计算采样出的价值目标：$$r+\gamma \max _{a^{\prime}} \hat{Q}\left(s^{\prime}, a^{\prime}, \textcolor{red}{\mathbf{w}^-} \right)$$
        3. 通过SGD算法更新网络权重:
            
            $$
            \Delta \mathbf{w}=\alpha\left(r+\gamma \max _{a^{\prime}} \hat{Q}\left(s^{\prime}, a^{\prime}, \textcolor{red}{\mathbf{w}^{-}}\right)-Q(s, a, \mathbf{w})\right) \nabla_{\mathbf{w}} \hat{Q}(s, a, \mathbf{w})
            $$
            
    
    😍 为什么需要固定目标？
    一开始更新的时候，对于Q的估计和Q的目标每一步都在变化。如果我们希望Q的估计可以逼近Q的目标，那么一个很好的方式就是固定Q的目标，让对Q的估计更精准。
    