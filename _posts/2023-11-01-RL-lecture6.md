---
layout: post
title: 第六章 Policy Optimization: State of the art (策略优化进阶篇)
date: 2023-11-01
description: PO SOTA
tags: intro
categories: Reinforcement-Learning
giscus_comments: true
related_posts: true
toc:
  beginning: true
---

> 上次介绍了策略优化的基础方法，本次介绍一些SOTA的策略优化方法
关于PG算法，有很多变种，这次会介绍。此外，策略优化的相关工作主要有两条线，
1. PG → Natural PG / TRPO → ACKTR → PPO
2. Q-learning → DDPG → TD3 → SAC
> 

# 一、策略梯度算法变种概览

1. 策略函数有很多种形式:
    
    $$
    \begin{aligned}\nabla_\theta J(\theta) & =\mathbb{E}_{\pi_\theta}\left[\nabla_\theta \log \pi_\theta(s, a) G_t\right] \text { - REINFORCE } \\& =\mathbb{E}_{\pi_\theta}\left[\nabla_\theta \log \pi_\theta(s, a) Q^w(s, a)\right] \text { - Q Actor-Critic } \\& =\mathbb{E}_{\pi_\theta}\left[\nabla_\theta \log \pi_\theta(s, a) A^w(s, a)\right] \text { - Advantage Actor-Critic } \\& =\mathbb{E}_{\pi_\theta}\left[\nabla_\theta \log \pi_\theta(s, a) \delta\right] \text { - TD Actor-Critic }\end{aligned}
    $$
    
2. 评论员用策略评估（MC或TD）去估计$$Q^\pi(s, a), A^\pi(s, a), \text { or } V^\pi(s)$$
3. 两条线的工作：
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% include figure.html path="assets/img/RL_chapter6/Untitled.png" class="rounded z-depth-1" style="height: 200px;" %}
    </div>
</div>
    

# 二、策略优化路线：PG → Natural PG / TRPO → ACKTR → PPO

## 2.1 策略梯度的问题

1. 采样效率很低：是on-policy learning
    
    $$
    \nabla_\theta J(\theta)=\mathbb{E}_{a \sim \pi_\theta}\left[\nabla_\theta \log \pi_\theta(s, a) r(s, a)\right]
    $$
    
2. 比较大尺度的策略更新或者不太合适的步长都会摧毁训练过程：
    1. 和监督学习不一样，数据和学习是独立的。
    2. 在RL中，如果步子太远，会得到比较差的策略，于是就会收集比较差的数据。
    3. 如果得到了比较差的策略，很难从中恢复一个好的策略，这样就会影响整体的性能。
3. 如何让训练过程更稳定？用TRPO和Natural PG
4. 如何让其训练类似一个off-policy优化？用TRPO中的重要性采样

## 2.2 Natural PG

PG方法是直接在参数空间中选择最陡的坡来优化，缺点在于这对策略函数十分敏感。

$$
d^*=\nabla_\theta J(\theta)=\lim _{\epsilon \rightarrow 0} \frac{1}{\epsilon} \arg \max J(\theta+d), \text { s.t. }\|d\| \leq \epsilon
$$

在分布空间（策略输出）中最陡的方向可以通过KL散度做约束：

$$
d^*=\arg \max J(\theta+d) \text {, s.t. } K L\left(\pi_\theta \| \pi_{\theta+d}\right)=c
$$

固定KL散度为一个常数$$c$$使得我们可以在参数空间中优化的速度是一个常数。KL散度可以衡量两个分布之间的差异：

$$
K L\left(\pi_\theta \| \pi_{\theta^{\prime}}\right)=E_{\pi_\theta}\left[\log \pi_\theta\right]-E_{\pi_\theta}\left[\log \pi_{\theta^{\prime}}\right]
$$

尽管KL散度是非对称的并且不是真正的metric，我们也可以适用。因为如果$$d\to 0$$，KL散度是渐进对称的。因此，在邻域内，KL散度是近似对称的。可以证明KL散度的二阶泰勒展开是：

$$
K L\left(\pi_\theta \| \pi_{\theta+d}\right) \approx \frac{1}{2} d^T F d
$$

其中**F**是**Fisher Information Matrix，**是KL散度$$E_{\pi_\theta}\left[\nabla \log \pi_\theta \nabla \log \pi_\theta^T\right]$$的二阶导。

我们用拉格朗日乘子重新写一下上述形式，然后再借助泰勒展开近似：

$$
d^*=\argmax_d J(\theta +d)-\lambda (KL(\pi)\theta \| \pi_{\theta+d})-c)\\ \approx \argmax_d J(\theta) + \nabla_\theta J(\theta)^Td - \frac12 \lambda d^TFd + \lambda c
$$

求导我们就能得到 natural policy gradient: $$d=\frac1\lambda F^{-1}\nabla_\theta J(\theta)$$

### NPG的性质

1. second-order优化，更精确，而且和模型无关：
    
    $$
    \theta_{t+1}=\theta_t + \alpha F^{-1} \nabla_\theta J(\theta)
    $$
    
    其中$$F=E_{\pi_\theta}\left[\nabla \log \pi_\theta \nabla \log \pi_\theta^T\right]$$是fisher information matrix，衡量了policy distribution的曲率
    
2. 不管模型怎么参数化，NPG都会产生一样的策略变化，因为$$F$$是固定的。

### 重要性采样

我们可以通过重要性采样，把PG变成一个off-policy的学习，有点类似于ELBO推导。

Importance sampling (IS) 计算$$f(x), x \sim p(x)$$的期望，如果我们不知道$$p$$，可以通过另一个分布$$q$$来做采样，

$$
\mathbb{E}_{x\sim p}[f(x)]=\int p(x)f(x)dx = \int q(x)\frac{p(x)}{q(x)}f(x)dx = \mathbb{E}_{x\sim q}[\frac{p(x)}{q(x)}f(x)]
$$

这样我们可以用IS来获得我们的目标函数，其中$$\hat\pi$$就是行为策略。

$$
J(\theta)=\mathbb{E}_{a\sim \pi_\theta}[r(s,a)]=\mathbb{E}_{a\sim \hat{\pi}}[\frac{\pi_\theta(s,a)}{\hat{\pi}(s,a)}r(s,a)]
$$

借助了IS的思想，我们准备直接用old policy作为行为策略，因此我们可以定义一个objective function为：

$$
\theta = \argmax_\theta J_{\theta_{old}}(\theta)=\argmax \mathbb{E}_t [\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}R_t]
$$

存在一个问题：如果$$\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$$太大，这个objective function的值也会非常大。所以有没有办法限制一下这个比值呢？比如，把这两个policy的差异变得小一些。例如，用KL divergence去度量这个距离

$$
K L\left(\pi_{\theta_{\text {old }}}|| \pi_\theta\right)=-\sum_a \pi_{\theta_{\text {old }}}(a \mid s) \log \frac{\pi_\theta(a \mid s)}{\pi_{\theta_{\text {old }}}(a \mid s)}
$$

这样，我们带有trust region的objective，可以写成如下的形式：

$$
\begin{gathered}J_{\theta_{\text {old }}}(\theta)=\mathbb{E}_t\left[\frac{\pi_\theta\left(a_t \mid s_t\right)}{\pi_{\theta_{\text {old }}}\left(a_t \mid s_t\right)} R_t\right] \\\text { subject to } K L\left(\pi_{\theta_{\text {old }}}\left(. \mid s_t\right)|| \pi_\theta\left(. \mid s_t\right)\right) \leq \delta\end{gathered}
$$

在 trust region 中，我们把参数搜索固定在一个范围内.经过一些推导和泰勒展开近似，我们有：

$$
\begin{aligned}
J_{\theta_t}(\theta) & \approx g^T\left(\theta-\theta_t\right) \\
K L\left(\theta_t \| \theta\right) & \approx \frac{1}{2}\left(\theta-\theta_t\right)^T H\left(\theta-\theta_t\right)
\end{aligned}
$$

其中 $$g=\nabla_\theta J_{\theta_t}(\theta),H=\nabla_\theta^2 K L\left(\theta_t \| \theta\right)$$ 其中 $$\theta_t$$ 是old policy parameter，因此：

$$
\theta_{t+1}=\underset{\theta}{\arg \max } g^T\left(\theta-\theta_t\right) \text { s.t. } \frac{1}{2}\left(\theta-\theta_t\right)^T H\left(\theta-\theta_t\right) \leq \delta
$$

可以求出解析解：

$$

\theta_{t+1}=\theta_t+\sqrt{\frac{2 \delta}{g^T H^{-1} g}} H^{-1} g
$$

- NG是fisher information matrix $$F$$下最陡峭的上山方向。
    
    $$
    H=\nabla_\theta^2 K L\left(\pi_{\theta_t}|| \pi_\theta\right)=E_{a, s \sim \pi_{\theta_t}}\left[\nabla_\theta \log \pi_\theta(a, s) \nabla_\theta \log \pi_\theta(a, s)^T\right]
    $$
    
- 学习率$$\delta$$可以被看作是选择一个normalized step size来改变policy
- 亮点：任何参数更新不会影响policy network的输出。

### TRPO中的natural policy gradient

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% include figure.html path="assets/img/RL_chapter6/Untitled 1.png" class="rounded z-depth-1" style="height: 200px;" %}
    </div>
</div>
## 2.3 TRPO

有一些问题：

1. FIM和逆计算是开销非常大
2. TRPO通过解一个线性方程来解决这个问题。即通过估解$$Hx=g$$来解$$x=H^{-1}g$$.那么可以等价于优化一个二次方程：$$\min _x \frac{1}{2} x^T H x-g^T x$$
3. 一些解释：解决 $$Ax=b$$等价于
    
    $$
    \begin{gathered}x=\underset{x}{\arg \max } f(x)=\frac{1}{2} x^T A x-b^T x \\\text { since } f^{\prime}(x)=A x-b=0\end{gathered}
    $$
    
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% include figure.html path="assets/img/RL_chapter6/Untitled 2.png" class="rounded z-depth-1" style="height: 200px;" %}
    </div>
</div>
    
    用的解法是 **conjugate gradient (CG)**
    
4. 其实TRPO算法有一点类似于EM算法，是一类Minorize-Maximization(MM)算法，解法都是先maximize一个邻近的函数然后逼近这个local expected reward
5. TRPO的问题：
    1. 计算FIM开销太大
    2. 计算精确FIM需要很多次采样
    3. CG算法实现起来比较困难

## 2.4 ACKTR

用 **Kronecker-factored approximation (K-FAC)** 来改进TRPO，具体而言就是降低计算FIM求逆的复杂度

$$
F=E_{x \sim \pi_{\theta_t}}\left[\left(\nabla_\theta \log \pi_\theta(x)\right)^T\left(\nabla_\theta \log \pi_\theta(x)\right)\right]
$$

把这个替换为layer-wise calculation，因为是对称的，所以可以分块矩阵计算

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% include figure.html path="assets/img/RL_chapter6/Untitled 3.png" class="rounded z-depth-1" style="height: 200px;" %}
    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% include figure.html path="assets/img/RL_chapter6/Untitled 4.png" class="rounded z-depth-1" style="height: 200px;" %}
    </div>
</div>
## 2.5 PPO

### 基础版PPO

其实就是把TRPO的loss改写了一下，变成拉格朗日的形式：

$$
\max \mathbb{E}_t\left[\frac{\pi_\theta\left(a_t \mid s_t\right)}{\pi_{\theta_{\text {old }}}\left(a_t \mid s_t\right)} R_t\right] \\\text { subject to } K L\left(\pi_{\theta_{\text {old }}}\left(. \mid s_t\right)|| \pi_\theta\left(. \mid s_t\right)\right) \leq \delta
$$

改为：

$$
\max \mathbb{E}_t\left[\frac{\pi_\theta\left(a_t \mid s_t\right)}{\pi_{\theta_{\text {old }}}\left(a_t \mid s_t\right)} R_t\right] -\beta K L\left(\pi_{\theta_{\text {old }}}\left(. \mid s_t\right)|| \pi_\theta\left(. \mid s_t\right)\right) 
$$

这样的好处就是可以直接用SGD来优化，速度比second order快很多。

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% include figure.html path="assets/img/RL_chapter6/Untitled 5.png" class="rounded z-depth-1" style="height: 200px;" %}
    </div>
</div>
### 带clipping的PPO

定义概率比值$$r_t(\theta)=\frac{\pi_\theta\left(a_t \mid s_t\right)}{\pi_{\theta_{\text {old }}}\left(a_t \mid s_t\right)}$$，那么我们有不同的objectives

- 不带trust region的PG: $$L_t(\theta)=r_t(\theta) \hat{A_t}$$
- KL限制：$$L_t(\theta)=r_t(\theta) \hat{A_t}.\text{s.t.}, KL[\pi_{\theta_{old}},\pi_\theta]\leq \delta$$
- KL惩罚：$$L_t(\theta)=r_t(\theta) \hat{A_t}- \beta KL[\pi_{\theta_{old}},\pi_\theta]$$
- (new) 限制policy不要和old policy离开太远：$$L_t(\theta)=\min (r_t(\theta) \hat{A_t},\text{clip}(r_t(\theta),1-\epsilon,1+\epsilon)\hat{A_t})$$

可以看作是一个正则化项：

- 当advantage为正时，鼓励action增加→ $$L_t(\theta)=\min (r_t(\theta),1+\epsilon)\hat{A_t}$$
- 当advantage为正时，鼓励action减少→$$L_t(\theta)=\min (r_t(\theta),1-\epsilon)\hat{A_t}$$

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% include figure.html path="assets/img/RL_chapter6/Untitled 6.png" class="rounded z-depth-1" style="height: 200px;" %}
    </div>
</div>
PPO比起TRPO稳定性可靠性更好，而且实现起来更简单。

> 总结一下SOTA的policy优化：
> 
> 
> <div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% include figure.html path="assets/img/RL_chapter6/Untitled 7.png" class="rounded z-depth-1" style="height: 200px;" %}
    </div>
</div>
> 

# 三、价值优化路线：Q-learning → DDPG → TD3 → SAC

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% include figure.html path="assets/img/RL_chapter6/Untitled 8.png" class="rounded z-depth-1" style="height: 200px;" %}
    </div>
</div>

## 3.1 **Deep Deterministic Policy Gradient (DDPG)**

原始的DQN是做离散动作的，我们可以把这个拓展到连续空间么？DDPG就可以看作是一个连续版本的DQN.

$$
\textbf{DQN}: a^*=\argmax_a Q^*(s,a)\\ \textbf{DDPG}: a^*=\argmax_a Q^*(s,a) \approx Q_\phi (s,\mu_\theta(s))
$$

- $$\mu_\theta(s)$$是一个deterministic的策略，直接给出一个最大化$$Q_\phi (s,\mu_\theta(s))$$的action
- 动作$$a$$是连续的
- 我们假设Q-function 对$$a$$是可导的

因此，DDPG有如下objective：

$$
\textbf{Q-target}:y(r,s',d)=r+\gamma(1-d)Q_{\phi_{targ}} (s',\mu_{\theta_{targ}}(s'))\\
\textbf{Q-function}:\min \mathbb{E}_{s,r,s',d\sim D}[Q_\phi(s,a)-y(r,s',d)]\\
\textbf{policy}: \max_\theta \mathbb{E}_{s\sim D}[Q_\phi (s,\mu_\theta(s))]
$$

同样，DDPG也用了reply buffer和target network

## 3.2 Twin Delayed DDPG (TD3)

DDPG的缺点：有严重的过拟合，为此TD3做了三个改进：

1. **Clipped Double-Q Learning**：TD3学习两个Q函数，用两个Q value里面小的那一个来得到target
2. **“Delayed” Policy Updates：**TD3更新策略缓慢，更新Q函数快速（两个Q函数更新一次policy再更新一次）
3. ********Target Policy Smoothing：********TD3在target action里面增加噪声，让policy更难利用Q函数。

具体而言，TD3学习两个Q函数$$Q_{\phi_1},Q_{\phi_2}$$，两个函数用一个target如下：

$$
y(r,s',d)=r+\gamma(1-d)\min_{i=1,2}Q_{\phi_{i,targ}}(s'a_{TD3}(s'))
$$

Target Policy Smoothing（类似于正则化）

$$
a_{TD3}(s')=clip(\mu_{\theta,targ}(s'))+clip(\epsilon,-c,c),a_{low},a_{high}),\epsilon\sim N(0,\sigma)
$$

## 3.3 **Soft Actor-Critic (SAC)**

SAC是用类似于DDPG的方法，进行stochastic policy的优化，融合了 **entropy regularization** （熵正则化）。具体而言，这个策略被训练来最大化 expected return和entropy之间的trade-of

$$
\pi^*=\argmax \mathbb{E}_{\tau\sim \pi} [\sum_t \gamma^t(R(s_t,a_t,s_{t+1})+\alpha H(\pi(\cdot | s_t)))]
$$

value function也包含了entropy bonus

$$
V^\pi(s)=\mathbb{E}_{\tau\sim \pi}[\sum_t \gamma^t (R(s_t,a_t,s_{t+1})+\alpha H(\pi(\cdot|s_t)))|s_0=s]
$$

我们可以推导出类似的bellman equation：

$$
Q^\pi(s,a)=\mathbb{E}_{s'\sim P,a'\sim \pi}[R(s,a,s')+\gamma(Q^\pi(s',a')+\alpha H(\pi(a'|s')))]\\ =\mathbb{E}_{s'\sim P,a'\sim \pi}[R(s,a,s')+\gamma(Q^\pi(s',a')-\alpha \log\pi(a'|s'))]
$$

因此Q函数的更新可以写成：

$$
Q^\pi(s,a)\leftarrow r+\gamma(Q^\pi(s',\hat{a}')-\alpha \log \pi(\hat{a}'|s')),\hat{a}'\sim \pi(\cdot|s')
$$

类似TD3，也是学习两个Q函数，用clipped double-Q trick。两个Q函数都是通过mean sqaure bellman error学习的：

$$
L(\phi_i,D)=\mathbb{E}[(Q_\phi(s,a)-y(r,s',d))^2]\\
y(r,s',d)=r+\gamma(1-d)(\min_{j=1,2}Q_{\phi_{targ,j}}(s',\hat{a}'-\alpha \log\pi_\theta(\hat{a}'|s'))),\\
\hat{a}'\sim \pi_{\theta}(\cdot|s')
$$

policy通过maximize$$V^\pi(s)$$来学习。

### 重参数化

接下来我们把action当作是从一个gaussian分布里面采样得到的，

$$
\hat{a}_\theta(s,\epsilon)=tanh(\mu_\theta(s)+\sigma_\theta(s)\odot\epsilon),\epsilon\sim N(0,I)
$$

重参数化的好处：可以让我们把对action的期望转换为一个对noise的期望——和参数无关的分布，即

$$
\mathbb{E}_{a\sim\pi_\theta}[Q^\pi_\theta(s,a)-\alpha \log \pi_\theta(a|s)]\\=\mathbb{E}_{\epsilon\sim N}[Q^\pi_\theta(s,\hat{a}_\theta(s,\epsilon))-\alpha \log \pi_\theta(\hat{a}_\theta(s,\epsilon)|s)]
$$

因此，policy优化公式可以写为：

$$
\max_{\theta}\mathbb{E}_{s\sim D,\epsilon\sim N}[\min_{j=1,2}Q_{\phi_j}(s,\hat{a}_\theta(s,\epsilon))-\alpha \log \pi_\theta(\hat{a}_\theta(s,\epsilon)|s)]
$$

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% include figure.html path="assets/img/RL_chapter6/Untitled 9.png" class="rounded z-depth-1" style="height: 200px;" %}
    </div>
</div>