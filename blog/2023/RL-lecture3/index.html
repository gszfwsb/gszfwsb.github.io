<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>第三章 Tabular methods Model-free Prediction and Control (表格型方法：免模型的预测和控制) | Shaobo Wang</title> <meta name="author" content="Steven Shaobo Wang"> <meta name="description" content="Tabular methods"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%91%A8%F0%9F%8F%BB%E2%80%8D%F0%9F%8E%93&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://gszfwsb.github.io/blog/2023/RL-lecture3/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="//">Shaobo Wang</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">第三章 Tabular methods Model-free Prediction and Control (表格型方法：免模型的预测和控制)</h1> <p class="post-meta">October 30, 2023</p> <p class="post-tags"> <a href="//blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="//blog/tag/intro"> <i class="fas fa-hashtag fa-sm"></i> intro</a>     ·   <a href="//blog/category/reinforcement-learning"> <i class="fas fa-tag fa-sm"></i> Reinforcement-Learning</a>   </p> </header> <article class="post-content"> <div id="table-of-contents"> <ul id="toc" class="section-nav"> <li class="toc-entry toc-h1"><a href="#%E4%B8%80%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81%E5%85%8D%E6%A8%A1%E5%9E%8Brl">一、为什么需要免模型RL？</a></li> <li class="toc-entry toc-h1"> <a href="#%E4%BA%8C%E5%85%8D%E6%A8%A1%E5%9E%8B%E9%A2%84%E6%B5%8B">二、免模型预测</a> <ul> <li class="toc-entry toc-h2"> <a href="#21-%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%96%B9%E6%B3%95">2.1 蒙特卡洛方法</a> <ul> <li class="toc-entry toc-h3"><a href="#211-%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E7%AD%96%E7%95%A5%E8%AF%84%E4%BC%B0">2.1.1 蒙特卡洛策略评估</a></li> <li class="toc-entry toc-h3"><a href="#212-%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%96%B9%E6%B3%95%E5%92%8C%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E6%96%B9%E6%B3%95">2.1.2 蒙特卡洛方法和动态规划方法</a></li> </ul> </li> <li class="toc-entry toc-h2"><a href="#22-%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86%E6%96%B9%E6%B3%95">2.2 时序差分方法</a></li> <li class="toc-entry toc-h2"> <a href="#23-%E9%87%87%E6%A0%B7%E5%92%8Cbootrap">2.3 采样和bootrap</a> <ul> <li class="toc-entry toc-h3"><a href="#231-%E9%87%87%E6%A0%B7%E5%92%8Cbootrap%E6%83%85%E5%BD%A2">2.3.1 采样和bootrap情形</a></li> <li class="toc-entry toc-h3"><a href="#232-dp-mc-td">2.3.2 DP, MC, TD</a></li> </ul> </li> </ul> </li> <li class="toc-entry toc-h1"> <a href="#%E4%B8%89%E5%85%8D%E6%A8%A1%E5%9E%8B%E6%8E%A7%E5%88%B6">三、免模型控制</a> <ul> <li class="toc-entry toc-h2"><a href="#31-%E5%B9%BF%E4%B9%89%E7%AD%96%E7%95%A5%E8%BF%AD%E4%BB%A3">3.1 广义策略迭代</a></li> <li class="toc-entry toc-h2"> <a href="#32-%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%96%B9%E6%B3%95">3.2 蒙特卡洛方法</a> <ul> <li class="toc-entry toc-h3"><a href="#321-%E6%8E%A2%E7%B4%A2%E6%80%A7%E5%BC%80%E5%A7%8B%E7%9A%84mc">3.2.1 探索性开始的MC</a></li> <li class="toc-entry toc-h3"><a href="#322-%E5%B8%A6%E6%9C%89varepsilon-greedy%E7%9A%84%E8%B4%AA%E5%BF%83%E7%AD%96%E7%95%A5">3.2.2 带有\(\varepsilon\)-greedy的贪心策略</a></li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#33-%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86%E6%96%B9%E6%B3%95">3.3 时序差分方法</a> <ul> <li class="toc-entry toc-h3"><a href="#331-sarsa-on-policy-td">3.3.1 Sarsa: on-policy TD</a></li> <li class="toc-entry toc-h3"><a href="#332-q-learning-off-policy-td">3.3.2 Q-learning: off-policy TD</a></li> <li class="toc-entry toc-h3"><a href="#333-on-policy%E5%92%8Coff-policy%E7%9A%84%E5%8C%BA%E5%88%AB">3.3.3 on-policy和off-policy的区别</a></li> </ul> </li> </ul> </li> </ul> </div> <hr> <div id="markdown-content"> <h1 id="一为什么需要免模型rl">一、为什么需要免模型RL？</h1> <ol> <li> <p>什么时候马尔可夫决策过程是已知的？</p> <p><strong>奖励和转移概率均已知，</strong>这样才可以用策略迭代和价值迭代进行求解。</p> </li> <li>策略迭代：给定一个已知的MDP，计算最优策略函数和价值函数。 <ol> <li> <p>策略评估：用贝尔曼期望方程迭代到收敛</p> \[V_{t+1}(s)=\sum_{a \in \mathcal{A}} \pi(a \mid s)(\textcolor{red}{R(s, a)}+\gamma \sum_{s^{\prime} \in \mathcal{S}} \textcolor{red}{P(s^{\prime} \mid s, a)} V_{t}(s^{\prime})\] </li> <li> <p>策略改进：用贝尔曼期望方程，并在价值函数上用贪心策略</p> \[Q_{\pi_i}(s, a)=\textcolor{red}{R(s, a)}+\gamma \sum_{s^{\prime} \in S} \textcolor{red}{P\left(s^{\prime} \mid s, a\right)} V_{\pi_i}\left(s^{\prime}\right)\\\pi_{i+1}(s)=\underset{a}{\arg \max } Q_{\pi_i}(s, a)\] </li> </ol> </li> <li>价值迭代：给定一个已知的MDP，计算最优价值函数。 <ol> <li> <p>用贝尔曼最优公式迭代</p> \[v_{i+1}(s)\leftarrow \max_{a\in \mathcal{A}}(\textcolor{red}{R(s, a)}+\gamma \sum_{s^{\prime} \in \mathcal{S}} \textcolor{red}{P(s^{\prime} \mid s, a)} v_{i}(s^{\prime})）\] </li> <li> <p>得到迭代后的最优策略：</p> \[\pi^*(s)=\underset{a}{\arg \max }\left[\textcolor{red}{R(s, a)}+\gamma \sum_{s^{\prime} \in S} \textcolor{red}{P\left(s^{\prime} \mid s, a\right)} V_{\textrm{end}}\left(s^{\prime}\right)\right]\] </li> </ol> </li> <li>知道世界如何运作的RL： <ol> <li>策略迭代和价值迭代都需要假设我们已知环境中的<strong>状态转移</strong>和<strong>奖励</strong>。</li> <li>现实情境中，MDP模型要么就是未知要么就是太大太复杂</li> </ol> </li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter3/Untitled-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter3/Untitled-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter3/Untitled-1400.webp"></source> <img src="/assets/img/RL_chapter3/Untitled.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ol> <li> <p>免模型RL：通过与环境交互交互学习</p> <p>马尔可夫决策过程的模型未知或者模型很大时，我们可以使用免模型强化学习的方法。免模型强化学习方法没有获取环境的<strong>状态转移</strong>和<strong>奖励函数</strong>，而是让智能体与环境进行<strong>交互</strong>，采集大量的<strong>轨迹数据</strong>，智能体从轨迹中获取信息来改进策略，从而获得更多的奖励。</p> <p>轨迹：包括\(\{S_1,A_1,R_1,S_2,A_2,R_2,...,S_T,A_T,R_T\}\)</p> </li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter3/Untitled%201-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter3/Untitled%201-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter3/Untitled%201-1400.webp"></source> <img src="/assets/img/RL_chapter3/Untitled%201.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h1 id="二免模型预测">二、免模型预测</h1> <p>在无法获取马尔可夫决策过程的模型情况下，我们可以通过<strong>蒙特卡洛方法</strong>和<strong>时序差分方法</strong>来估计某个给定策略的价值。</p> <h2 id="21-蒙特卡洛方法">2.1 蒙特卡洛方法</h2> <h3 id="211-蒙特卡洛策略评估">2.1.1 蒙特卡洛策略评估</h3> <p>蒙特卡洛方法是基于采样的方法，给定策略 \(\pi\) ，我们让智能体与环境进行交互，可以得到很多轨迹。每个轨迹都有对应的回报：</p> \[G_t=r_{t+1}+\gamma r_{t+2}+\gamma^2 r_{t+3}+\ldots \] <p>我们求出所有轨迹的回报的平均值，就可以知道某一个策略对应状态的价值，即</p> \[V_\pi(s)=\mathbb{E}_{\tau \sim \pi}\left[G_t \mid s_t=s\right]\] <ol> <li>特点： <ul> <li>蒙特卡洛仿真是指我们可以采样大量的<strong>轨迹</strong>\(\tau\)（从策略\(\pi\)采样），计算所有轨迹\(\tau\)的真实回报，然后计算平均值。</li> <li>蒙特卡洛方法使用<strong>经验平均回报</strong>（empirical mean return) 的方法来估计，它不需要马尔可夫决策过程的状态转移函数和奖励函数，并且不需要像动态规划那样用bootstrap的方法。</li> <li>此外，蒙特卡洛方法有一定的局限性，它只能用在有终止的马尔可夫决策过程中。</li> </ul> </li> <li>为了得到评估 \(V(s)\) ，我们采取了如下的步骤。 <ol> <li>在每个回合中，如果在时间步 \(t\) 状态 \(s\) 被访问了，那么 <ul> <li>状态 \(s\) 的访问数 \(N(s)\) 增加 \(1 ， N(s) \leftarrow N(s)+1\) 。</li> <li>状态 \(s\) 的总的回报 \(S(s)\) 增加 \(G_t, S(s) \leftarrow S(s)+G_{t}\)</li> <li>状态 \(s\) 的价值可以通过回报的平均来估计，即 \(V(s)=S(s) / N(s)\) 。</li> </ul> </li> <li>根据大数定律，只要我们得到足够多的轨迹，就可以趋近这个策略对应的价值函数。当 \(N(s) \rightarrow \infty\)时， \(V(s) \rightarrow V_\pi(s)\) 。</li> </ol> </li> <li> <p>具体更新时，我们可以把<strong>经验均值</strong>（empirical mean）转换成<strong>增量均值</strong>（incremental mean）的形式：</p> \[\begin{aligned}\mu_t &amp; =\frac{1}{t} \sum_{j=1}^t x_j \\&amp; =\frac{1}{t}\left(x_t+\sum_{j=1}^{t-1} x_j\right) \\&amp; =\frac{1}{t}\left(x_t+(t-1) \mu_{t-1}\right) \\&amp; =\mu_{t-1}+\frac{1}{t}\left(x_t-\mu_{t-1}\right)\end{aligned}\] </li> <li>增量均值形式的MC算法： <ul> <li>采样一轮游戏\(\left(S_1, A_1, R_1, \ldots, S_t\right)\)</li> <li> <p>对每一个状态\(S_t\)和回报\(G_t\)</p> \[\begin{aligned} &amp; N\left(S_t\right) \leftarrow N\left(S_t\right)+1 \\ &amp; v\left(S_t\right) \leftarrow v\left(S_t\right)+\frac{1}{N\left(S_t\right)}\left(G_t-v\left(S_t\right)\right) \end{aligned}\] </li> <li> <p>或者可以用running mean. 对于non-stationary问题有好处，\(\alpha\)是learning rate</p> \[v\left(S_t\right) \leftarrow v\left(S_t\right)+\alpha\left(G_t-v\left(S_t\right)\right) \] </li> </ul> </li> </ol> <h3 id="212-蒙特卡洛方法和动态规划方法">2.1.2 蒙特卡洛方法和动态规划方法</h3> <ol> <li> <p>在动态规划方法里面，我们使用了bootstrap的思想。bootstrap就是我们基于之前估计的量来估计一个量。此外，动态规划方法使用贝尔曼期望备份（Bellman expectation backup），通过上一时刻的\(V_{t}\)更新这一时刻的\(V_{t+1}\).</p> \[V_{t+1}(s) \leftarrow \sum_{a \in \mathcal{A}} \pi(a \mid s)\left(R(s, a)+\gamma \sum_{s^{\prime} \in \mathcal{S}} P\left(s^{\prime} \mid s, a\right) V_{t}\left(s^{\prime}\right)\right)\] <p>将其不停迭代，最后可以收敛。如图所示，贝尔曼期望公式有两层加和，即内部加和和外部加和，需要知道所有的状态和转移矩阵（model）计算两次期望，得到一个更新。</p> </li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter3/Untitled%202-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter3/Untitled%202-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter3/Untitled%202-1400.webp"></source> <img src="/assets/img/RL_chapter3/Untitled%202.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ol> <li> <p>蒙特卡洛方法通过一个回合的经验平均回报（实际得到的奖励）来进行更新，即</p> \[V\left(s_t\right) \leftarrow V\left(s_t\right)+\alpha\left(G_{i, t}-V\left(s_t\right)\right)\] </li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter3/Untitled%203-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter3/Untitled%203-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter3/Untitled%203-1400.webp"></source> <img src="/assets/img/RL_chapter3/Untitled%203.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>如图，我们使用蒙特卡洛方法得到的轨迹对应树上蓝色的轨迹，轨迹上的状态已经是决定的，采取的动作也是已经决定的。我们现在只更新这条轨迹上的所有状态，与这条轨迹没有关系的状态都不进行更新。
</code></pre></div></div> <ol> <li> <strong>蒙特卡洛方法</strong>相比<strong>动态规划方法</strong>是有一些优势的。 <ol> <li>蒙特卡洛方法适用于<strong>环境未知</strong>的情况，而动态规划是<strong>有模型</strong>的方法（需要转移矩阵）。</li> <li>即使知道了环境的全部信息，计算转移概率往往也是很复杂的。</li> <li>蒙特卡洛方法只需要更新<strong>一条轨迹</strong>的状态，而动态规划方法需要更新<strong>所有的状态</strong>。状态数量很多的时候（比如100万个、200万个），我们使用动态规划方法进行迭代，速度是非常慢。</li> </ol> </li> </ol> <h2 id="22-时序差分方法">2.2 时序差分方法</h2> <p>时序差分是介于蒙特卡洛和动态规划之间的方法，它是免模型的，不需要马尔可夫决策过程的转移矩阵和奖励函数。 此外，时序差分方法可以从不完整的回合中学习，并且结合了bootstrap的思想。</p> <ol> <li>特点： <ol> <li>直接从轨迹中学习</li> <li>免模型：不需要MDP的转移概率和奖励</li> <li>通过bootstrap从<strong>不完整</strong>的轨迹中学习</li> </ol> </li> <li>时序差分方法的目的是对于某个给定的策略\(\pi\)，在线（online）地算出它的价值函数\(V_\pi\)，即一步一步地（step-by-step）算。 <ol> <li> <p>最简单的算法是一步时序差分（one-step TD），TD(0)：</p> \[V\left(s_t\right) \leftarrow V\left(s_t\right)+\alpha\left[\textcolor{red}{R_t+\gamma V\left(s_{t+1}\right)}-V\left(s_t\right)\right]\] </li> <li>\(R_{t+1}+\gamma V\left(S_{t+1}\right)\) 是TD目标</li> <li>\(\delta_t=R_{t+1}+\gamma V\left(S_{t+1}\right)-V\left(S_t\right)\)是TD error</li> </ol> <p>😍 MC和TD： 类比增量式蒙特卡洛方法，给定一个回合\(i\)，我们可以更新 \(V(s_t)\) 来逼近真实的回报，具体更新公式为:</p> \[V\left(s_t\right) \leftarrow V\left(s_t\right)+\alpha\left(\textcolor{red}{G_{i, t}}-V\left(s_t\right)\right)\] <p>回顾贝尔曼期望方程便知道原因：</p> \[\begin{aligned}V_\pi(s) &amp; =\mathbb{E}_\pi\left[G_t \mid S_t=s\right] \\&amp; =\mathbb{E}_\pi\left[\sum_{k=0}^{\infty} \gamma^k R_{t+k} \mid S_t=s\right] \\&amp; =\mathbb{E}_\pi\left[R_t+\gamma \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \mid S_t=s\right] \\&amp; =\mathbb{E}_\pi\left[R_t+\gamma V_\pi\left(S_{t+1}\right) \mid S_t=s\right]\end{aligned}\] <p><strong>蒙特卡洛方法</strong>将上式<strong>第一行</strong>作为更新的目标，而<strong>时序差分算法</strong>将上式<strong>最后一行</strong>作为更新的目标。于是，在用策略和环境交互时，每采样一步，我们就可以用时序差分算法来更新状态价值估计。时序差分算法用到了\(V(s_{t+1})\)的估计值，可以证明它最终收敛到策略\(\pi\) 的价值函数。</p> </li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter3/Untitled%204-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter3/Untitled%204-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter3/Untitled%204-1400.webp"></source> <img src="/assets/img/RL_chapter3/Untitled%204.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter3/Untitled%205-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter3/Untitled%205-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter3/Untitled%205-1400.webp"></source> <img src="/assets/img/RL_chapter3/Untitled%205.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>进一步比较时序差分方法和蒙特卡洛方法。

- 时序差分方法可以**在线学习**（online learning），每走一步就可以更新效率高。蒙特卡洛方法必须等游戏结束时才可以学习。
- 时序差分方法可以从**不完整序列**上进行学习。蒙特卡洛方法只能从**完整的序列**上进行学习。
- 时序差分方法可以在**连续的环境下**（没有终止）进行学习。蒙特卡洛方法只能在**有终止**的情况下学习。
- 时序差分方法利用了**马尔可夫性质**，在马尔可夫环境下有更高的学习效率。蒙特卡洛方法没有假设环境具有马尔可夫性质，利用采样的价值来估计某个状态的价值，在不是马尔可夫的环境下更加有效。
</code></pre></div></div> <ol> <li>多步时序差分（n-step TD）：之前是只往前走一步，即TD(0)。 我们可以调整步数（step），变成<strong><em>n</em>步时序差分。</strong> </li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter3/Untitled%206-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter3/Untitled%206-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter3/Untitled%206-1400.webp"></source> <img src="/assets/img/RL_chapter3/Untitled%206.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>这样我们就可以通过步数来调整算法需要的实际奖励和bootstrap。

$$
\begin{aligned}  n=1(\mathrm{TD}) \quad &amp; G_t^{(1)}=r_{t+1}+\gamma V\left(s_{t+1}\right) \\ n=2(\mathrm{TD(2)})) \quad &amp; G_t^{(2)}=r_{t+1}+\gamma r_{t+2}+\gamma^2 V\left(s_{t+2}\right) \\ \ldots \\ n=\infty(\mathrm{MC}) \quad &amp; G_t^{\infty} \begin{array}{l}=r_{t+1}+\gamma r_{t+2}+\ldots+\gamma^{T-t-1} r_T\end{array}\end{aligned}
$$

通过调整步数，可以进行蒙特卡洛方法和时序差分方法之间的权衡。如果$$n=\infty$$， 即整个游戏结束后，再进行更新，时序差分方法就变成了蒙特卡洛方法。*n*步时序差分可写为:

$$
G_t^n=r_{t+1}+\gamma r_{t+2}+\ldots+\gamma^{n-1} r_{t+n}+\gamma^n V\left(s_{t+n}\right)
$$

得到时序差分目标之后，我们用增量式学习 (incremental learning) 的方法来更新状态的价值:

$$
V\left(s_t\right) \leftarrow V\left(s_t\right)+\alpha\left(G_t^n-V\left(s_t\right)\right) 
$$
</code></pre></div></div> <h2 id="23-采样和bootrap">2.3 采样和bootrap</h2> <p>动态规划方法、蒙特卡洛方法以及时序差分方法的bootstrap和采样有什么联系和区别呢？</p> <h3 id="231-采样和bootrap情形">2.3.1 采样和bootrap情形</h3> <ol> <li> <strong>Bootstrap是指更新时使用了估计</strong>。 <ol> <li>蒙特卡洛方法没有使用bootstrap，因为它根据实际的回报进行更新。</li> <li>动态规划方法和时序差分方法使用了bootstrap。</li> </ol> </li> <li> <strong>采样是指更新时通过采样得到一个期望</strong>。 <ol> <li>蒙特卡洛方法是纯采样的方法。</li> <li>动态规划方法没有使用采样，它是直接用贝尔曼期望方程来更新状态价值的。</li> <li>时序差分方法使用了采样。时序差分目标由两部分组成，一部分是采样，一部分是bootstrap。</li> </ol> </li> </ol> <h3 id="232-dp-mc-td">2.3.2 DP, MC, TD</h3> <ol> <li> <p>动态规划方法直接计算期望，它把所有相关的状态都进行加和，即</p> \[V\left(s_t\right) \leftarrow \mathbb{E}_\pi\left[r_{t+1}+\gamma V\left(s_{t+1}\right)\right]\] </li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter3/Untitled%207-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter3/Untitled%207-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter3/Untitled%207-1400.webp"></source> <img src="/assets/img/RL_chapter3/Untitled%207.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ol> <li> <p>蒙特卡洛方法在当前状态下，采取一条支路，在这条路径上进行更新，更新这条路径上的所有状态，即</p> \[V\left(s_t\right) \leftarrow V\left(s_t\right)+\alpha\left(G_t-V\left(s_t\right)\right)\] </li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter3/Untitled%208-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter3/Untitled%208-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter3/Untitled%208-1400.webp"></source> <img src="/assets/img/RL_chapter3/Untitled%208.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ol> <li> <p>时序差分从当前状态开始，往前走了一步，关注的是非常局部的步骤，即</p> \[\mathrm{TD}(0): V\left(s_t\right) \leftarrow V\left(s_t\right)+\alpha\left(r_{t+1}+\gamma V\left(s_{t+1}\right)-V\left(s_t\right)\right)\] </li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter3/Untitled%209-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter3/Untitled%209-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter3/Untitled%209-1400.webp"></source> <img src="/assets/img/RL_chapter3/Untitled%209.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ol> <li>如果时序差分方法需要更广度的更新，就变成了动态规划方法（因为动态规划方法是把所有状态都考虑进去来进行更新）。如果时序差分方法需要更深度的更新，就变成了蒙特卡洛方法。图 右下角是穷举搜索的方法（exhaustive search），穷举搜索的方法不仅需要很深度的信息，还需要很广度的信息。</li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter3/Untitled%2010-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter3/Untitled%2010-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter3/Untitled%2010-1400.webp"></source> <img src="/assets/img/RL_chapter3/Untitled%2010.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h1 id="三免模型控制">三、免模型控制</h1> <h2 id="31-广义策略迭代">3.1 广义策略迭代</h2> <p>在我们不知道马尔可夫决策过程模型的情况下，如何优化价值函数，得到最佳的策略呢？我们可以把策略迭代进行广义的推广，使它能够兼容蒙特卡洛和时序差分的方法，即带有<strong>蒙特卡洛方法</strong>和<strong>时序差分</strong>方法的<strong>广义策略迭代（generalized policy iteration，GPI）</strong></p> <p>当我们不知道奖励函数和状态转移时，如何进行策略的优化？我们引入了广义的策略迭代的方法。 我们对策略评估部分进行修改，使用蒙特卡洛的方法代替动态规划的方法估计 \(Q\) 函数。我们首先进行策略评估，使用蒙特卡洛方法来估计策略 \(Q=Q_\pi\)，然后进行策略更新，即得到 \(Q\) 函数后，我们就可以通过贪心的方法去改进它：</p> \[\pi(s)=\underset{a}{\arg \max } Q(s, a)\] <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter3/Untitled%2011-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter3/Untitled%2011-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter3/Untitled%2011-1400.webp"></source> <img src="/assets/img/RL_chapter3/Untitled%2011.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h2 id="32-蒙特卡洛方法">3.2 蒙特卡洛方法</h2> <h3 id="321-探索性开始的mc">3.2.1 探索性开始的MC</h3> <p>一个保证策略迭代收敛的<strong>假设</strong>是回合有<strong>探索性开始（exploring start）</strong>。假设每一个回合都有一个<strong>探索性开始</strong>，探索性开始保证所有的状态和动作都在无限步的执行后能被采样到，这样才能很好地进行估计。 算法通过蒙特卡洛方法产生很多轨迹，每条轨迹都可以算出它的价值。然后，我们可以通过平均的方法去估计 Q 函数。Q 函数可以看成一个表格，我们通过采样的方法把表格的每个单元值都填上，然后使用策略改进来选取更好的策略。 如何用蒙特卡洛方法来填 Q 表格是这个算法的核心。</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter3/Untitled%2012-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter3/Untitled%2012-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter3/Untitled%2012-1400.webp"></source> <img src="/assets/img/RL_chapter3/Untitled%2012.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="322-带有varepsilon-greedy的贪心策略">3.2.2 带有\(\varepsilon\)-greedy的贪心策略</h3> <p>为了确保蒙特卡洛方法能够有足够的探索，我们使用了 \(\varepsilon\)-贪心探索。 \(\varepsilon\)-贪心是指我们有  \(1-\varepsilon\) 的概率会按照 Q函数来决定动作，通常 \(\varepsilon\) 就设一个很小的值， \(1-\varepsilon\) 可能是 0.9，也就是 0.9 的概率会按照Q函数来决定动作，但是我们有 0.1 的概率是随机的。通常在实现上， \(\varepsilon\) 的值会随着时间递减。在最开始的时候，因为我们还不知道哪个动作是比较好的，所以会花比较多的时间探索。接下来随着训练的次数越来越多，我们已经比较确定哪一个动作是比较好的，就会减少探索，把 \(\varepsilon\) 的值变小。主要根据 Q 函数来决定动作，比较少随机决定动作，这就是\(\varepsilon\)-贪心。</p> \[\pi(a \mid s)= \begin{cases}\epsilon /|\mathcal{A}|+1-\epsilon &amp; \text { if } a^*=\arg \max _{a \in \mathcal{A}} Q(s, a) \\ \epsilon /|\mathcal{A}| &amp; \text { otherwise }\end{cases}\] <p>当我们使用蒙特卡洛方法和\(\varepsilon\)-贪心探索的时候，可以确保价值函数是单调的、改进的。对于任何 \(\epsilon\)-贪心策略 \(\pi\)，关于 \(Q_\pi\) 的 \(\varepsilon\)-贪心策略 \(\pi'\) 都是一个改进，即 \(V_{\pi}(s)\leq V_{\pi'}(s)\)，证明过程如下：</p> \[\begin{aligned}V_\pi\left(s, \pi^{\prime}(s)\right) &amp; =\sum_{a \in A} \pi^{\prime}(a \mid s) Q_\pi(s, a) \\&amp; =\frac{\varepsilon}{|A|} \sum_{a \in A} Q_\pi(s, a)+(1-\varepsilon) \max _a Q_\pi(s, a) \\&amp; \geqslant \frac{\varepsilon}{|A|} \sum_{a \in A} Q_\pi(s, a)+(1-\varepsilon) \sum_{a \in A} \frac{\pi(a \mid s)-\frac{\varepsilon}{|A|}}{1-\varepsilon} Q_\pi(s, a) \\&amp; =\sum_{a \in A} \pi(a \mid s) Q_\pi(s, a)=V_\pi(s)\end{aligned}\] <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter3/Untitled%2013-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter3/Untitled%2013-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter3/Untitled%2013-1400.webp"></source> <img src="/assets/img/RL_chapter3/Untitled%2013.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h2 id="33-时序差分方法">3.3 时序差分方法</h2> <p>与蒙特卡洛方法相比，时序差分方法有如下几个优势：<strong>低方差，能够在线学习，能够从不完整的序列中学习。</strong> 所以我们可以把时序差分方法也放到<strong>控制循环</strong>（control loop）里面去估计Q表格，再采取\(\varepsilon\)贪心探索改进。这样就可以在回合没结束的时候更新已经采集到的状态价值。</p> <h3 id="331-sarsa-on-policy-td">3.3.1 Sarsa: on-policy TD</h3> <ol> <li> <p>Sarsa：</p> <p>时序差分方法是给定一个策略，然后我们去估计它的价值函数。接着我们要考虑怎么使用时序差分方法的框架来估计Q函数，也就是 Sarsa 算法。</p> </li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter3/Untitled%2014-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter3/Untitled%2014-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter3/Untitled%2014-1400.webp"></source> <img src="/assets/img/RL_chapter3/Untitled%2014.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Sarsa 所做出的改变很简单，它将原本时序差分方法更新 *V* 的过程，变成了更新 *Q*，即

$$
Q\left(S_t, A_t\right) \leftarrow Q\left(S_t, A_t\right)+\alpha\left[R_{t+1}+\gamma Q\left(S_{t+1}, A_{t+1}\right)-Q\left(S_t, A_t\right)\right]
$$

&gt; 注：prediction中我们更新V，control中我们更新Q
&gt; 
&gt; 
&gt; $$
&gt; V\left(s_t\right) \leftarrow V\left(s_t\right)+\alpha\left(R_{t+1}+\gamma V\left(s_{t+1}\right)-V\left(s_t\right)\right)
&gt; $$
&gt; 

TD目标：$$\delta_t=R_{t+1}+\gamma Q\left(S_{t+1}, A_{t+1}\right)$$
</code></pre></div></div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter3/Untitled%2015-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter3/Untitled%2015-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter3/Untitled%2015-1400.webp"></source> <img src="/assets/img/RL_chapter3/Untitled%2015.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ol> <li> <p>n-step Sarsa</p> <p>Sarsa 属于单步更新算法，每执行一个动作，就会更新一次价值和策略。如果不进行单步更新，而是采取\(n\)步更新或者回合更新，即在执行<em>n</em>步之后再更新价值和策略，这样我们就得到了<strong><em>n</em> 步 Sarsa（<em>n</em>-step Sarsa）:</strong></p> \[\begin{aligned}n=1(\text { Sarsa }) q_t^{(1)}= &amp; R_{t+1}+\gamma Q\left(S_{t+1}, A_{t+1}\right) \\n=2 \quad q_t^{(2)}= &amp; R_{t+1}+\gamma R_{t+2}+\gamma^2 Q\left(S_{t+2}, A_{t+2}\right) \\&amp; \vdots &amp; \\n=\infty(M C) \quad q_t^{\infty}= &amp; R_{t+1}+\gamma R_{t+2}+\ldots+\gamma^{T-t-1} R_T\end{aligned}\] <p>n-step收益为：</p> \[q_t^{(n)}=R_{t+1}+\gamma R_{t+2}+\ldots+\gamma^{n-1} R_{t+n}+\gamma^n Q\left(S_{t+n}, A_{t+n}\right)\] <p>n-step迭代更新：</p> \[Q\left(S_t, A_t\right) \leftarrow Q\left(S_t, A_t\right)+\alpha\left(q_t^{(n)}-Q\left(S_t, A_t\right)\right)\] </li> </ol> <h3 id="332-q-learning-off-policy-td">3.3.2 Q-learning: off-policy TD</h3> <ol> <li>off-policy <ol> <li>Sarsa 是一种<strong>在线策略（on-policy）</strong>算法，它优化的是它实际执行的策略，它直接用下一步会执行的动作去优化 Q 表格。<strong>在线策略</strong>在学习的过程中，只存在一种策略，它用一种策略去做动作的选取，也用一种策略去做优化。所以 Sarsa 知道它下一步的动作有可能会跑到悬崖那边去，它就会在优化自己的策略的时候，尽可能离悬崖远一点。这样子就会保证，它下一步哪怕是有随机动作，它也还是在安全区域内。</li> <li>Q学习是一种<strong>离线策略（off-policy）</strong>算法。如图所示，<strong>离线策略</strong>在学习的过程中，有两种不同的策略：<strong>目标策略（target policy）</strong>和<strong>行为策略（behavior policy）</strong>。 <ul> <li>目标策略是我们需要去学习的策略，一般用 \(\pi\) 来表示。目标策略就像是在后方指挥战术的一个军师，它可以根据自己的经验来学习最优的策略，不需要去和环境交互。</li> <li>行为策略是探索环境的策略，一般用 <em>μ</em> 来表示。行为策略可以大胆地去探索到所有可能的轨迹，采集轨迹，采集数据，然后把采集到的数据“喂”给目标策略学习。而且“喂”给目标策略的数据中并不需要 \(a_{t+1}\)，而 Sarsa 是要有 \(a_{t+1}\) 的。行为策略像是一个战士，可以在环境里面探索所有的动作、轨迹和经验，然后把这些经验交给目标策略去学习。比如目标策略优化的时候，Q学习不会管我们下一步去往哪里探索，它只选取奖励最大的策略。</li> </ul> </li> </ol> </li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter3/Untitled%2016-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter3/Untitled%2016-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter3/Untitled%2016-1400.webp"></source> <img src="/assets/img/RL_chapter3/Untitled%2016.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ol> <li>off-policy学习有很多好处。 <ol> <li>我们可以利用探索策略来学到最佳的策略，学习效率高；</li> <li>其次，off-policy学习可以让我们学习其他智能体的动作，进行模仿学习，学习人或者其他智能体产生的轨迹；</li> <li>最后，off-policy学习可以让我们重用旧的策略产生的轨迹，探索过程需要很多计算资源，这样可以节省资源。</li> </ol> </li> <li> <p>离线算法：</p> <p>Q学习有两种策略: 行为策略和目标策略。</p> <ul> <li> <p>目标策略 \(\pi\) 直接在 Q 表格上使用贪心策略，取它下一步能得到的所有状态，即</p> \[\pi\left(s_{t+1}\right)=\underset{a^{\prime}}{\arg \max } Q\left(s_{t+1}, a^{\prime}\right) \] </li> <li> <p>行为策略 \(\mu\) 可以是一个随机的策略，但我们采取 \(\varepsilon\)-贪心策略，让行为策略不至于是完全随机的，它是基于 Q 表格逐渐改进的。我们可以构造Q 学习目标， Q学习的下一个动作都是通过 argmax 操作选出来的，于是我们可得</p> \[\begin{aligned} r_{t+1}+\gamma Q\left(s_{t+1}, A^{\prime}\right) &amp; =r_{t+1}+\gamma Q\left(s_{t+1}, \underset{a^{\prime}}{\arg \max } Q\left(s_{t+1}, a^{\prime}\right)\right) \\ &amp; =r_{t+1}+\gamma \max_{a^{\prime}} Q\left(s_{t+1}, a^{\prime}\right) \end{aligned}\] <p>接着我们可以把 Q 学习更新写成增量学习的形式，时序差分目标变成了 \(r_{t+1}+\gamma \max_a Q\left(s_{t+1}, a\right)\) ，即</p> \[Q\left(s_t, a_t\right) \leftarrow Q\left(s_t, a_t\right)+\alpha\left[r_{t+1}+\gamma \max_a Q\left(s_{t+1}, a\right)-Q\left(s_t, a_t\right)\right]\] </li> </ul> </li> <li>Q-learning和Sarsa：</li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter3/Untitled%2017-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter3/Untitled%2017-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter3/Untitled%2017-1400.webp"></source> <img src="/assets/img/RL_chapter3/Untitled%2017.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter3/Untitled%2018-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter3/Untitled%2018-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter3/Untitled%2018-1400.webp"></source> <img src="/assets/img/RL_chapter3/Untitled%2018.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>通过备份图也可以看出差异：

- Sarsa里面A和A’都是通过一个同样的policy进行采样的，所以是在线策略
- Q-learning里面，A和A’不是同一个策略。A是探索策略（目标策略），A’是直接通过max操作得到的策略（行为策略）。
</code></pre></div></div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter3/Untitled%2019-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter3/Untitled%2019-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter3/Untitled%2019-1400.webp"></source> <img src="/assets/img/RL_chapter3/Untitled%2019.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1. DP和TD的总结：
</code></pre></div></div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter3/Untitled%2020-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter3/Untitled%2020-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter3/Untitled%2020-1400.webp"></source> <img src="/assets/img/RL_chapter3/Untitled%2020.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="333-on-policy和off-policy的区别">3.3.3 on-policy和off-policy的区别</h3> <p>总结一下在线策略和离线策略的区别。</p> <ul> <li>只用了一个策略 \(\pi\)，它不仅使用策略 \(\pi\) 学习，还使用策略 \(\pi\) 与环境交互产生经验。 如果策略采用 \(\epsilon\)贪心算法，它需要兼顾探索，为了兼顾探索和利用，它训练的时候会显得有点“胆小”。它在解决悬崖行走问题的时候，会尽可能地远离悬崖边，确保哪怕自己不小心探索了一点儿，也还是在安全区域内。此外，因为采用的是 \(\epsilon\)贪心 算法，策略会不断改变（\(\epsilon\) 值会不断变小），所以策略不稳定。</li> <li>有两种策略————目标策略和行为策略。 <ul> <li>行为策略探索得到的经验轨迹来优化目标策略，从而更有可能探索到最佳策略。行为策略可以采用 \(\epsilon\)贪心 算法</li> <li>目标策略采用的是贪心算法，它直接根据行为策略采集到的数据来采用最佳策略，所以不需要兼顾探索。</li> <li>off-policy是通过从行为策略\(\mu\)中的经验采样来学习目标策略\(\pi\)</li> </ul> </li> <li>我们比较一下 Q学习 和 Sarsa 的更新公式，就可以发现 Sarsa 并没有选取最大值的最大化操作。因此，Q学习是一个非常激进的方法，它希望每一步都获得最大的利益；Sarsa 则相对较为保守，它会选择一条相对安全的迭代路线。</li> </ul> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="//blog/2023/RL-lecture6/">第六章 Policy Optimization State of the art (策略优化进阶篇)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="//blog/2023/RL-lecture5/">第五章 Policy Optimization Foundation (策略优化基础篇)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="//blog/2023/RL-lecture4/">第四章 Value Function Approximation and Deep Q-learning (价值函数近似和深度Q学习)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="//blog/2023/RL-lecture2/">第二章 Markov Decision Process (马尔可夫决策过程)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="//blog/2023/RL-lecture1/">第一章 Overview (课程概括与RL基础)</a> </li> <div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"alshedivat/al-folio","data-repo-id":"MDEwOlJlcG9zaXRvcnk2MDAyNDM2NQ==","data-category":"Comments","data-category-id":"DIC_kwDOA5PmLc4CTBt6","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,a])=>giscusScript.setAttribute(t,a)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Steven Shaobo Wang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>