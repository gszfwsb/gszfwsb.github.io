<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>第四章 Value Function Approximation and Deep Q-learning (价值函数近似和深度Q学习) | Shaobo Wang</title> <meta name="author" content="Steven Shaobo Wang"> <meta name="description" content="VFA and Q-learning"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%91%A8%F0%9F%8F%BB%E2%80%8D%F0%9F%8E%93&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://gszfwsb.github.io/blog/2023/RL-lecture4/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="//">Shaobo Wang</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">第四章 Value Function Approximation and Deep Q-learning (价值函数近似和深度Q学习)</h1> <p class="post-meta">October 30, 2023</p> <p class="post-tags"> <a href="//blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="//blog/tag/intro"> <i class="fas fa-hashtag fa-sm"></i> intro</a>     ·   <a href="//blog/category/reinforcement-learning"> <i class="fas fa-tag fa-sm"></i> Reinforcement-Learning</a>   </p> </header> <article class="post-content"> <div id="table-of-contents"> <ul id="toc" class="section-nav"> <li class="toc-entry toc-h1"> <a href="#%E4%B8%80%E5%87%BD%E6%95%B0%E8%BF%91%E4%BC%BC">一、函数近似</a> <ul> <li class="toc-entry toc-h2"><a href="#11-%E9%97%AE%E9%A2%98%E5%BC%95%E5%85%A5%E5%A4%A7%E8%A7%84%E6%A8%A1%E7%9A%84mdp%E9%97%AE%E9%A2%98%E5%A6%82%E4%BD%95%E4%BC%B0%E8%AE%A1%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0">1.1 问题引入：大规模的MDP问题如何估计价值函数？</a></li> <li class="toc-entry toc-h2"><a href="#12-%E9%80%9A%E8%BF%87%E5%87%BD%E6%95%B0%E8%BF%91%E4%BC%BC%E7%9A%84%E6%96%B9%E6%B3%95%E6%9D%A5%E8%A7%A3%E5%86%B3%E5%A4%A7%E8%A7%84%E6%A8%A1rl">1.2 通过函数近似的方法来解决大规模RL</a></li> </ul> </li> <li class="toc-entry toc-h1"> <a href="#%E4%BA%8C%E9%A2%84%E6%B5%8B%E4%B8%AD%E7%9A%84%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0%E8%BF%91%E4%BC%BC">二、预测中的价值函数近似</a> <ul> <li class="toc-entry toc-h2"> <a href="#21-%E6%9C%89oracle%E6%A8%A1%E5%9E%8B%E9%A2%84%E6%B5%8Bvfa">2.1 有Oracle/模型预测VFA</a> <ul> <li class="toc-entry toc-h3"><a href="#211-%E5%9F%BA%E6%9C%AC%E5%86%85%E5%AE%B9">2.1.1 基本内容</a></li> <li class="toc-entry toc-h3"><a href="#212-%E7%BA%BF%E6%80%A7%E6%96%B9%E6%B3%95">2.1.2 线性方法</a></li> <li class="toc-entry toc-h3"><a href="#213-%E9%80%9A%E8%BF%87%E6%9F%A5%E7%89%B9%E5%BE%81%E8%A1%A8%E7%9A%84%E7%BA%BF%E6%80%A7%E6%96%B9%E6%B3%95">2.1.3 通过查特征表的线性方法</a></li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#22-%E6%97%A0%E6%A8%A1%E5%9E%8B%E9%A2%84%E6%B5%8Bvfa">2.2 无模型预测VFA</a> <ul> <li class="toc-entry toc-h3"><a href="#221-%E5%A2%9E%E9%87%8F%E5%BC%8F%E5%87%BD%E6%95%B0%E8%BF%91%E4%BC%BC%E9%A2%84%E6%B5%8B%E7%AE%97%E6%B3%95incremental-vfa-prediction-algorithms">2.2.1 增量式函数近似预测算法（Incremental VFA Prediction Algorithms）</a></li> <li class="toc-entry toc-h3"><a href="#222-%E5%9F%BA%E4%BA%8Evfa%E7%9A%84mc%E9%A2%84%E6%B5%8B">2.2.2 基于VFA的MC预测</a></li> <li class="toc-entry toc-h3"><a href="#223-%E5%9F%BA%E4%BA%8Evfa%E7%9A%84td%E9%A2%84%E6%B5%8B">2.2.3 基于VFA的TD预测</a></li> </ul> </li> </ul> </li> <li class="toc-entry toc-h1"> <a href="#%E4%B8%89-%E6%8E%A7%E5%88%B6%E4%B8%AD%E7%9A%84%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0%E8%BF%91%E4%BC%BC">三、 控制中的价值函数近似</a> <ul> <li class="toc-entry toc-h2"><a href="#31-%E6%9C%89oracle%E6%A8%A1%E5%9E%8B%E6%8E%A7%E5%88%B6vfa">3.1 有Oracle/模型控制VFA</a></li> <li class="toc-entry toc-h2"> <a href="#32-%E6%97%A0%E6%A8%A1%E5%9E%8B%E6%8E%A7%E5%88%B6%E4%B8%AD%E7%9A%84vfa">3.2 无模型控制中的VFA</a> <ul> <li class="toc-entry toc-h3"><a href="#321-%E5%A2%9E%E9%87%8F%E6%8E%A7%E5%88%B6%E7%AE%97%E6%B3%95incremental-control-algorithm">3.2.1 增量控制算法（Incremental Control Algorithm）</a></li> <li class="toc-entry toc-h3"><a href="#322-%E6%94%B6%E6%95%9B%E6%80%A7%E5%88%86%E6%9E%90sarsaq-learning">3.2.2 收敛性分析（Sarsa/Q-learning）</a></li> <li class="toc-entry toc-h3"><a href="#323-batch%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0">3.2.3 batch强化学习</a></li> </ul> </li> </ul> </li> <li class="toc-entry toc-h1"> <a href="#%E4%B8%89%E6%B7%B1%E5%BA%A6q%E7%BD%91%E7%BB%9C">三、深度Q网络</a> <ul> <li class="toc-entry toc-h2"><a href="#31-deep-q-learning-dqn">3.1 Deep Q-Learning (DQN)</a></li> <li class="toc-entry toc-h2"><a href="#32-dqnexperience-replay">3.2 DQN：experience replay</a></li> </ul> </li> </ul> </div> <hr> <div id="markdown-content"> <h1 id="一函数近似">一、函数近似</h1> <h2 id="11-问题引入大规模的mdp问题如何估计价值函数">1.1 问题引入：大规模的MDP问题如何估计价值函数？</h2> <p>在面对大规模 MDP 问题时，要避免用table去表示特征（Q-tabel等），而是采用带参数的<strong>函数近似</strong>的方式去近似估计V、Q、π</p> <ol> <li>表格型方法 <ul> <li>在表格型方法中，我们是通过查表的方式去计算价值函数的</li> <li>每一个状态-动作对 \(&lt;s,a&gt;\)有一个元素\(Q(s,a)\)</li> </ul> </li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter4/Untitled-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter4/Untitled-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter4/Untitled-1400.webp"></source> <img src="/assets/img/RL_chapter4/Untitled.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ol> <li>MDP的挑战： <ul> <li>太多状态和动作需要存下来</li> <li>单独学习每一个状态的价值很慢</li> </ul> </li> </ol> <h2 id="12-通过函数近似的方法来解决大规模rl">1.2 通过函数近似的方法来解决大规模RL</h2> <ol> <li>如何隐式学习或存储每个状态？需要学习的包括： <ol> <li>状态转移模型、奖励模型</li> <li>价值函数，Q函数</li> <li>策略</li> </ol> </li> <li> <p>解决方法：通过<strong>价值函数近似（VFA）</strong>来逼近。通过参数\(\mathbf{w}\)估计价值函数、Q函数和策略。</p> \[\begin{aligned}\hat{v}(s, \mathbf{w}) &amp; \approx v^\pi(s) \\\hat{q}(s, a, \mathbf{w}) &amp; \approx q^\pi(s, a) \\\hat{\pi}(a, s, \mathbf{w}) &amp; \approx \pi(a \mid s)\end{aligned}\] <ol> <li>可以从见过的状态泛化到没有见到的状态</li> <li>可以通过MC或者TD方法训练\(\mathbf{w}\)</li> </ol> </li> <li>一些近似方法：其中线性方法和神经网络是我们关注的重点 <ol> <li>特征线性组合</li> <li>神经网络</li> <li>决策树</li> <li>最近邻</li> </ol> </li> </ol> <h1 id="二预测中的价值函数近似">二、预测中的价值函数近似</h1> <h2 id="21-有oracle模型预测vfa">2.1 有Oracle/模型预测VFA</h2> <h3 id="211-基本内容">2.1.1 基本内容</h3> <ol> <li>Oracle：我们知道gt的value function \(v^\pi(s)\)，对所有状态\(s\)都是已知的。</li> <li>我们的目标是学习对\(v^\pi(s)\)的近似。</li> <li> <p>因此我们MSE来定义loss：</p> \[J(\mathbf{w})=\mathbb{E}_\pi\left[\left(v^\pi(s)-\hat{v}(s, \mathbf{w})\right)^2\right]\] </li> <li> <p>梯度下降：</p> \[\begin{aligned}\Delta \mathbf{w} &amp; =-\frac{1}{2} \alpha \nabla_{\mathbf{w}} J(\mathbf{w}) \\\mathbf{w}_{t+1} &amp; =\mathbf{w}_t+\Delta \mathbf{w}\end{aligned}\] </li> </ol> <h3 id="212-线性方法">2.1.2 线性方法</h3> <ol> <li> <p>如何通过特征向量表示状态？</p> \[\mathbf{x}(s)=\left(x_1(s), \ldots, x_n(s)\right)^T\] </li> <li> <p>价值函数——特征线性组合：</p> </li> </ol> \[\hat{v}(s, \mathbf{w})=\mathbf{x}(s)^T \mathbf{w}=\sum_{j=1}^n x_j(s) w_j\] <ol> <li>目标函数：</li> </ol> \[J(\mathbf{w})=\mathbb{E}_\pi\left[\left(v^\pi(s)-\mathbf{x}(s)^T \mathbf{w}\right)^2\right]\] \[\Delta \mathbf{w}=\alpha\left(v^\pi(s)-\hat{v}(s, \mathbf{w})\right) \mathbf{x}(s)\] <p>😍 <strong><em>Update = StepSize × PredictionError × FeatureValue</em></strong></p> <h3 id="213-通过查特征表的线性方法">2.1.3 通过查特征表的线性方法</h3> <ol> <li>是一种特殊的线性VFA</li> <li>查表的方式是通过1-hot向量实现的，特征都是0或1：</li> </ol> \[\mathbf{x}^{\text {table }}(s)=\left(\mathbf{1}\left(s=s_1\right), \ldots, \mathbf{1}\left(s=s_n\right)\right)^T\] <ol> <li>事实上价值函数的每一项恰好就是训练参数：</li> </ol> \[\hat{v}(s, \mathbf{w})=\left(\mathbf{1}\left(s=s_1\right), \ldots, \mathbf{1}\left(s=s_n\right)\right)\left(w_1, \ldots, w_n\right)^T\\\hat{v}\left(s_k, \mathbf{w}\right)=w_k\] <h2 id="22-无模型预测vfa">2.2 无模型预测VFA</h2> <p>实践中，没有办法知道所有状态的gt价值函数，没有oracle。</p> <p>回想下在无模型的预测中做的事：</p> <ol> <li>目标是去通过给定策略 \(\pi\) 估计价值函数 \(v^\pi\)</li> <li>维护一个 \(v^\pi\) 或者 \(q^\pi\) 的表格</li> <li>每回合评估更新（MC） 或 每一步评估更新（TD）</li> </ol> <h3 id="221-增量式函数近似预测算法incremental-vfa-prediction-algorithms">2.2.1 <strong>增量式函数近似预测算法</strong>（Incremental VFA Prediction Algorithms）</h3> <ol> <li> <p>在有gt价值函数的情形下，我们有：</p> \[\Delta \mathbf{w}=\alpha\left(\textcolor{red}{v^\pi(s)}-\hat{v}(s, \mathbf{w})\right) \nabla_{\mathbf{w}} \hat{v}\left(s_t, \mathbf{w}\right)\] </li> <li> <p>在真实场景下没有\(\textcolor{red}{v^\pi(s)}\)，我们需要寻求替代方案：</p> <ul> <li> <p>在MC中，我们用实际return代替</p> \[\Delta \mathbf{w}=\alpha\left(\textcolor{red}{G_t}-\hat{v}\left(s_t, \mathbf{w}\right)\right) \nabla_{\mathbf{w}} \hat{v}\left(s_t, \mathbf{w}\right)\] </li> <li> <p>在TD(0)中，我们用\(\delta_t\)代替</p> \[\Delta \mathbf{w}=\alpha\left(\textcolor{red}{R_{t+1}+\gamma \hat{v}\left(s_{t+1}, \mathbf{w}\right)}-\hat{v}\left(s_t, \mathbf{w}\right)\right) \nabla_{\mathbf{w}} \hat{v}\left(s_t, \mathbf{w}\right)\] </li> </ul> </li> </ol> <h3 id="222-基于vfa的mc预测">2.2.2 基于VFA的MC预测</h3> <ol> <li>奖励 \(G_t\) 是unbiased的，但是对于真正的value function是带有noise的，只有采样大量的\(G_t\) 才能恢复出\(v^\pi(s_t)\)，因为\(\mathbb{E}[G_t]=v^\pi(s_t)\)。</li> <li> <p>因此我们可以把 \(G_t\) 看成label，\(S_t\) 看成input，做一个监督学习任务。</p> \[&lt;S_1, G_1&gt;,&lt;S_2, G_2&gt;, \ldots,&lt;s_t, G_T&gt;\] </li> <li> <p>MC策略评估更新可以写为：</p> \[\begin{aligned} \Delta \mathbf{w} &amp; =\alpha\left(G_t-\hat{v}\left(s_t, \mathbf{w}\right)\right) \nabla_{\mathbf{w}} \hat{v}\left(s_t, \mathbf{w}\right) \\ &amp; =\alpha\left(G_t-\hat{v}\left(s_t, \mathbf{w}\right)\right) \mathbf{x}\left(s_t\right) \end{aligned}\] </li> <li>MC预测在线性/非线性VFA中都可以收敛到globel最优。</li> </ol> <h3 id="223-基于vfa的td预测">2.2.3 基于VFA的TD预测</h3> <ol> <li>TD目标\(R_{t+1}+\gamma \hat{v}\left(s_{t+1}, \mathbf{w}\right)\)是biased的，因为\(\mathbb{E}[R_{t+1}+\gamma \hat{v}\left(s_{t+1}, \mathbf{w}\right)]\neq v^\pi (s_t)\)</li> <li> <p>我们有类似的监督学习训练pair如下：</p> \[&lt;S_1, R_2+\gamma \hat{v}\left(s_2, \mathbf{w}\right)&gt;,&lt;S_2, R_3+\gamma \hat{v}\left(s_3, \mathbf{w}\right)&gt;, \ldots,&lt;S_{T-1}, R_T&gt;\] </li> <li> <p>TD(0)中的更新可以写为：</p> \[\begin{aligned}\Delta \mathbf{w} &amp; =\alpha\left(R+\gamma \hat{v}\left(s^{\prime}, \mathbf{w}\right)-\hat{v}(s, \mathbf{w})\right) \nabla_{\mathbf{w}} \hat{v}(s, \mathbf{w}) \\&amp; =\alpha\left(R+\gamma \hat{v}\left(s^{\prime}, \mathbf{w}\right)-\hat{v}(s, \mathbf{w})\right) \mathbf{x}(s)\end{aligned}\] <p>这个也叫做semi-gradient，因为我们忽略了改变权重向量\(\mathbf{w}\)对于target的效应。</p> </li> <li>线性TD(0)才可以收敛到globel最优。</li> </ol> <h1 id="三-控制中的价值函数近似">三、 控制中的价值函数近似</h1> <p>广义策略迭代：</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter4/Untitled%201-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter4/Untitled%201-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter4/Untitled%201-1400.webp"></source> <img src="/assets/img/RL_chapter4/Untitled%201.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>策略评估：近似策略评估，\(\hat{q}(., ., \mathbf{w}) \approx q^\pi\)</li> <li>策略改进：\(\varepsilon\)-贪心策略改进</li> </ul> <h2 id="31-有oracle模型控制vfa">3.1 有Oracle/模型控制VFA</h2> <ol> <li> <p>近似动作价值函数：</p> \[\hat{q}(s, a, \mathbf{w}) \approx q^\pi(s, a)\] </li> <li> <p>loss function:</p> \[J(\mathbf{w})=\mathbb{E}_\pi\left[\left(q^\pi(s, a)-\hat{q}(s, a, \mathbf{w})\right)^2\right]\] </li> <li> <p>梯度求解：</p> \[\Delta \mathbf{w}=\alpha\left(q^\pi(s, a)-\hat{q}(s, a, \mathbf{w})\right) \nabla_{\mathbf{w}} \hat{q}(s, a, \mathbf{w})\] </li> </ol> <h2 id="32-无模型控制中的vfa">3.2 无模型控制中的VFA</h2> <h3 id="321-增量控制算法incremental-control-algorithm">3.2.1 增量控制算法（Incremental Control Algorithm）</h3> <p>类似的，真实情况中我们没有oracle了。</p> <ol> <li> <p>MC：</p> \[\Delta \mathbf{w}=\alpha\left(\textcolor{red}{G_t}-\hat{q}\left(s_t, a_t, \mathbf{w}\right)\right) \nabla_{\mathbf{w}} \hat{q}\left(s_t, a_t, \mathbf{w}\right)\] </li> <li> <p>Sarsa：</p> \[\Delta \mathbf{w}=\alpha\left(\textcolor{red}{R_{t+1}+\gamma \hat{q}\left(s_{t+1}, a_{t+1}, \mathbf{w}\right)}-\hat{q}\left(s_t, a_t, \mathbf{w}\right)\right) \nabla_{\mathbf{w}} \hat{q}\left(s_t, a_t, \mathbf{w}\right)\] </li> <li> <p>Q-learning：</p> \[\Delta \mathbf{w}=\alpha\left(\textcolor{red}{R_{t+1}+\gamma \max _a \hat{q}\left(s_{t+1}, a, \mathbf{w}\right)}-\hat{q}\left(s_t, a_t, \mathbf{w}\right)\right) \nabla_{\mathbf{w}} \hat{q}\left(s_t, a_t, \mathbf{w}\right)\] </li> <li> <p>Semi-gradient Sarsa算法流程：</p> </li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter4/Untitled%202-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter4/Untitled%202-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter4/Untitled%202-1400.webp"></source> <img src="/assets/img/RL_chapter4/Untitled%202.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="322-收敛性分析sarsaq-learning">3.2.2 收敛性分析（Sarsa/Q-learning）</h3> <ol> <li>TD中的VFA不遵从任何一个Loss function</li> <li>这个更新包含两个近似，都会引入噪声： <ol> <li>贝尔曼备份公式中的近似</li> <li>近似价值函数的近似</li> </ol> </li> <li>一个挑战：行为策略和目标策略不同，因此价值函数的VFA可能没法收敛。</li> <li>死亡三角： <ol> <li>函数近似：近似会引入误差</li> <li>bootstrap：TD依赖于之前的估计会引入bias，MC方法避免了这个问题</li> <li>off-policy训练：采样分布和实际分布差异比较大</li> </ol> </li> <li>收敛性总结：</li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter4/Untitled%203-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter4/Untitled%203-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter4/Untitled%203-1400.webp"></source> <img src="/assets/img/RL_chapter4/Untitled%203.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="323-batch强化学习">3.2.3 batch强化学习</h3> <ol> <li>想法： <ol> <li>增量梯度下降更新是简单的，但是并不是有效率的采样方式，每走一步优化一次。</li> <li>基于batch的方法会找一批中的数据。</li> </ol> </li> <li> <p>建模：假设经验包含了\(&lt;state, value&gt;\)对。</p> \[D=\{&lt;s_i,v_i&gt;\}_{t=1}^T\] <p>迭代时重复两步操作：</p> <ol> <li>随机采样一对，\(&lt;s, v^\pi&gt;\sim \mathcal{D}\)</li> <li>用梯度下降法进行优化：\(\Delta \mathbf{w}=\alpha\left(v^\pi-\hat{v}(s, \mathbf{w})\right) \nabla_{\mathbf{w}} \hat{v}(s, \mathbf{w})\)</li> </ol> <p>等价于MSE loss</p> \[\mathbf{w}^{L S}=\underset{\mathbf{w}}{\arg \min } \sum_{t=1}^T\left(v_t^\pi-\hat{v}\left(s_t, \mathbf{w}\right)\right)^2\] </li> </ol> <h1 id="三深度q网络">三、深度Q网络</h1> <ol> <li>线性VFA vs 非线性VFA <ol> <li>线性VFA通常在给定正确的特征时效果很好，但是需要人工设计特征集。</li> <li>非线性VFA不需要人工涉及特征，直接用DNN</li> </ol> </li> <li>深度强化学习 <ol> <li>DNN需要表示：价值函数、策略函数以及环境模型</li> <li>优化：SGD</li> </ol> </li> </ol> <h2 id="31-deep-q-learning-dqn">3.1 Deep Q-Learning (DQN)</h2> <ol> <li>DQN通过神经网络表示Q函数</li> <li>DQN玩雅达利游戏： <ol> <li>端到端学习\(Q(s,a)\)，直接把像素帧作为输入</li> <li>输入最近4帧状态\(s\)的像素</li> <li>输出是\(Q(s,a)\)代表18个按钮位置</li> <li>奖励是这一步的分数</li> </ol> </li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter4/Untitled%204-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter4/Untitled%204-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter4/Untitled%204-1400.webp"></source> <img src="/assets/img/RL_chapter4/Untitled%204.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ol> <li>两个重要的问题： <ol> <li>样本之间的关系怎么处理？像素级别的关联很高。解决：experience replay</li> <li>target是带有noise的。解决：固定Q targets</li> </ol> </li> </ol> <h2 id="32-dqnexperience-replay">3.2 DQN：experience replay</h2> <ol> <li>为了减少样本之间的关联性，我们把 \((s_t,a_t,r_t,s_{t+1})\) 存进一个回放状态转移memory \(D\)，这是随机打乱的。希望采样出来的经验和目前状态没有很强的相关性。</li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter4/Untitled%205-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter4/Untitled%205-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter4/Untitled%205-1400.webp"></source> <img src="/assets/img/RL_chapter4/Untitled%205.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ol> <li>经验回放具体步骤： <ol> <li>在数据集中采样经验：\(\left(s, a, r, s^{\prime}\right) \sim \mathcal{D}\)</li> <li>计算采样出的价值目标：\(r+\gamma \max _{a^{\prime}} \hat{Q}\left(s^{\prime}, a^{\prime}, \mathbf{w}\right)\)</li> <li> <p>通过SGD算法更新网络权重:</p> \[\Delta \mathbf{w}=\alpha\left(r+\gamma \max _{a^{\prime}} \hat{Q}\left(s^{\prime}, a^{\prime}, \mathbf{w}\right)-Q(s, a, \mathbf{w})\right) \nabla_{\mathbf{w}} \hat{Q}(s, a, \mathbf{w})\] </li> </ol> </li> <li>固定Q目标 <ol> <li>为了提高训练的稳定性，希望在多次更新时，计算target时固定其权重。</li> <li>用一组新的参数\(\mathbf{w^-}\)作为使用的权重，\(\mathbf{w}\)是我们需要更新的权重。</li> <li>固定目标的经验回放具体步骤： <ol> <li>在数据集中采样经验：\(\left(s, a, r, s^{\prime}\right) \sim \mathcal{D}\)</li> <li>计算采样出的价值目标：\(r+\gamma \max _{a^{\prime}} \hat{Q}\left(s^{\prime}, a^{\prime}, \textcolor{red}{\mathbf{w}^-} \right)\)</li> <li> <p>通过SGD算法更新网络权重:</p> \[\Delta \mathbf{w}=\alpha\left(r+\gamma \max _{a^{\prime}} \hat{Q}\left(s^{\prime}, a^{\prime}, \textcolor{red}{\mathbf{w}^{-}}\right)-Q(s, a, \mathbf{w})\right) \nabla_{\mathbf{w}} \hat{Q}(s, a, \mathbf{w})\] </li> </ol> </li> </ol> <p>😍 为什么需要固定目标？ 一开始更新的时候，对于Q的估计和Q的目标每一步都在变化。如果我们希望Q的估计可以逼近Q的目标，那么一个很好的方式就是固定Q的目标，让对Q的估计更精准。</p> </li> </ol> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="//blog/2023/RL-lecture6/">第六章 Policy Optimization State of the art (策略优化进阶篇)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="//blog/2023/RL-lecture5/">第五章 Policy Optimization Foundation (策略优化基础篇)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="//blog/2023/RL-lecture3/">第三章 Tabular methods Model-free Prediction and Control (表格型方法：免模型的预测和控制)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="//blog/2023/RL-lecture2/">第二章 Markov Decision Process (马尔可夫决策过程)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="//blog/2023/RL-lecture1/">第一章 Overview (课程概括与RL基础)</a> </li> <div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"alshedivat/al-folio","data-repo-id":"MDEwOlJlcG9zaXRvcnk2MDAyNDM2NQ==","data-category":"Comments","data-category-id":"DIC_kwDOA5PmLc4CTBt6","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,a])=>giscusScript.setAttribute(t,a)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Steven Shaobo Wang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>