<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>第六章 Policy Optimization State of the art (策略优化进阶篇) | Shaobo Wang</title> <meta name="author" content="Steven Shaobo Wang"> <meta name="description" content="PO SOTA"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%91%A8%F0%9F%8F%BB%E2%80%8D%F0%9F%8E%93&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://gszfwsb.github.io/blog/2023/RL-lecture6/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="//">Shaobo Wang</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">第六章 Policy Optimization State of the art (策略优化进阶篇)</h1> <p class="post-meta">November 1, 2023</p> <p class="post-tags"> <a href="//blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="//blog/tag/intro"> <i class="fas fa-hashtag fa-sm"></i> intro</a>     ·   <a href="//blog/category/reinforcement-learning"> <i class="fas fa-tag fa-sm"></i> Reinforcement-Learning</a>   </p> </header> <article class="post-content"> <div id="table-of-contents"> <ul id="toc" class="section-nav"> <li class="toc-entry toc-h1"><a href="#%E4%B8%80%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E7%AE%97%E6%B3%95%E5%8F%98%E7%A7%8D%E6%A6%82%E8%A7%88">一、策略梯度算法变种概览</a></li> <li class="toc-entry toc-h1"> <a href="#%E4%BA%8C%E7%AD%96%E7%95%A5%E4%BC%98%E5%8C%96%E8%B7%AF%E7%BA%BFpg--natural-pg--trpo--acktr--ppo">二、策略优化路线：PG → Natural PG / TRPO → ACKTR → PPO</a> <ul> <li class="toc-entry toc-h2"><a href="#21-%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E7%9A%84%E9%97%AE%E9%A2%98">2.1 策略梯度的问题</a></li> <li class="toc-entry toc-h2"> <a href="#22-natural-pg">2.2 Natural PG</a> <ul> <li class="toc-entry toc-h3"><a href="#npg%E7%9A%84%E6%80%A7%E8%B4%A8">NPG的性质</a></li> <li class="toc-entry toc-h3"><a href="#%E9%87%8D%E8%A6%81%E6%80%A7%E9%87%87%E6%A0%B7">重要性采样</a></li> <li class="toc-entry toc-h3"><a href="#trpo%E4%B8%AD%E7%9A%84natural-policy-gradient">TRPO中的natural policy gradient</a></li> </ul> </li> <li class="toc-entry toc-h2"><a href="#23-trpo">2.3 TRPO</a></li> <li class="toc-entry toc-h2"><a href="#24-acktr">2.4 ACKTR</a></li> <li class="toc-entry toc-h2"> <a href="#25-ppo">2.5 PPO</a> <ul> <li class="toc-entry toc-h3"><a href="#%E5%9F%BA%E7%A1%80%E7%89%88ppo">基础版PPO</a></li> <li class="toc-entry toc-h3"><a href="#%E5%B8%A6clipping%E7%9A%84ppo">带clipping的PPO</a></li> </ul> </li> </ul> </li> <li class="toc-entry toc-h1"> <a href="#%E4%B8%89%E4%BB%B7%E5%80%BC%E4%BC%98%E5%8C%96%E8%B7%AF%E7%BA%BFq-learning--ddpg--td3--sac">三、价值优化路线：Q-learning → DDPG → TD3 → SAC</a> <ul> <li class="toc-entry toc-h2"><a href="#31-deep-deterministic-policy-gradient-ddpg">3.1 Deep Deterministic Policy Gradient (DDPG)</a></li> <li class="toc-entry toc-h2"><a href="#32-twin-delayed-ddpg-td3">3.2 Twin Delayed DDPG (TD3)</a></li> <li class="toc-entry toc-h2"> <a href="#33-soft-actor-critic-sac">3.3 Soft Actor-Critic (SAC)</a> <ul> <li class="toc-entry toc-h3"><a href="#%E9%87%8D%E5%8F%82%E6%95%B0%E5%8C%96">重参数化</a></li> </ul> </li> </ul> </li> </ul> </div> <hr> <div id="markdown-content"> <blockquote> <p>上次介绍了策略优化的基础方法，本次介绍一些SOTA的策略优化方法 关于PG算法，有很多变种，这次会介绍。此外，策略优化的相关工作主要有两条线，</p> <ol> <li>PG → Natural PG / TRPO → ACKTR → PPO</li> <li>Q-learning → DDPG → TD3 → SAC</li> </ol> </blockquote> <h1 id="一策略梯度算法变种概览">一、策略梯度算法变种概览</h1> <ol> <li> <p>策略函数有很多种形式:</p> \[\begin{aligned}\nabla_\theta J(\theta) &amp; =\mathbb{E}_{\pi_\theta}\left[\nabla_\theta \log \pi_\theta(s, a) G_t\right] \text { - REINFORCE } \\&amp; =\mathbb{E}_{\pi_\theta}\left[\nabla_\theta \log \pi_\theta(s, a) Q^w(s, a)\right] \text { - Q Actor-Critic } \\&amp; =\mathbb{E}_{\pi_\theta}\left[\nabla_\theta \log \pi_\theta(s, a) A^w(s, a)\right] \text { - Advantage Actor-Critic } \\&amp; =\mathbb{E}_{\pi_\theta}\left[\nabla_\theta \log \pi_\theta(s, a) \delta\right] \text { - TD Actor-Critic }\end{aligned}\] </li> <li>评论员用策略评估（MC或TD）去估计\(Q^\pi(s, a), A^\pi(s, a), \text { or } V^\pi(s)\)</li> <li>两条线的工作：</li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter6/Untitled-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter6/Untitled-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter6/Untitled-1400.webp"></source> <img src="/assets/img/RL_chapter6/Untitled.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h1 id="二策略优化路线pg--natural-pg--trpo--acktr--ppo">二、策略优化路线：PG → Natural PG / TRPO → ACKTR → PPO</h1> <h2 id="21-策略梯度的问题">2.1 策略梯度的问题</h2> <ol> <li> <p>采样效率很低：是on-policy learning</p> \[\nabla_\theta J(\theta)=\mathbb{E}_{a \sim \pi_\theta}\left[\nabla_\theta \log \pi_\theta(s, a) r(s, a)\right]\] </li> <li>比较大尺度的策略更新或者不太合适的步长都会摧毁训练过程： <ol> <li>和监督学习不一样，数据和学习是独立的。</li> <li>在RL中，如果步子太远，会得到比较差的策略，于是就会收集比较差的数据。</li> <li>如果得到了比较差的策略，很难从中恢复一个好的策略，这样就会影响整体的性能。</li> </ol> </li> <li>如何让训练过程更稳定？用TRPO和Natural PG</li> <li>如何让其训练类似一个off-policy优化？用TRPO中的重要性采样</li> </ol> <h2 id="22-natural-pg">2.2 Natural PG</h2> <p>PG方法是直接在参数空间中选择最陡的坡来优化，缺点在于这对策略函数十分敏感。</p> \[d^*=\nabla_\theta J(\theta)=\lim _{\epsilon \rightarrow 0} \frac{1}{\epsilon} \arg \max J(\theta+d), \text { s.t. }\|d\| \leq \epsilon\] <p>在分布空间（策略输出）中最陡的方向可以通过KL散度做约束：</p> \[d^*=\arg \max J(\theta+d) \text {, s.t. } K L\left(\pi_\theta \| \pi_{\theta+d}\right)=c\] <p>固定KL散度为一个常数\(c\)使得我们可以在参数空间中优化的速度是一个常数。KL散度可以衡量两个分布之间的差异：</p> \[K L\left(\pi_\theta \| \pi_{\theta^{\prime}}\right)=E_{\pi_\theta}\left[\log \pi_\theta\right]-E_{\pi_\theta}\left[\log \pi_{\theta^{\prime}}\right]\] <p>尽管KL散度是非对称的并且不是真正的metric，我们也可以适用。因为如果\(d\to 0\)，KL散度是渐进对称的。因此，在邻域内，KL散度是近似对称的。可以证明KL散度的二阶泰勒展开是：</p> \[K L\left(\pi_\theta \| \pi_{\theta+d}\right) \approx \frac{1}{2} d^T F d\] <p>其中<strong>F</strong>是<strong>Fisher Information Matrix，</strong>是KL散度\(E_{\pi_\theta}\left[\nabla \log \pi_\theta \nabla \log \pi_\theta^T\right]\)的二阶导。</p> <p>我们用拉格朗日乘子重新写一下上述形式，然后再借助泰勒展开近似：</p> \[d^*=\argmax_d J(\theta +d)-\lambda (KL(\pi)\theta \| \pi_{\theta+d})-c)\\ \approx \argmax_d J(\theta) + \nabla_\theta J(\theta)^Td - \frac12 \lambda d^TFd + \lambda c\] <p>求导我们就能得到 natural policy gradient: \(d=\frac1\lambda F^{-1}\nabla_\theta J(\theta)\)</p> <h3 id="npg的性质">NPG的性质</h3> <ol> <li> <p>second-order优化，更精确，而且和模型无关：</p> \[\theta_{t+1}=\theta_t + \alpha F^{-1} \nabla_\theta J(\theta)\] <p>其中\(F=E_{\pi_\theta}\left[\nabla \log \pi_\theta \nabla \log \pi_\theta^T\right]\)是fisher information matrix，衡量了policy distribution的曲率</p> </li> <li> <p>不管模型怎么参数化，NPG都会产生一样的策略变化，因为\(F\)是固定的。</p> </li> </ol> <h3 id="重要性采样">重要性采样</h3> <p>我们可以通过重要性采样，把PG变成一个off-policy的学习，有点类似于ELBO推导。</p> <p>Importance sampling (IS) 计算\(f(x), x \sim p(x)\)的期望，如果我们不知道\(p\)，可以通过另一个分布\(q\)来做采样，</p> \[\mathbb{E}_{x\sim p}[f(x)]=\int p(x)f(x)dx = \int q(x)\frac{p(x)}{q(x)}f(x)dx = \mathbb{E}_{x\sim q}[\frac{p(x)}{q(x)}f(x)]\] <p>这样我们可以用IS来获得我们的目标函数，其中\(\hat\pi\)就是行为策略。</p> \[J(\theta)=\mathbb{E}_{a\sim \pi_\theta}[r(s,a)]=\mathbb{E}_{a\sim \hat{\pi}}[\frac{\pi_\theta(s,a)}{\hat{\pi}(s,a)}r(s,a)]\] <p>借助了IS的思想，我们准备直接用old policy作为行为策略，因此我们可以定义一个objective function为：</p> \[\theta = \argmax_\theta J_{\theta_{old}}(\theta)=\argmax \mathbb{E}_t [\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}R_t]\] <table> <tbody> <tr> <td>存在一个问题：如果$$\frac{\pi_\theta(a_t</td> <td>s_t)}{\pi_{\theta_{old}}(a_t</td> <td>s_t)}$$太大，这个objective function的值也会非常大。所以有没有办法限制一下这个比值呢？比如，把这两个policy的差异变得小一些。例如，用KL divergence去度量这个距离</td> </tr> </tbody> </table> \[K L\left(\pi_{\theta_{\text {old }}}|| \pi_\theta\right)=-\sum_a \pi_{\theta_{\text {old }}}(a \mid s) \log \frac{\pi_\theta(a \mid s)}{\pi_{\theta_{\text {old }}}(a \mid s)}\] <p>这样，我们带有trust region的objective，可以写成如下的形式：</p> \[\begin{gathered}J_{\theta_{\text {old }}}(\theta)=\mathbb{E}_t\left[\frac{\pi_\theta\left(a_t \mid s_t\right)}{\pi_{\theta_{\text {old }}}\left(a_t \mid s_t\right)} R_t\right] \\\text { subject to } K L\left(\pi_{\theta_{\text {old }}}\left(. \mid s_t\right)|| \pi_\theta\left(. \mid s_t\right)\right) \leq \delta\end{gathered}\] <p>在 trust region 中，我们把参数搜索固定在一个范围内.经过一些推导和泰勒展开近似，我们有：</p> \[\begin{aligned} J_{\theta_t}(\theta) &amp; \approx g^T\left(\theta-\theta_t\right) \\ K L\left(\theta_t \| \theta\right) &amp; \approx \frac{1}{2}\left(\theta-\theta_t\right)^T H\left(\theta-\theta_t\right) \end{aligned}\] <p>其中 \(g=\nabla_\theta J_{\theta_t}(\theta),H=\nabla_\theta^2 K L\left(\theta_t \| \theta\right)\) 其中 \(\theta_t\) 是old policy parameter，因此：</p> \[\theta_{t+1}=\underset{\theta}{\arg \max } g^T\left(\theta-\theta_t\right) \text { s.t. } \frac{1}{2}\left(\theta-\theta_t\right)^T H\left(\theta-\theta_t\right) \leq \delta\] <p>可以求出解析解：</p> \[\theta_{t+1}=\theta_t+\sqrt{\frac{2 \delta}{g^T H^{-1} g}} H^{-1} g\] <ul> <li> <p>NG是fisher information matrix \(F\)下最陡峭的上山方向。</p> \[H=\nabla_\theta^2 K L\left(\pi_{\theta_t}|| \pi_\theta\right)=E_{a, s \sim \pi_{\theta_t}}\left[\nabla_\theta \log \pi_\theta(a, s) \nabla_\theta \log \pi_\theta(a, s)^T\right]\] </li> <li>学习率\(\delta\)可以被看作是选择一个normalized step size来改变policy</li> <li>亮点：任何参数更新不会影响policy network的输出。</li> </ul> <h3 id="trpo中的natural-policy-gradient">TRPO中的natural policy gradient</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter6/Untitled%201-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter6/Untitled%201-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter6/Untitled%201-1400.webp"></source> <img src="/assets/img/RL_chapter6/Untitled%201.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h2 id="23-trpo">2.3 TRPO</h2> <p>有一些问题：</p> <ol> <li>FIM和逆计算是开销非常大</li> <li>TRPO通过解一个线性方程来解决这个问题。即通过估解\(Hx=g\)来解\(x=H^{-1}g\).那么可以等价于优化一个二次方程：\(\min _x \frac{1}{2} x^T H x-g^T x\)</li> <li> <p>一些解释：解决 \(Ax=b\)等价于</p> \[\begin{gathered}x=\underset{x}{\arg \max } f(x)=\frac{1}{2} x^T A x-b^T x \\\text { since } f^{\prime}(x)=A x-b=0\end{gathered}\] </li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter6/Untitled%202-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter6/Untitled%202-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter6/Untitled%202-1400.webp"></source> <img src="/assets/img/RL_chapter6/Untitled%202.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>用的解法是 **conjugate gradient (CG)**
</code></pre></div></div> <ol> <li>其实TRPO算法有一点类似于EM算法，是一类Minorize-Maximization(MM)算法，解法都是先maximize一个邻近的函数然后逼近这个local expected reward</li> <li>TRPO的问题： <ol> <li>计算FIM开销太大</li> <li>计算精确FIM需要很多次采样</li> <li>CG算法实现起来比较困难</li> </ol> </li> </ol> <h2 id="24-acktr">2.4 ACKTR</h2> <p>用 <strong>Kronecker-factored approximation (K-FAC)</strong> 来改进TRPO，具体而言就是降低计算FIM求逆的复杂度</p> \[F=E_{x \sim \pi_{\theta_t}}\left[\left(\nabla_\theta \log \pi_\theta(x)\right)^T\left(\nabla_\theta \log \pi_\theta(x)\right)\right]\] <p>把这个替换为layer-wise calculation，因为是对称的，所以可以分块矩阵计算</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter6/Untitled%203-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter6/Untitled%203-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter6/Untitled%203-1400.webp"></source> <img src="/assets/img/RL_chapter6/Untitled%203.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter6/Untitled%204-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter6/Untitled%204-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter6/Untitled%204-1400.webp"></source> <img src="/assets/img/RL_chapter6/Untitled%204.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h2 id="25-ppo">2.5 PPO</h2> <h3 id="基础版ppo">基础版PPO</h3> <p>其实就是把TRPO的loss改写了一下，变成拉格朗日的形式：</p> \[\max \mathbb{E}_t\left[\frac{\pi_\theta\left(a_t \mid s_t\right)}{\pi_{\theta_{\text {old }}}\left(a_t \mid s_t\right)} R_t\right] \\\text { subject to } K L\left(\pi_{\theta_{\text {old }}}\left(. \mid s_t\right)|| \pi_\theta\left(. \mid s_t\right)\right) \leq \delta\] <p>改为：</p> \[\max \mathbb{E}_t\left[\frac{\pi_\theta\left(a_t \mid s_t\right)}{\pi_{\theta_{\text {old }}}\left(a_t \mid s_t\right)} R_t\right] -\beta K L\left(\pi_{\theta_{\text {old }}}\left(. \mid s_t\right)|| \pi_\theta\left(. \mid s_t\right)\right)\] <p>这样的好处就是可以直接用SGD来优化，速度比second order快很多。</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter6/Untitled%205-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter6/Untitled%205-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter6/Untitled%205-1400.webp"></source> <img src="/assets/img/RL_chapter6/Untitled%205.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="带clipping的ppo">带clipping的PPO</h3> <p>定义概率比值\(r_t(\theta)=\frac{\pi_\theta\left(a_t \mid s_t\right)}{\pi_{\theta_{\text {old }}}\left(a_t \mid s_t\right)}\)，那么我们有不同的objectives</p> <ul> <li>不带trust region的PG: \(L_t(\theta)=r_t(\theta) \hat{A_t}\)</li> <li>KL限制：\(L_t(\theta)=r_t(\theta) \hat{A_t}.\text{s.t.}, KL[\pi_{\theta_{old}},\pi_\theta]\leq \delta\)</li> <li>KL惩罚：\(L_t(\theta)=r_t(\theta) \hat{A_t}- \beta KL[\pi_{\theta_{old}},\pi_\theta]\)</li> <li>(new) 限制policy不要和old policy离开太远：\(L_t(\theta)=\min (r_t(\theta) \hat{A_t},\text{clip}(r_t(\theta),1-\epsilon,1+\epsilon)\hat{A_t})\)</li> </ul> <p>可以看作是一个正则化项：</p> <ul> <li>当advantage为正时，鼓励action增加→ \(L_t(\theta)=\min (r_t(\theta),1+\epsilon)\hat{A_t}\)</li> <li>当advantage为正时，鼓励action减少→\(L_t(\theta)=\min (r_t(\theta),1-\epsilon)\hat{A_t}\)</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter6/Untitled%206-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter6/Untitled%206-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter6/Untitled%206-1400.webp"></source> <img src="/assets/img/RL_chapter6/Untitled%206.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>PPO比起TRPO稳定性可靠性更好，而且实现起来更简单。</p> <blockquote> <p>总结一下SOTA的policy优化：</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> </figure> </div> </div> </blockquote> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter6/Untitled%207-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter6/Untitled%207-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter6/Untitled%207-1400.webp"></source> <img src="/assets/img/RL_chapter6/Untitled%207.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <p>&lt;/figure&gt;</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;/div&gt;
</code></pre></div></div> <p>&lt;/div&gt;</p> <blockquote> </blockquote> <h1 id="三价值优化路线q-learning--ddpg--td3--sac">三、价值优化路线：Q-learning → DDPG → TD3 → SAC</h1> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter6/Untitled%208-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter6/Untitled%208-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter6/Untitled%208-1400.webp"></source> <img src="/assets/img/RL_chapter6/Untitled%208.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h2 id="31-deep-deterministic-policy-gradient-ddpg">3.1 <strong>Deep Deterministic Policy Gradient (DDPG)</strong> </h2> <p>原始的DQN是做离散动作的，我们可以把这个拓展到连续空间么？DDPG就可以看作是一个连续版本的DQN.</p> \[\textbf{DQN}: a^*=\argmax_a Q^*(s,a)\\ \textbf{DDPG}: a^*=\argmax_a Q^*(s,a) \approx Q_\phi (s,\mu_\theta(s))\] <ul> <li>\(\mu_\theta(s)\)是一个deterministic的策略，直接给出一个最大化\(Q_\phi (s,\mu_\theta(s))\)的action</li> <li>动作\(a\)是连续的</li> <li>我们假设Q-function 对\(a\)是可导的</li> </ul> <p>因此，DDPG有如下objective：</p> \[\textbf{Q-target}:y(r,s',d)=r+\gamma(1-d)Q_{\phi_{targ}} (s',\mu_{\theta_{targ}}(s'))\\ \textbf{Q-function}:\min \mathbb{E}_{s,r,s',d\sim D}[Q_\phi(s,a)-y(r,s',d)]\\ \textbf{policy}: \max_\theta \mathbb{E}_{s\sim D}[Q_\phi (s,\mu_\theta(s))]\] <p>同样，DDPG也用了reply buffer和target network</p> <h2 id="32-twin-delayed-ddpg-td3">3.2 Twin Delayed DDPG (TD3)</h2> <p>DDPG的缺点：有严重的过拟合，为此TD3做了三个改进：</p> <ol> <li> <strong>Clipped Double-Q Learning</strong>：TD3学习两个Q函数，用两个Q value里面小的那一个来得到target</li> <li> <strong>“Delayed” Policy Updates：</strong>TD3更新策略缓慢，更新Q函数快速（两个Q函数更新一次policy再更新一次）</li> <li> <strong>**</strong><strong>Target Policy Smoothing：</strong><strong>**</strong>TD3在target action里面增加噪声，让policy更难利用Q函数。</li> </ol> <p>具体而言，TD3学习两个Q函数\(Q_{\phi_1},Q_{\phi_2}\)，两个函数用一个target如下：</p> \[y(r,s',d)=r+\gamma(1-d)\min_{i=1,2}Q_{\phi_{i,targ}}(s'a_{TD3}(s'))\] <p>Target Policy Smoothing（类似于正则化）</p> \[a_{TD3}(s')=clip(\mu_{\theta,targ}(s'))+clip(\epsilon,-c,c),a_{low},a_{high}),\epsilon\sim N(0,\sigma)\] <h2 id="33-soft-actor-critic-sac">3.3 <strong>Soft Actor-Critic (SAC)</strong> </h2> <p>SAC是用类似于DDPG的方法，进行stochastic policy的优化，融合了 <strong>entropy regularization</strong> （熵正则化）。具体而言，这个策略被训练来最大化 expected return和entropy之间的trade-of</p> \[\pi^*=\argmax \mathbb{E}_{\tau\sim \pi} [\sum_t \gamma^t(R(s_t,a_t,s_{t+1})+\alpha H(\pi(\cdot | s_t)))]\] <p>value function也包含了entropy bonus</p> \[V^\pi(s)=\mathbb{E}_{\tau\sim \pi}[\sum_t \gamma^t (R(s_t,a_t,s_{t+1})+\alpha H(\pi(\cdot|s_t)))|s_0=s]\] <p>我们可以推导出类似的bellman equation：</p> \[Q^\pi(s,a)=\mathbb{E}_{s'\sim P,a'\sim \pi}[R(s,a,s')+\gamma(Q^\pi(s',a')+\alpha H(\pi(a'|s')))]\\ =\mathbb{E}_{s'\sim P,a'\sim \pi}[R(s,a,s')+\gamma(Q^\pi(s',a')-\alpha \log\pi(a'|s'))]\] <p>因此Q函数的更新可以写成：</p> \[Q^\pi(s,a)\leftarrow r+\gamma(Q^\pi(s',\hat{a}')-\alpha \log \pi(\hat{a}'|s')),\hat{a}'\sim \pi(\cdot|s')\] <p>类似TD3，也是学习两个Q函数，用clipped double-Q trick。两个Q函数都是通过mean sqaure bellman error学习的：</p> \[L(\phi_i,D)=\mathbb{E}[(Q_\phi(s,a)-y(r,s',d))^2]\\ y(r,s',d)=r+\gamma(1-d)(\min_{j=1,2}Q_{\phi_{targ,j}}(s',\hat{a}'-\alpha \log\pi_\theta(\hat{a}'|s'))),\\ \hat{a}'\sim \pi_{\theta}(\cdot|s')\] <p>policy通过maximize\(V^\pi(s)\)来学习。</p> <h3 id="重参数化">重参数化</h3> <p>接下来我们把action当作是从一个gaussian分布里面采样得到的，</p> \[\hat{a}_\theta(s,\epsilon)=tanh(\mu_\theta(s)+\sigma_\theta(s)\odot\epsilon),\epsilon\sim N(0,I)\] <p>重参数化的好处：可以让我们把对action的期望转换为一个对noise的期望——和参数无关的分布，即</p> \[\mathbb{E}_{a\sim\pi_\theta}[Q^\pi_\theta(s,a)-\alpha \log \pi_\theta(a|s)]\\=\mathbb{E}_{\epsilon\sim N}[Q^\pi_\theta(s,\hat{a}_\theta(s,\epsilon))-\alpha \log \pi_\theta(\hat{a}_\theta(s,\epsilon)|s)]\] <p>因此，policy优化公式可以写为：</p> \[\max_{\theta}\mathbb{E}_{s\sim D,\epsilon\sim N}[\min_{j=1,2}Q_{\phi_j}(s,\hat{a}_\theta(s,\epsilon))-\alpha \log \pi_\theta(\hat{a}_\theta(s,\epsilon)|s)]\] <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter6/Untitled%209-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter6/Untitled%209-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter6/Untitled%209-1400.webp"></source> <img src="/assets/img/RL_chapter6/Untitled%209.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="//blog/2023/RL-lecture5/">第五章 Policy Optimization Foundation (策略优化基础篇)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="//blog/2023/RL-lecture4/">第四章 Value Function Approximation and Deep Q-learning (价值函数近似和深度Q学习)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="//blog/2023/RL-lecture3/">第三章 Tabular methods Model-free Prediction and Control (表格型方法：免模型的预测和控制)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="//blog/2023/RL-lecture2/">第二章 Markov Decision Process (马尔可夫决策过程)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="//blog/2023/RL-lecture1/">第一章 Overview (课程概括与RL基础)</a> </li> <div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"alshedivat/al-folio","data-repo-id":"MDEwOlJlcG9zaXRvcnk2MDAyNDM2NQ==","data-category":"Comments","data-category-id":"DIC_kwDOA5PmLc4CTBt6","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,a])=>giscusScript.setAttribute(t,a)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Steven Shaobo Wang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>