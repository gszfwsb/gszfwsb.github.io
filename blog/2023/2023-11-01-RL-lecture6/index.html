<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <blockquote> <p>上次介绍了策略优化的基础方法，本次介绍一些SOTA的策略优化方法 关于PG算法，有很多变种，这次会介绍。此外，策略优化的相关工作主要有两条线，</p> <ol> <li>PG → Natural PG / TRPO → ACKTR → PPO</li> <li>Q-learning → DDPG → TD3 → SAC</li> </ol> </blockquote> <h1 id="一策略梯度算法变种概览">一、策略梯度算法变种概览</h1> <ol> <li> <p>策略函数有很多种形式:</p> \[\begin{aligned}\nabla_\theta J(\theta) &amp; =\mathbb{E}_{\pi_\theta}\left[\nabla_\theta \log \pi_\theta(s, a) G_t\right] \text { - REINFORCE } \\&amp; =\mathbb{E}_{\pi_\theta}\left[\nabla_\theta \log \pi_\theta(s, a) Q^w(s, a)\right] \text { - Q Actor-Critic } \\&amp; =\mathbb{E}_{\pi_\theta}\left[\nabla_\theta \log \pi_\theta(s, a) A^w(s, a)\right] \text { - Advantage Actor-Critic } \\&amp; =\mathbb{E}_{\pi_\theta}\left[\nabla_\theta \log \pi_\theta(s, a) \delta\right] \text { - TD Actor-Critic }\end{aligned}\] </li> <li>评论员用策略评估（MC或TD）去估计\(Q^\pi(s, a), A^\pi(s, a), \text { or } V^\pi(s)\)</li> <li>两条线的工作：</li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter6/Untitled-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter6/Untitled-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter6/Untitled-1400.webp"></source> <img src="/assets/img/RL_chapter6/Untitled.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h1 id="二策略优化路线pg--natural-pg--trpo--acktr--ppo">二、策略优化路线：PG → Natural PG / TRPO → ACKTR → PPO</h1> <h2 id="21-策略梯度的问题">2.1 策略梯度的问题</h2> <ol> <li> <p>采样效率很低：是on-policy learning</p> \[\nabla_\theta J(\theta)=\mathbb{E}_{a \sim \pi_\theta}\left[\nabla_\theta \log \pi_\theta(s, a) r(s, a)\right]\] </li> <li>比较大尺度的策略更新或者不太合适的步长都会摧毁训练过程： <ol> <li>和监督学习不一样，数据和学习是独立的。</li> <li>在RL中，如果步子太远，会得到比较差的策略，于是就会收集比较差的数据。</li> <li>如果得到了比较差的策略，很难从中恢复一个好的策略，这样就会影响整体的性能。</li> </ol> </li> <li>如何让训练过程更稳定？用TRPO和Natural PG</li> <li>如何让其训练类似一个off-policy优化？用TRPO中的重要性采样</li> </ol> <h2 id="22-natural-pg">2.2 Natural PG</h2> <p>PG方法是直接在参数空间中选择最陡的坡来优化，缺点在于这对策略函数十分敏感。</p> \[d^*=\nabla_\theta J(\theta)=\lim _{\epsilon \rightarrow 0} \frac{1}{\epsilon} \arg \max J(\theta+d), \text { s.t. }\|d\| \leq \epsilon\] <p>在分布空间（策略输出）中最陡的方向可以通过KL散度做约束：</p> \[d^*=\arg \max J(\theta+d) \text {, s.t. } K L\left(\pi_\theta \| \pi_{\theta+d}\right)=c\] <p>固定KL散度为一个常数\(c\)使得我们可以在参数空间中优化的速度是一个常数。KL散度可以衡量两个分布之间的差异：</p> \[K L\left(\pi_\theta \| \pi_{\theta^{\prime}}\right)=E_{\pi_\theta}\left[\log \pi_\theta\right]-E_{\pi_\theta}\left[\log \pi_{\theta^{\prime}}\right]\] <p>尽管KL散度是非对称的并且不是真正的metric，我们也可以适用。因为如果\(d\to 0\)，KL散度是渐进对称的。因此，在邻域内，KL散度是近似对称的。可以证明KL散度的二阶泰勒展开是：</p> \[K L\left(\pi_\theta \| \pi_{\theta+d}\right) \approx \frac{1}{2} d^T F d\] <p>其中<strong>F</strong>是<strong>Fisher Information Matrix，</strong>是KL散度\(E_{\pi_\theta}\left[\nabla \log \pi_\theta \nabla \log \pi_\theta^T\right]\)的二阶导。</p> <p>我们用拉格朗日乘子重新写一下上述形式，然后再借助泰勒展开近似：</p> \[d^*=\argmax_d J(\theta +d)-\lambda (KL(\pi)\theta \| \pi_{\theta+d})-c)\\ \approx \argmax_d J(\theta) + \nabla_\theta J(\theta)^Td - \frac12 \lambda d^TFd + \lambda c\] <p>求导我们就能得到 natural policy gradient: \(d=\frac1\lambda F^{-1}\nabla_\theta J(\theta)\)</p> <h3 id="npg的性质">NPG的性质</h3> <ol> <li> <p>second-order优化，更精确，而且和模型无关：</p> \[\theta_{t+1}=\theta_t + \alpha F^{-1} \nabla_\theta J(\theta)\] <p>其中\(F=E_{\pi_\theta}\left[\nabla \log \pi_\theta \nabla \log \pi_\theta^T\right]\)是fisher information matrix，衡量了policy distribution的曲率</p> </li> <li> <p>不管模型怎么参数化，NPG都会产生一样的策略变化，因为\(F\)是固定的。</p> </li> </ol> <h3 id="重要性采样">重要性采样</h3> <p>我们可以通过重要性采样，把PG变成一个off-policy的学习，有点类似于ELBO推导。</p> <p>Importance sampling (IS) 计算\(f(x), x \sim p(x)\)的期望，如果我们不知道\(p\)，可以通过另一个分布\(q\)来做采样，</p> \[\mathbb{E}_{x\sim p}[f(x)]=\int p(x)f(x)dx = \int q(x)\frac{p(x)}{q(x)}f(x)dx = \mathbb{E}_{x\sim q}[\frac{p(x)}{q(x)}f(x)]\] <p>这样我们可以用IS来获得我们的目标函数，其中\(\hat\pi\)就是行为策略。</p> \[J(\theta)=\mathbb{E}_{a\sim \pi_\theta}[r(s,a)]=\mathbb{E}_{a\sim \hat{\pi}}[\frac{\pi_\theta(s,a)}{\hat{\pi}(s,a)}r(s,a)]\] <p>借助了IS的思想，我们准备直接用old policy作为行为策略，因此我们可以定义一个objective function为：</p> \[\theta = \argmax_\theta J_{\theta_{old}}(\theta)=\argmax \mathbb{E}_t [\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}R_t]\] <table> <tbody> <tr> <td>存在一个问题：如果$$\frac{\pi_\theta(a_t</td> <td>s_t)}{\pi_{\theta_{old}}(a_t</td> <td>s_t)}$$太大，这个objective function的值也会非常大。所以有没有办法限制一下这个比值呢？比如，把这两个policy的差异变得小一些。例如，用KL divergence去度量这个距离</td> </tr> </tbody> </table> \[K L\left(\pi_{\theta_{\text {old }}}|| \pi_\theta\right)=-\sum_a \pi_{\theta_{\text {old }}}(a \mid s) \log \frac{\pi_\theta(a \mid s)}{\pi_{\theta_{\text {old }}}(a \mid s)}\] <p>这样，我们带有trust region的objective，可以写成如下的形式：</p> \[\begin{gathered}J_{\theta_{\text {old }}}(\theta)=\mathbb{E}_t\left[\frac{\pi_\theta\left(a_t \mid s_t\right)}{\pi_{\theta_{\text {old }}}\left(a_t \mid s_t\right)} R_t\right] \\\text { subject to } K L\left(\pi_{\theta_{\text {old }}}\left(. \mid s_t\right)|| \pi_\theta\left(. \mid s_t\right)\right) \leq \delta\end{gathered}\] <p>在 trust region 中，我们把参数搜索固定在一个范围内.经过一些推导和泰勒展开近似，我们有：</p> \[\begin{aligned} J_{\theta_t}(\theta) &amp; \approx g^T\left(\theta-\theta_t\right) \\ K L\left(\theta_t \| \theta\right) &amp; \approx \frac{1}{2}\left(\theta-\theta_t\right)^T H\left(\theta-\theta_t\right) \end{aligned}\] <p>其中 \(g=\nabla_\theta J_{\theta_t}(\theta),H=\nabla_\theta^2 K L\left(\theta_t \| \theta\right)\) 其中 \(\theta_t\) 是old policy parameter，因此：</p> \[\theta_{t+1}=\underset{\theta}{\arg \max } g^T\left(\theta-\theta_t\right) \text { s.t. } \frac{1}{2}\left(\theta-\theta_t\right)^T H\left(\theta-\theta_t\right) \leq \delta\] <p>可以求出解析解：</p> \[\theta_{t+1}=\theta_t+\sqrt{\frac{2 \delta}{g^T H^{-1} g}} H^{-1} g\] <ul> <li> <p>NG是fisher information matrix \(F\)下最陡峭的上山方向。</p> \[H=\nabla_\theta^2 K L\left(\pi_{\theta_t}|| \pi_\theta\right)=E_{a, s \sim \pi_{\theta_t}}\left[\nabla_\theta \log \pi_\theta(a, s) \nabla_\theta \log \pi_\theta(a, s)^T\right]\] </li> <li>学习率\(\delta\)可以被看作是选择一个normalized step size来改变policy</li> <li>亮点：任何参数更新不会影响policy network的输出。</li> </ul> <h3 id="trpo中的natural-policy-gradient">TRPO中的natural policy gradient</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter6/Untitled%201-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter6/Untitled%201-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter6/Untitled%201-1400.webp"></source> <img src="/assets/img/RL_chapter6/Untitled%201.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h2 id="23-trpo">2.3 TRPO</h2> <p>有一些问题：</p> <ol> <li>FIM和逆计算是开销非常大</li> <li>TRPO通过解一个线性方程来解决这个问题。即通过估解\(Hx=g\)来解\(x=H^{-1}g\).那么可以等价于优化一个二次方程：\(\min _x \frac{1}{2} x^T H x-g^T x\)</li> <li> <p>一些解释：解决 \(Ax=b\)等价于</p> \[\begin{gathered}x=\underset{x}{\arg \max } f(x)=\frac{1}{2} x^T A x-b^T x \\\text { since } f^{\prime}(x)=A x-b=0\end{gathered}\] </li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter6/Untitled%202-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter6/Untitled%202-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter6/Untitled%202-1400.webp"></source> <img src="/assets/img/RL_chapter6/Untitled%202.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>用的解法是 **conjugate gradient (CG)**
</code></pre></div></div> <ol> <li>其实TRPO算法有一点类似于EM算法，是一类Minorize-Maximization(MM)算法，解法都是先maximize一个邻近的函数然后逼近这个local expected reward</li> <li>TRPO的问题： <ol> <li>计算FIM开销太大</li> <li>计算精确FIM需要很多次采样</li> <li>CG算法实现起来比较困难</li> </ol> </li> </ol> <h2 id="24-acktr">2.4 ACKTR</h2> <p>用 <strong>Kronecker-factored approximation (K-FAC)</strong> 来改进TRPO，具体而言就是降低计算FIM求逆的复杂度</p> \[F=E_{x \sim \pi_{\theta_t}}\left[\left(\nabla_\theta \log \pi_\theta(x)\right)^T\left(\nabla_\theta \log \pi_\theta(x)\right)\right]\] <p>把这个替换为layer-wise calculation，因为是对称的，所以可以分块矩阵计算</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter6/Untitled%203-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter6/Untitled%203-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter6/Untitled%203-1400.webp"></source> <img src="/assets/img/RL_chapter6/Untitled%203.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter6/Untitled%204-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter6/Untitled%204-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter6/Untitled%204-1400.webp"></source> <img src="/assets/img/RL_chapter6/Untitled%204.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h2 id="25-ppo">2.5 PPO</h2> <h3 id="基础版ppo">基础版PPO</h3> <p>其实就是把TRPO的loss改写了一下，变成拉格朗日的形式：</p> \[\max \mathbb{E}_t\left[\frac{\pi_\theta\left(a_t \mid s_t\right)}{\pi_{\theta_{\text {old }}}\left(a_t \mid s_t\right)} R_t\right] \\\text { subject to } K L\left(\pi_{\theta_{\text {old }}}\left(. \mid s_t\right)|| \pi_\theta\left(. \mid s_t\right)\right) \leq \delta\] <p>改为：</p> \[\max \mathbb{E}_t\left[\frac{\pi_\theta\left(a_t \mid s_t\right)}{\pi_{\theta_{\text {old }}}\left(a_t \mid s_t\right)} R_t\right] -\beta K L\left(\pi_{\theta_{\text {old }}}\left(. \mid s_t\right)|| \pi_\theta\left(. \mid s_t\right)\right)\] <p>这样的好处就是可以直接用SGD来优化，速度比second order快很多。</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter6/Untitled%205-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter6/Untitled%205-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter6/Untitled%205-1400.webp"></source> <img src="/assets/img/RL_chapter6/Untitled%205.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="带clipping的ppo">带clipping的PPO</h3> <p>定义概率比值\(r_t(\theta)=\frac{\pi_\theta\left(a_t \mid s_t\right)}{\pi_{\theta_{\text {old }}}\left(a_t \mid s_t\right)}\)，那么我们有不同的objectives</p> <ul> <li>不带trust region的PG: \(L_t(\theta)=r_t(\theta) \hat{A_t}\)</li> <li>KL限制：\(L_t(\theta)=r_t(\theta) \hat{A_t}.\text{s.t.}, KL[\pi_{\theta_{old}},\pi_\theta]\leq \delta\)</li> <li>KL惩罚：\(L_t(\theta)=r_t(\theta) \hat{A_t}- \beta KL[\pi_{\theta_{old}},\pi_\theta]\)</li> <li>(new) 限制policy不要和old policy离开太远：\(L_t(\theta)=\min (r_t(\theta) \hat{A_t},\text{clip}(r_t(\theta),1-\epsilon,1+\epsilon)\hat{A_t})\)</li> </ul> <p>可以看作是一个正则化项：</p> <ul> <li>当advantage为正时，鼓励action增加→ \(L_t(\theta)=\min (r_t(\theta),1+\epsilon)\hat{A_t}\)</li> <li>当advantage为正时，鼓励action减少→\(L_t(\theta)=\min (r_t(\theta),1-\epsilon)\hat{A_t}\)</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter6/Untitled%206-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter6/Untitled%206-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter6/Untitled%206-1400.webp"></source> <img src="/assets/img/RL_chapter6/Untitled%206.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>PPO比起TRPO稳定性可靠性更好，而且实现起来更简单。</p> <blockquote> <p>总结一下SOTA的policy优化：</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> </figure> </div> </div> </blockquote> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter6/Untitled%207-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter6/Untitled%207-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter6/Untitled%207-1400.webp"></source> <img src="/assets/img/RL_chapter6/Untitled%207.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <p>&lt;/figure&gt;</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;/div&gt;
</code></pre></div></div> <p>&lt;/div&gt;</p> <blockquote> </blockquote> <h1 id="三价值优化路线q-learning--ddpg--td3--sac">三、价值优化路线：Q-learning → DDPG → TD3 → SAC</h1> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter6/Untitled%208-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter6/Untitled%208-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter6/Untitled%208-1400.webp"></source> <img src="/assets/img/RL_chapter6/Untitled%208.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h2 id="31-deep-deterministic-policy-gradient-ddpg">3.1 <strong>Deep Deterministic Policy Gradient (DDPG)</strong> </h2> <p>原始的DQN是做离散动作的，我们可以把这个拓展到连续空间么？DDPG就可以看作是一个连续版本的DQN.</p> \[\textbf{DQN}: a^*=\argmax_a Q^*(s,a)\\ \textbf{DDPG}: a^*=\argmax_a Q^*(s,a) \approx Q_\phi (s,\mu_\theta(s))\] <ul> <li>\(\mu_\theta(s)\)是一个deterministic的策略，直接给出一个最大化\(Q_\phi (s,\mu_\theta(s))\)的action</li> <li>动作\(a\)是连续的</li> <li>我们假设Q-function 对\(a\)是可导的</li> </ul> <p>因此，DDPG有如下objective：</p> \[\textbf{Q-target}:y(r,s',d)=r+\gamma(1-d)Q_{\phi_{targ}} (s',\mu_{\theta_{targ}}(s'))\\ \textbf{Q-function}:\min \mathbb{E}_{s,r,s',d\sim D}[Q_\phi(s,a)-y(r,s',d)]\\ \textbf{policy}: \max_\theta \mathbb{E}_{s\sim D}[Q_\phi (s,\mu_\theta(s))]\] <p>同样，DDPG也用了reply buffer和target network</p> <h2 id="32-twin-delayed-ddpg-td3">3.2 Twin Delayed DDPG (TD3)</h2> <p>DDPG的缺点：有严重的过拟合，为此TD3做了三个改进：</p> <ol> <li> <strong>Clipped Double-Q Learning</strong>：TD3学习两个Q函数，用两个Q value里面小的那一个来得到target</li> <li> <strong>“Delayed” Policy Updates：</strong>TD3更新策略缓慢，更新Q函数快速（两个Q函数更新一次policy再更新一次）</li> <li> <strong>**</strong><strong>Target Policy Smoothing：</strong><strong>**</strong>TD3在target action里面增加噪声，让policy更难利用Q函数。</li> </ol> <p>具体而言，TD3学习两个Q函数\(Q_{\phi_1},Q_{\phi_2}\)，两个函数用一个target如下：</p> \[y(r,s',d)=r+\gamma(1-d)\min_{i=1,2}Q_{\phi_{i,targ}}(s'a_{TD3}(s'))\] <p>Target Policy Smoothing（类似于正则化）</p> \[a_{TD3}(s')=clip(\mu_{\theta,targ}(s'))+clip(\epsilon,-c,c),a_{low},a_{high}),\epsilon\sim N(0,\sigma)\] <h2 id="33-soft-actor-critic-sac">3.3 <strong>Soft Actor-Critic (SAC)</strong> </h2> <p>SAC是用类似于DDPG的方法，进行stochastic policy的优化，融合了 <strong>entropy regularization</strong> （熵正则化）。具体而言，这个策略被训练来最大化 expected return和entropy之间的trade-of</p> \[\pi^*=\argmax \mathbb{E}_{\tau\sim \pi} [\sum_t \gamma^t(R(s_t,a_t,s_{t+1})+\alpha H(\pi(\cdot | s_t)))]\] <p>value function也包含了entropy bonus</p> \[V^\pi(s)=\mathbb{E}_{\tau\sim \pi}[\sum_t \gamma^t (R(s_t,a_t,s_{t+1})+\alpha H(\pi(\cdot|s_t)))|s_0=s]\] <p>我们可以推导出类似的bellman equation：</p> \[Q^\pi(s,a)=\mathbb{E}_{s'\sim P,a'\sim \pi}[R(s,a,s')+\gamma(Q^\pi(s',a')+\alpha H(\pi(a'|s')))]\\ =\mathbb{E}_{s'\sim P,a'\sim \pi}[R(s,a,s')+\gamma(Q^\pi(s',a')-\alpha \log\pi(a'|s'))]\] <p>因此Q函数的更新可以写成：</p> \[Q^\pi(s,a)\leftarrow r+\gamma(Q^\pi(s',\hat{a}')-\alpha \log \pi(\hat{a}'|s')),\hat{a}'\sim \pi(\cdot|s')\] <p>类似TD3，也是学习两个Q函数，用clipped double-Q trick。两个Q函数都是通过mean sqaure bellman error学习的：</p> \[L(\phi_i,D)=\mathbb{E}[(Q_\phi(s,a)-y(r,s',d))^2]\\ y(r,s',d)=r+\gamma(1-d)(\min_{j=1,2}Q_{\phi_{targ,j}}(s',\hat{a}'-\alpha \log\pi_\theta(\hat{a}'|s'))),\\ \hat{a}'\sim \pi_{\theta}(\cdot|s')\] <p>policy通过maximize\(V^\pi(s)\)来学习。</p> <h3 id="重参数化">重参数化</h3> <p>接下来我们把action当作是从一个gaussian分布里面采样得到的，</p> \[\hat{a}_\theta(s,\epsilon)=tanh(\mu_\theta(s)+\sigma_\theta(s)\odot\epsilon),\epsilon\sim N(0,I)\] <p>重参数化的好处：可以让我们把对action的期望转换为一个对noise的期望——和参数无关的分布，即</p> \[\mathbb{E}_{a\sim\pi_\theta}[Q^\pi_\theta(s,a)-\alpha \log \pi_\theta(a|s)]\\=\mathbb{E}_{\epsilon\sim N}[Q^\pi_\theta(s,\hat{a}_\theta(s,\epsilon))-\alpha \log \pi_\theta(\hat{a}_\theta(s,\epsilon)|s)]\] <p>因此，policy优化公式可以写为：</p> \[\max_{\theta}\mathbb{E}_{s\sim D,\epsilon\sim N}[\min_{j=1,2}Q_{\phi_j}(s,\hat{a}_\theta(s,\epsilon))-\alpha \log \pi_\theta(\hat{a}_\theta(s,\epsilon)|s)]\] <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RL_chapter6/Untitled%209-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RL_chapter6/Untitled%209-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RL_chapter6/Untitled%209-1400.webp"></source> <img src="/assets/img/RL_chapter6/Untitled%209.png" class="rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </body></html>